# Migration Guide: SDK 1.x to SDK 2.0

This guide helps you migrate your agent from SDK 1.x to SDK 2.0.

## Why SDK 2.0?

SDK 2.0 introduces an **agent-controlled execution model**:

- **SDK 1.x**: The harness controls execution. Your agent receives requests and returns responses. The harness executes commands.
- **SDK 2.0**: Your agent controls execution. You run commands directly, manage your own loop, and signal when done.

Benefits of SDK 2.0:
- Simpler mental model (imperative instead of reactive)
- Direct command execution (no round-trip latency)
- Better control over execution flow
- Easier debugging

## Quick Comparison

### SDK 1.x (Old)

```python
from term_sdk import Agent, Request, Response, run

class MyAgent(Agent):
    def setup(self):
        self.llm = LLM()
    
    def solve(self, req: Request) -> Response:
        # Reactive: respond to each request
        if req.first:
            return Response.cmd("ls -la")
        
        if req.failed:
            return Response.done("Task failed")
        
        if "hello.txt" in req.output:
            return Response.done("Found it!")
        
        return Response.cmd("find . -name '*.txt'")

if __name__ == "__main__":
    run(MyAgent())
```

### SDK 2.0 (New)

```python
from term_sdk import Agent, AgentContext, run

class MyAgent(Agent):
    def setup(self):
        self.llm = LLM()
    
    def run(self, ctx: AgentContext):
        # Imperative: control your own execution
        result = ctx.shell("ls -la")
        
        if result.failed:
            ctx.log("Task failed")
            ctx.done()
            return
        
        if "hello.txt" in result.stdout:
            ctx.log("Found it!")
            ctx.done()
            return
        
        result = ctx.shell("find . -name '*.txt'")
        ctx.done()

if __name__ == "__main__":
    run(MyAgent())
```

## Migration Steps

### Step 1: Update Imports

```python
# Old (SDK 1.x)
from term_sdk import Agent, Request, Response, run

# New (SDK 2.0)
from term_sdk import Agent, AgentContext, run
```

### Step 2: Replace `solve()` with `run()`

```python
# Old
def solve(self, req: Request) -> Response:
    ...
    return Response.cmd("ls -la")

# New
def run(self, ctx: AgentContext):
    ...
    result = ctx.shell("ls -la")
```

### Step 3: Replace Response Returns with Direct Actions

| SDK 1.x | SDK 2.0 |
|---------|---------|
| `return Response.cmd("ls")` | `result = ctx.shell("ls")` |
| `return Response.done()` | `ctx.done()` |
| `return Response.done("message")` | `ctx.log("message"); ctx.done()` |

### Step 4: Replace Request Properties with Context

| SDK 1.x (`req.`) | SDK 2.0 (`ctx.` / `result.`) |
|------------------|------------------------------|
| `req.instruction` | `ctx.instruction` |
| `req.first` | `ctx.step == 1` |
| `req.step` | `ctx.step` |
| `req.output` | `result.stdout` (after `ctx.shell()`) |
| `req.exit_code` | `result.exit_code` |
| `req.ok` | `result.ok` |
| `req.failed` | `result.failed` |
| `req.has("pattern")` | `result.has("pattern")` |

### Step 5: Convert Reactive Logic to Imperative

**SDK 1.x (Reactive)**

The harness calls `solve()` repeatedly. You track state to know what to do next:

```python
def solve(self, req: Request) -> Response:
    if req.first:
        return Response.cmd("ls -la")
    
    if req.step == 2:
        if "target.txt" in req.output:
            return Response.cmd("cat target.txt")
        else:
            return Response.done("File not found")
    
    if req.step == 3:
        return Response.done()
```

**SDK 2.0 (Imperative)**

You control the flow directly:

```python
def run(self, ctx: AgentContext):
    result = ctx.shell("ls -la")
    
    if "target.txt" not in result.stdout:
        ctx.log("File not found")
        ctx.done()
        return
    
    result = ctx.shell("cat target.txt")
    ctx.done()
```

## Common Patterns

### Pattern 1: Simple Command Sequence

**SDK 1.x:**
```python
def solve(self, req: Request) -> Response:
    if req.step == 1:
        return Response.cmd("mkdir -p /app/output")
    elif req.step == 2:
        return Response.cmd("echo 'Hello' > /app/output/hello.txt")
    elif req.step == 3:
        return Response.cmd("cat /app/output/hello.txt")
    else:
        return Response.done()
```

**SDK 2.0:**
```python
def run(self, ctx: AgentContext):
    ctx.shell("mkdir -p /app/output")
    ctx.shell("echo 'Hello' > /app/output/hello.txt")
    result = ctx.shell("cat /app/output/hello.txt")
    ctx.log(f"Created file with: {result.stdout}")
    ctx.done()
```

### Pattern 2: LLM-Driven Loop

**SDK 1.x:**
```python
def solve(self, req: Request) -> Response:
    messages = self._build_messages(req)
    response = self.llm.chat(messages)
    return Response.from_llm(response.text)
```

**SDK 2.0:**
```python
def run(self, ctx: AgentContext):
    messages = [{"role": "user", "content": ctx.instruction}]
    
    while ctx.step < 100:  # Limit to 100 steps
        response = self.llm.chat(messages)
        data = response.json()
        
        if data.get("task_complete"):
            break
        
        cmd = data.get("command")
        if cmd:
            result = ctx.shell(cmd)
            messages.append({"role": "assistant", "content": response.text})
            messages.append({"role": "user", "content": f"Output:\n{result.output}"})
    
    ctx.done()
```

### Pattern 3: Error Handling

**SDK 1.x:**
```python
def solve(self, req: Request) -> Response:
    if req.failed:
        self.error_count += 1
        if self.error_count > 3:
            return Response.done("Too many errors")
        return Response.cmd("pwd")  # Recovery command
    return Response.cmd(self.next_command())
```

**SDK 2.0:**
```python
def run(self, ctx: AgentContext):
    error_count = 0
    
    for cmd in self.get_commands():
        result = ctx.shell(cmd)
        
        if result.failed:
            error_count += 1
            if error_count > 3:
                ctx.log("Too many errors")
                ctx.done()
                return
            ctx.shell("pwd")  # Recovery command
    
    ctx.done()
```

### Pattern 4: File Operations

**SDK 1.x:**
```python
def solve(self, req: Request) -> Response:
    if req.step == 1:
        return Response.cmd("cat config.json")
    elif req.step == 2:
        config = json.loads(req.output)
        new_config = self.modify_config(config)
        # Need to escape JSON for shell
        return Response.cmd(f"echo '{json.dumps(new_config)}' > config.json")
```

**SDK 2.0:**
```python
def run(self, ctx: AgentContext):
    # Direct file read
    content = ctx.read("config.json")
    config = json.loads(content.stdout)
    
    # Modify and write back
    new_config = self.modify_config(config)
    ctx.write("config.json", json.dumps(new_config, indent=2))
    
    ctx.done()
```

### Pattern 5: Conditional Branching

**SDK 1.x:**
```python
def solve(self, req: Request) -> Response:
    if req.first:
        return Response.cmd("test -f package.json && echo EXISTS || echo MISSING")
    
    if "EXISTS" in req.output:
        self.has_package_json = True
        return Response.cmd("npm install")
    else:
        return Response.cmd("pip install -r requirements.txt")
```

**SDK 2.0:**
```python
def run(self, ctx: AgentContext):
    check = ctx.shell("test -f package.json && echo EXISTS || echo MISSING")
    
    if "EXISTS" in check.stdout:
        ctx.shell("npm install")
    else:
        ctx.shell("pip install -r requirements.txt")
    
    ctx.done()
```

## LLM Integration (Unchanged)

The `LLM` class works exactly the same in SDK 2.0:

```python
from term_sdk import Agent, AgentContext, LLM, run

class MyAgent(Agent):
    def setup(self):
        # Same as before
        self.llm = LLM(
            provider="openrouter",
            default_model="anthropic/claude-3.5-sonnet"
        )
    
    def run(self, ctx: AgentContext):
        # Streaming works the same
        for chunk in self.llm.stream(ctx.instruction):
            print(chunk, end="", flush=True)
        
        # Non-streaming works the same
        result = self.llm.ask("What should I do?")
        
        # Function calling works the same
        tools = [Tool(name="search", description="Search files", parameters={...})]
        result = self.llm.chat(messages, tools=tools)
        
        ctx.done()
    
    def cleanup(self):
        self.llm.close()
```

## Checklist

Before submitting your migrated agent:

- [ ] Updated imports (`AgentContext` instead of `Request`/`Response`)
- [ ] Replaced `solve()` with `run()`
- [ ] Replaced `Response.cmd()` with `ctx.shell()`
- [ ] Replaced `Response.done()` with `ctx.done()`
- [ ] Updated property access (`ctx.instruction`, `result.stdout`, etc.)
- [ ] Converted reactive logic to imperative flow
- [ ] Tested locally with `term bench agent`
- [ ] Verified LLM integration still works

## Troubleshooting

### "AgentContext has no attribute 'output'"

You're trying to access the output before running a command. In SDK 2.0, output comes from `ShellResult`:

```python
# Wrong
output = ctx.output

# Right
result = ctx.shell("ls")
output = result.stdout
```

### "Agent keeps running forever"

Make sure you call `ctx.done()` to signal completion:

```python
def run(self, ctx: AgentContext):
    ctx.shell("do something")
    ctx.done()  # Don't forget this!
```

### "Max steps exceeded"

Your agent ran too many commands. Check `ctx.step` and exit early:

```python
while ctx.step < 100:  # Limit to 100 steps
    # ... do work ...
    if should_stop:
        break

ctx.done()
```

### "Response.from_llm not working"

`Response.from_llm()` is a SDK 1.x method. In SDK 2.0, parse the LLM response yourself:

```python
# SDK 1.x
return Response.from_llm(llm_result.text)

# SDK 2.0
data = llm_result.json()  # Parse JSON from response
if data.get("command"):
    ctx.shell(data["command"])
if data.get("task_complete"):
    ctx.done()
```

## Getting Help

- [Agent Development Guide](miner/agent-development.md) - Full SDK 2.0 documentation
- [SDK Reference](miner/sdk-reference.md) - Complete API reference
- [Examples](examples/) - Working example agents
