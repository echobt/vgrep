# Agent Development Guide

This guide covers everything you need to build effective Term Challenge agents.

## Complete Example Project

For a fully-featured agent implementation, check out our reference project:

**[https://github.com/PlatformNetwork/baseagent](https://github.com/PlatformNetwork/baseagent)**

This repository contains a production-ready agent with:
- Complete project structure
- LLM integration patterns
- Error handling best practices
- Testing utilities

> **Note**: All our repositories include an `AGENTS.md` file at the root. This file provides comprehensive documentation about the project architecture, making it easier for AI agents to understand and work with the codebase.

## Agent Lifecycle

Every agent follows a three-phase lifecycle:

```
┌─────────────────────────────────────────────────────────────┐
│                     AGENT LIFECYCLE                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│   1. setup()          2. run(ctx)           3. cleanup()     │
│   ┌─────────┐        ┌───────────┐         ┌──────────┐     │
│   │ Init    │───────>│ Execute   │────────>│ Teardown │     │
│   │ LLM,    │        │ commands, │         │ close    │     │
│   │ state   │        │ LLM calls │         │ resources│     │
│   └─────────┘        └───────────┘         └──────────┘     │
│                                                              │
│   Called once        Called per task       Called once       │
│   at startup         (your main logic)     at shutdown       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Basic Structure

```python
from term_sdk import Agent, AgentContext, LLM, run

class MyAgent(Agent):
    def setup(self):
        """Initialize resources. Called once at startup."""
        self.llm = LLM(default_model="anthropic/claude-3.5-sonnet")
    
    def run(self, ctx: AgentContext):
        """Execute the task. Called for each task."""
        # Your task-solving logic here
        ctx.done()
    
    def cleanup(self):
        """Release resources. Called at shutdown."""
        self.llm.close()

if __name__ == "__main__":
    run(MyAgent())
```

## AgentContext

The `AgentContext` object provides everything you need to interact with the task environment.

### Properties

| Property | Type | Description |
|----------|------|-------------|
| `ctx.instruction` | `str` | The task description |
| `ctx.step` | `int` | Current step number (starts at 1) |
| `ctx.history` | `List[HistoryEntry]` | Previous commands and outputs |
| `ctx.is_done` | `bool` | Whether task is marked complete |
| `ctx.elapsed_secs` | `float` | Seconds since task started |


### Methods

| Method | Description |
|--------|-------------|
| `ctx.shell(cmd)` | Execute a shell command |
| `ctx.read(path)` | Read a file's contents |
| `ctx.write(path, content)` | Write content to a file |
| `ctx.log(msg)` | Log a message |
| `ctx.done()` | Mark task as complete |

## Shell Execution

The `ctx.shell()` method executes shell commands and returns a `ShellResult`:

```python
result = ctx.shell("ls -la")

# Check success
if result.ok:
    print("Command succeeded")

if result.failed:
    print(f"Command failed with exit code {result.exit_code}")

# Access output
print(result.stdout)   # Standard output
print(result.stderr)   # Standard error
print(result.output)   # Combined stdout + stderr

# Check for patterns
if result.has("error", "failed"):
    print("Output contains error or failed")

# Timing info
print(f"Took {result.duration_ms}ms")
print(f"Timed out: {result.timed_out}")
```

### ShellResult Properties

| Property | Type | Description |
|----------|------|-------------|
| `command` | `str` | The executed command |
| `stdout` | `str` | Standard output |
| `stderr` | `str` | Standard error |
| `exit_code` | `int` | Exit code (0 = success) |
| `output` | `str` | Combined stdout + stderr |
| `ok` | `bool` | True if exit_code == 0 |
| `failed` | `bool` | True if exit_code != 0 |
| `timed_out` | `bool` | True if command timed out |
| `duration_ms` | `int` | Execution time in milliseconds |

### Shell Options

```python
# Custom timeout (default: 60 seconds)
result = ctx.shell("npm install", timeout=120)

# Custom working directory
result = ctx.shell("ls", cwd="/app/subdir")
```

## File Operations

### Reading Files

```python
# Read a file
content = ctx.read("config.json")
if content.ok:
    data = json.loads(content.stdout)
else:
    ctx.log(f"Failed to read: {content.stderr}")
```

### Writing Files

```python
# Write a file (creates parent directories if needed)
result = ctx.write("output/result.txt", "Hello, World!")
if result.ok:
    ctx.log("File written successfully")
```

### When to Use Shell vs File Methods

| Use `ctx.shell()` | Use `ctx.read()`/`ctx.write()` |
|-------------------|--------------------------------|
| Running programs | Reading config files |
| Complex file operations | Writing output files |
| Piped commands | JSON/YAML parsing |
| System commands | Text transformations |

```python
# Shell for complex operations
ctx.shell("find . -name '*.py' | xargs grep 'def '")

# File methods for simple read/write
config = ctx.read("settings.yaml")
ctx.write("output.txt", processed_data)
```

## LLM Integration

The SDK provides a powerful `LLM` class for language model integration.

### Basic Usage

```python
from term_sdk import LLM

# Initialize (in setup)
self.llm = LLM(
    provider="openrouter",
    default_model="anthropic/claude-3.5-sonnet",
    temperature=0.3,
    max_tokens=4096
)

# Simple question
response = self.llm.ask("What is 2+2?")
print(response.text)

# With system prompt
response = self.llm.ask(
    "Explain this error",
    system="You are a helpful coding assistant."
)
```

### Streaming

For real-time output:

```python
# Iterator-based streaming
for chunk in self.llm.stream("Tell me a story"):
    print(chunk, end="", flush=True)

# Callback-based streaming
def handle_chunk(chunk):
    print(chunk, end="", flush=True)
    return True  # Return False to stop early

response = self.llm.ask_stream("Tell me a story", on_chunk=handle_chunk)
```

### Chat Conversations

```python
messages = [
    {"role": "system", "content": "You are a terminal expert."},
    {"role": "user", "content": "How do I find large files?"}
]

response = self.llm.chat(messages)
print(response.text)

# Add to conversation
messages.append({"role": "assistant", "content": response.text})
messages.append({"role": "user", "content": "What about files over 1GB?"})

response = self.llm.chat(messages)
```

### Function Calling

Define tools for the LLM to use:

```python
from term_sdk import Tool

# Define tools
tools = [
    Tool(
        name="run_command",
        description="Execute a shell command",
        parameters={
            "type": "object",
            "properties": {
                "command": {
                    "type": "string",
                    "description": "The shell command to run"
                }
            },
            "required": ["command"]
        }
    ),
    Tool(
        name="read_file",
        description="Read contents of a file",
        parameters={
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Path to the file"
                }
            },
            "required": ["path"]
        }
    )
]

# Register handlers
def run_command(command: str) -> str:
    result = ctx.shell(command)
    return result.output

def read_file(path: str) -> str:
    result = ctx.read(path)
    return result.stdout

self.llm.register_function("run_command", run_command)
self.llm.register_function("read_file", read_file)

# Use with automatic function execution
response = self.llm.chat_with_functions(messages, tools, max_iterations=10)
```

### LLM Response

```python
response = self.llm.ask("Question")

# Access response data
print(response.text)           # Response text
print(response.model)          # Model used
print(response.tokens)         # Total tokens
print(response.cost)           # Cost in USD
print(response.latency_ms)     # Request latency

# Parse as JSON (handles markdown code blocks)
data = response.json()
if data:
    print(data["command"])

# Function calls
if response.has_function_calls():
    for call in response.function_calls:
        print(f"Function: {call.name}")
        print(f"Arguments: {call.arguments}")
```

### Error Handling

```python
from term_sdk import LLMError, CostLimitExceeded

try:
    response = self.llm.ask("Question")
except CostLimitExceeded as e:
    # Fatal: stop the agent
    ctx.log(f"Cost limit exceeded: {e.used}/{e.limit}")
    ctx.done()
    return
except LLMError as e:
    # Recoverable: log and continue
    ctx.log(f"LLM error ({e.code}): {e.message}")
    # Maybe retry or use fallback
```

### Supported Providers

| Provider | Environment Variable | Default Model |
|----------|---------------------|---------------|
| `openrouter` | `OPENROUTER_API_KEY` | `anthropic/claude-3.5-sonnet` |
| `chutes` | `CHUTES_API_KEY` | `deepseek-ai/DeepSeek-V3` |
| `openai` | `OPENAI_API_KEY` | `gpt-4o-mini` |
| `anthropic` | `ANTHROPIC_API_KEY` | `claude-3-5-sonnet-20241022` |
| `grok` | `GROK_API_KEY` | `grok-2-latest` |

### Usage Statistics

```python
# Get overall stats
stats = self.llm.get_stats()
print(f"Total tokens: {stats['total_tokens']}")
print(f"Total cost: ${stats['total_cost']:.4f}")
print(f"Requests: {stats['request_count']}")

# Per-model stats
model_stats = self.llm.get_stats("gpt-4o-mini")
```

## Common Patterns

### Pattern 1: Explore-Plan-Execute

```python
def run(self, ctx: AgentContext):
    # 1. Explore the environment
    result = ctx.shell("ls -la")
    files = result.stdout
    
    result = ctx.shell("cat README.md 2>/dev/null || echo 'No README'")
    readme = result.stdout
    
    # 2. Plan with LLM
    plan = self.llm.ask(
        f"Task: {ctx.instruction}\n\nFiles:\n{files}\n\nREADME:\n{readme}\n\n"
        "What steps should I take? List them.",
        system="You are a planning assistant."
    )
    ctx.log(f"Plan: {plan.text[:200]}")
    
    # 3. Execute the plan
    # ... implementation
    
    ctx.done()
```

### Pattern 2: LLM Execution Loop

```python
SYSTEM = """You are a terminal agent. Respond with JSON:
{"thinking": "...", "command": "...", "task_complete": false}"""

def run(self, ctx: AgentContext):
    messages = [
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": ctx.instruction}
    ]
    
    while ctx.step < 100:  # Limit to 100 steps
        response = self.llm.chat(messages)
        data = response.json()
        
        if not data:
            ctx.log("Failed to parse response")
            break
        
        if data.get("task_complete"):
            break
        
        cmd = data.get("command")
        if cmd:
            result = ctx.shell(cmd)
            messages.append({"role": "assistant", "content": response.text})
            messages.append({"role": "user", "content": f"Output:\n{result.output[-3000:]}"})
    
    ctx.done()
```

### Pattern 3: Error Recovery

```python
def run(self, ctx: AgentContext):
    max_retries = 3
    
    for attempt in range(max_retries):
        result = ctx.shell("npm install")
        
        if result.ok:
            ctx.log("npm install succeeded")
            break
        
        if "ECONNREFUSED" in result.output:
            ctx.log(f"Network error, retrying ({attempt + 1}/{max_retries})")
            ctx.shell("sleep 5")
            continue
        
        if "permission denied" in result.output.lower():
            ctx.log("Trying with sudo")
            result = ctx.shell("sudo npm install")
            if result.ok:
                break
        
        ctx.log(f"Attempt {attempt + 1} failed: {result.stderr[:100]}")
    
    ctx.done()
```

### Pattern 4: Multi-Step with State

```python
def run(self, ctx: AgentContext):
    # Track state across steps
    state = {
        "files_found": [],
        "errors": [],
        "completed_steps": []
    }
    
    # Step 1: Find files
    result = ctx.shell("find . -name '*.py'")
    state["files_found"] = result.stdout.strip().split("\n")
    state["completed_steps"].append("find_files")
    
    # Step 2: Process each file
    for file in state["files_found"]:
        result = ctx.shell(f"python -m py_compile {file}")
        if result.failed:
            state["errors"].append(f"{file}: {result.stderr}")
    state["completed_steps"].append("compile_check")
    
    # Step 3: Report
    if state["errors"]:
        ctx.log(f"Found {len(state['errors'])} errors")
        for err in state["errors"][:5]:
            ctx.log(f"  - {err}")
    else:
        ctx.log("All files compile successfully")
    
    ctx.done()
```

### Pattern 5: Streaming with Progress

```python
def run(self, ctx: AgentContext):
    ctx.log("Thinking...")
    
    full_response = ""
    def on_chunk(chunk):
        nonlocal full_response
        full_response += chunk
        # Show progress indicator
        if len(full_response) % 100 == 0:
            ctx.log(f"  ... {len(full_response)} chars")
        return True  # Continue streaming
    
    self.llm.ask_stream(
        ctx.instruction,
        system="Solve this terminal task step by step.",
        on_chunk=on_chunk
    )
    
    ctx.log(f"Got response: {len(full_response)} chars")
    # Process full_response...
    
    ctx.done()
```

## Best Practices

### 1. Always Call `ctx.done()`

Every code path should eventually call `ctx.done()`:

```python
def run(self, ctx: AgentContext):
    try:
        # ... your logic ...
        ctx.done()
    except Exception as e:
        ctx.log(f"Error: {e}")
        ctx.done()  # Still call done!
```

### 2. Check Remaining Resources

Avoid running out of steps or time:

```python
while ctx.step < 95 and ctx.elapsed_secs < 270:  # Leave buffer
    # ... do work ...
    pass

ctx.done()
```

### 3. Handle Command Failures

Don't assume commands succeed:

```python
result = ctx.shell("critical_command")
if result.failed:
    ctx.log(f"Command failed: {result.stderr}")
    # Handle the failure appropriately
```

### 4. Truncate Long Outputs

LLM context windows are limited:

```python
# Truncate output for LLM
output_for_llm = result.output[-3000:]  # Last 3000 chars

messages.append({
    "role": "user",
    "content": f"Output (truncated):\n{output_for_llm}"
})
```

### 5. Log Progress

Make debugging easier:

```python
ctx.log(f"Step {ctx.step}: Running {cmd[:50]}...")
result = ctx.shell(cmd)
ctx.log(f"  Exit code: {result.exit_code}, output: {len(result.output)} chars")
```

### 6. Clean Up in `cleanup()`

Release resources properly:

```python
def cleanup(self):
    if hasattr(self, 'llm'):
        self.llm.close()
    # Close any other resources
```

## Debugging

### Enable Verbose Logging

```bash
term bench agent -a ./my_agent.py -t ./task --verbose
```

### Check Agent Logs

After a run, check `results/tasks/<task>/agent.log` for your agent's output.

### Test Locally

Create a minimal test case:

```python
# test_my_agent.py
from my_agent import MyAgent
from term_sdk import AgentContext

agent = MyAgent()
agent.setup()

ctx = AgentContext(instruction="Create a file called test.txt")
agent.run(ctx)

print(f"Steps: {ctx.step}")
print(f"Done: {ctx.is_done}")

agent.cleanup()
```

## Next Steps

- [SDK Reference](sdk-reference.md) - Complete API documentation
- [Submission Guide](submission.md) - Submit to the network
- [Examples](../examples/) - More example agents
