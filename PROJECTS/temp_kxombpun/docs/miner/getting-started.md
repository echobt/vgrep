# Getting Started

This guide will help you create your first Term Challenge agent and run it on the benchmark.

## Prerequisites

Before you begin, ensure you have:

- **Docker** (required - agents run in containers)
- **Rust 1.70+** (to build the CLI)
- **Python 3.10+** (for agent development)
- **LLM API Key** (OpenRouter, Anthropic, OpenAI, or other supported provider)

## Installation

### 1. Clone and Build the CLI

```bash
git clone https://github.com/PlatformNetwork/term-challenge.git
cd term-challenge
cargo build --release

# Add to PATH (optional)
export PATH="$PWD/target/release:$PATH"

# Verify installation
term --version
```

### 2. Install the Python SDK

```bash
pip install -e sdk/python
```

### 3. Download the Benchmark Dataset

```bash
# Download Terminal-Bench 2.0 (91 tasks)
term bench download terminal-bench@2.0

# Verify download
term bench cache
```

## Your First Agent

Create a file called `my_agent.py`:

```python
from term_sdk import Agent, AgentContext, run

class MyAgent(Agent):
    def run(self, ctx: AgentContext):
        """Execute the task."""
        # Log the task we received
        ctx.log(f"Task: {ctx.instruction[:100]}...")
        
        # Explore the environment
        result = ctx.shell("ls -la")
        ctx.log(f"Files found:\n{result.stdout}")
        
        # For this simple example, just create a hello.txt file
        ctx.shell("echo 'Hello, World!' > hello.txt")
        
        # Verify the file was created
        result = ctx.shell("cat hello.txt")
        if result.ok:
            ctx.log(f"Created hello.txt with: {result.stdout}")
        
        # Signal task completion
        ctx.done()

if __name__ == "__main__":
    run(MyAgent())
```

## Test Your Agent

### Run on a Single Task

Start with a simple task to verify your setup:

```bash
term bench agent -a ./my_agent.py \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world \
    --api-key "sk-or-..." \
    -p openrouter \
    -m anthropic/claude-3.5-sonnet
```

**Note:** Replace `sk-or-...` with your actual API key.

### Understanding the Output

```
[2024-01-15 10:30:15] Starting agent evaluation...
[2024-01-15 10:30:16] Task: hello-world
[2024-01-15 10:30:17] Agent step 1: ls -la
[2024-01-15 10:30:17] Agent step 2: echo 'Hello, World!' > hello.txt
[2024-01-15 10:30:18] Agent step 3: cat hello.txt
[2024-01-15 10:30:18] Agent completed in 3 steps
[2024-01-15 10:30:19] Running verification...
[2024-01-15 10:30:20] Result: PASS

Summary:
  Tasks: 1
  Passed: 1
  Failed: 0
  Score: 100.0%
```

### Run on Multiple Tasks

Once your agent works on one task, test on more:

```bash
# Run on first 10 tasks
term bench agent -a ./my_agent.py \
    -d terminal-bench@2.0 \
    --api-key "sk-or-..." \
    --max-tasks 10
```

### Run the Full Benchmark

```bash
# Run on all 91 tasks
term bench agent -a ./my_agent.py \
    -d terminal-bench@2.0 \
    --api-key "sk-or-..."

# Run with parallelism (faster, uses more API quota)
term bench agent -a ./my_agent.py \
    -d terminal-bench@2.0 \
    --api-key "sk-or-..." \
    --concurrent 4
```

## Adding LLM Intelligence

A simple agent can only do so much. Let's add LLM-powered reasoning:

```python
from term_sdk import Agent, AgentContext, LLM, run

SYSTEM_PROMPT = """You are a terminal agent. Given a task, respond with JSON:
{
    "thinking": "your reasoning",
    "command": "shell command to run",
    "task_complete": false
}

When the task is done, set task_complete to true."""

class SmartAgent(Agent):
    def setup(self):
        """Initialize the LLM client."""
        self.llm = LLM(
            provider="openrouter",
            default_model="anthropic/claude-3.5-sonnet"
        )
    
    def run(self, ctx: AgentContext):
        """Execute the task using LLM reasoning."""
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Task: {ctx.instruction}"}
        ]
        
        while ctx.step < 100:  # Limit to 100 steps
            # Ask the LLM what to do
            response = self.llm.chat(messages)
            data = response.json()
            
            if not data:
                ctx.log("Failed to parse LLM response")
                break
            
            ctx.log(f"Thinking: {data.get('thinking', '')[:100]}")
            
            # Check if task is complete
            if data.get("task_complete"):
                ctx.log("Task marked complete by LLM")
                break
            
            # Execute the command
            cmd = data.get("command")
            if cmd:
                result = ctx.shell(cmd)
                
                # Add the exchange to conversation history
                messages.append({"role": "assistant", "content": response.text})
                messages.append({
                    "role": "user",
                    "content": f"Command output (exit code {result.exit_code}):\n{result.output[-2000:]}"
                })
        
        ctx.done()
    
    def cleanup(self):
        """Clean up resources."""
        self.llm.close()

if __name__ == "__main__":
    run(SmartAgent())
```

## CLI Options Reference

| Option | Short | Description |
|--------|-------|-------------|
| `--agent` | `-a` | Path to your agent Python file |
| `--task` | `-t` | Path to a single task directory |
| `--dataset` | `-d` | Dataset name (e.g., `terminal-bench@2.0`) |
| `--api-key` | | LLM API key (required) |
| `--provider` | `-p` | LLM provider (openrouter, anthropic, etc.) |
| `--model` | `-m` | Model name |
| `--concurrent` | | Number of parallel tasks |
| `--max-tasks` | | Limit number of tasks to run |
| `--timeout` | | Per-task timeout in seconds |
| `--output` | `-o` | Output directory for results |
| `--verbose` | `-v` | Enable verbose logging |

## Environment Variables

Your agent receives these environment variables inside the container:

| Variable | Description |
|----------|-------------|
| `LLM_PROVIDER` | Provider name (openrouter, anthropic, etc.) |
| `LLM_MODEL` | Model name |
| `LLM_API_KEY` | API key for the provider |
| `OPENROUTER_API_KEY` | Also set if provider is openrouter |
| `AGENT_PORT` | HTTP server port (8765) |

## Results Structure

After running, results are saved to `./results/` (or your `--output` directory):

```
results/
├── summary.json          # Overall benchmark results
├── tasks/
│   ├── hello-world/
│   │   ├── result.json   # Task result (pass/fail, steps, timing)
│   │   ├── agent.log     # Agent stdout/stderr
│   │   └── commands.json # Commands executed
│   └── ...
└── report.html           # Human-readable report
```

## Troubleshooting

### "Docker not found"

Ensure Docker is installed and running:

```bash
docker --version
docker ps
```

### "API key required"

The `--api-key` flag is mandatory. Get a key from:
- [OpenRouter](https://openrouter.ai/) (recommended - access to many models)
- [Anthropic](https://console.anthropic.com/)
- [OpenAI](https://platform.openai.com/)

### "Task timeout"

Your agent took too long. Common causes:
- Infinite loop in agent logic
- LLM taking too long to respond
- Commands that hang (e.g., waiting for input)

Increase timeout with `--timeout 600` or optimize your agent.

### "Permission denied"

Ensure your agent file is readable:

```bash
chmod +r my_agent.py
```

## Next Steps

- [Agent Development Guide](agent-development.md) - Deep dive into the SDK
- [SDK Reference](sdk-reference.md) - Complete API documentation
- [Submission Guide](submission.md) - Submit your agent to the network
- [Examples](../examples/) - More agent examples
