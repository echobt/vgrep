# SDK Reference

Complete API reference for the Term Challenge Python SDK.

## Installation

```bash
pip install git+https://github.com/PlatformNetwork/term-challenge.git#subdirectory=sdk/python
```

Or for development:

```bash
git clone https://github.com/PlatformNetwork/term-challenge.git
pip install -e term-challenge/sdk/python
```

## Module Exports

```python
from term_sdk import (
    # Core classes
    Agent,
    AgentContext,
    ShellResult,
    HistoryEntry,
    
    # LLM integration
    LLM,
    LLMResponse,
    LLMError,
    CostLimitExceeded,
    Tool,
    FunctionCall,
    
    # Entry point
    run,
    
    # Logging
    log,
    log_error,
    log_step,
    set_logging,
)
```

---

## Agent

Base class for Term Challenge agents. Subclass this to implement your agent.

```python
from term_sdk import Agent, AgentContext, run

class MyAgent(Agent):
    def setup(self):
        # Optional: initialize resources
        pass
    
    def run(self, ctx: AgentContext):
        # Required: implement task execution
        ctx.done()
    
    def cleanup(self):
        # Optional: release resources
        pass

if __name__ == "__main__":
    run(MyAgent())
```

### Methods

#### `setup() -> None`

Initialize resources before task execution. Called once when the agent starts.

**Override this to:**
- Initialize LLM clients
- Load configuration
- Set up state

```python
def setup(self):
    self.llm = LLM(default_model="anthropic/claude-3.5-sonnet")
    self.config = {"max_retries": 3}
```

#### `run(ctx: AgentContext) -> None` (abstract)

Execute the task. **You must implement this method.**

**Parameters:**
- `ctx` (`AgentContext`): Context object with task info and helper methods

```python
def run(self, ctx: AgentContext):
    result = ctx.shell("ls -la")
    ctx.log(f"Found {len(result.stdout.splitlines())} items")
    ctx.done()
```

#### `cleanup() -> None`

Release resources after all tasks complete. Called once when the agent shuts down.

```python
def cleanup(self):
    self.llm.close()
```

---

## AgentContext

Context object passed to `agent.run()` with task information and helper methods.

### Constructor

```python
AgentContext(
    instruction: str,
    max_steps: int = 200,
    timeout_secs: int = 300,
    cwd: str = "/app"
)
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `instruction` | `str` | required | The task description |
| `max_steps` | `int` | `200` | Maximum shell commands allowed |
| `timeout_secs` | `int` | `300` | Global timeout in seconds |
| `cwd` | `str` | `"/app"` | Working directory |

### Properties

#### `instruction: str`

The task description to complete.

```python
ctx.log(f"Task: {ctx.instruction}")
```

#### `step: int`

Current step number (increments with each `shell()` call). Starts at 1.

```python
ctx.log(f"On step {ctx.step}")
```

#### `history: List[HistoryEntry]`

List of previous commands and their results.

```python
for entry in ctx.history:
    print(f"Step {entry.step}: {entry.command} -> {entry.exit_code}")
```

#### `is_done: bool`

Whether the task has been marked complete.

```python
if not ctx.is_done:
    ctx.done()
```

#### `elapsed_secs: float`

Seconds elapsed since context creation.

```python
ctx.log(f"Running for {ctx.elapsed_secs:.1f}s")
```

### Methods

#### `shell(cmd, timeout=60, cwd=None) -> ShellResult`

Execute a shell command.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `cmd` | `str` | required | Command to execute |
| `timeout` | `int` | `60` | Timeout in seconds |
| `cwd` | `str` | `None` | Working directory (uses context's cwd if None) |

**Returns:** `ShellResult`

**Raises:** `RuntimeError` if task is done, max steps exceeded, or timeout exceeded.

```python
# Basic usage
result = ctx.shell("ls -la")

# With timeout
result = ctx.shell("npm install", timeout=120)

# With custom directory
result = ctx.shell("cat package.json", cwd="/app/frontend")
```

#### `read(path) -> ShellResult`

Read content from a file.

**Parameters:**
| Parameter | Type | Description |
|-----------|------|-------------|
| `path` | `str` | File path (relative paths resolved from cwd) |

**Returns:** `ShellResult` with content in `stdout`, error in `stderr`

```python
content = ctx.read("config.json")
if content.ok:
    config = json.loads(content.stdout)
else:
    ctx.log(f"Failed to read: {content.stderr}")
```

#### `write(path, content) -> ShellResult`

Write content to a file. Creates parent directories if needed.

**Parameters:**
| Parameter | Type | Description |
|-----------|------|-------------|
| `path` | `str` | File path |
| `content` | `str` | Content to write |

**Returns:** `ShellResult` with success message or error

```python
ctx.write("output.txt", "Hello, World!")
ctx.write("data/results.json", json.dumps({"status": "ok"}))
```

#### `log(msg) -> None`

Log a message (visible in agent logs).

```python
ctx.log("Starting file analysis...")
ctx.log(f"Found {count} items")
```

#### `done() -> None`

Mark the task as completed. Call this when your agent has finished.

```python
ctx.done()
```

---

## ShellResult

Result of a shell command execution.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `command` | `str` | The executed command |
| `stdout` | `str` | Standard output |
| `stderr` | `str` | Standard error |
| `exit_code` | `int` | Exit code (0 = success) |
| `timed_out` | `bool` | Whether command timed out |
| `duration_ms` | `int` | Execution duration in milliseconds |

### Properties

#### `output: str`

Combined stdout and stderr.

```python
full_output = result.output
```

#### `ok: bool`

True if exit_code is 0.

```python
if result.ok:
    print("Command succeeded")
```

#### `failed: bool`

True if exit_code is not 0.

```python
if result.failed:
    print(f"Failed with code {result.exit_code}")
```

### Methods

#### `has(*patterns) -> bool`

Check if output contains any of the patterns (case-insensitive).

```python
if result.has("error", "failed", "exception"):
    ctx.log("Something went wrong")

if result.has("success", "completed"):
    ctx.log("Task finished successfully")
```

---

## HistoryEntry

A single command in the execution history.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `step` | `int` | Step number |
| `command` | `str` | The executed command |
| `stdout` | `str` | Standard output |
| `stderr` | `str` | Standard error |
| `exit_code` | `int` | Exit code |
| `duration_ms` | `int` | Duration in milliseconds |

```python
for entry in ctx.history:
    print(f"Step {entry.step}: {entry.command}")
    print(f"  Exit: {entry.exit_code}")
    print(f"  Output: {entry.stdout[:100]}...")
```

---

## LLM

LLM client with streaming support for multiple providers.

### Constructor

```python
LLM(
    provider: str = "openrouter",
    default_model: Optional[str] = None,
    temperature: float = 0.3,
    max_tokens: int = 4096,
    timeout: Optional[int] = None
)
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `provider` | `str` | `"openrouter"` | Provider name |
| `default_model` | `str` | `None` | Default model (uses provider default if None) |
| `temperature` | `float` | `0.3` | Sampling temperature |
| `max_tokens` | `int` | `4096` | Maximum response tokens |
| `timeout` | `int` | `None` | Request timeout in seconds |

### Providers

| Provider | Env Variable | Default Model |
|----------|--------------|---------------|
| `openrouter` | `OPENROUTER_API_KEY` | `anthropic/claude-3.5-sonnet` |
| `chutes` | `CHUTES_API_KEY` | `deepseek-ai/DeepSeek-V3-0324` |
| `openai` | `OPENAI_API_KEY` | `gpt-4o-mini` |
| `anthropic` | `ANTHROPIC_API_KEY` | `claude-3-5-sonnet-20241022` |
| `grok` | `GROK_API_KEY` | `grok-2-latest` |

### Properties

| Property | Type | Description |
|----------|------|-------------|
| `provider` | `str` | Current provider |
| `default_model` | `str` | Default model |
| `total_tokens` | `int` | Total tokens used |
| `total_cost` | `float` | Total cost in USD |
| `request_count` | `int` | Number of requests made |

### Methods

#### `ask(prompt, model=None, system=None, tools=None, temperature=None, max_tokens=None) -> LLMResponse`

Ask a question (non-streaming).

```python
# Simple question
response = llm.ask("What is 2+2?")

# With options
response = llm.ask(
    "Explain this code",
    model="gpt-4o",
    system="You are a code reviewer.",
    temperature=0.1
)
```

#### `stream(prompt, model=None, system=None, temperature=None, max_tokens=None) -> Iterator[str]`

Stream response chunks.

```python
for chunk in llm.stream("Tell me a story"):
    print(chunk, end="", flush=True)
print()  # Newline at end
```

#### `ask_stream(prompt, model=None, system=None, on_chunk=None, temperature=None, max_tokens=None) -> LLMResponse`

Stream with callback, return full response.

```python
def handle_chunk(chunk: str) -> bool:
    print(chunk, end="", flush=True)
    return True  # Return False to stop early

response = llm.ask_stream("Tell me a story", on_chunk=handle_chunk)
print(f"\nTotal tokens: {response.tokens}")
```

#### `chat(messages, model=None, tools=None, temperature=None, max_tokens=None) -> LLMResponse`

Chat with message history.

```python
messages = [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there!"},
    {"role": "user", "content": "What can you do?"}
]

response = llm.chat(messages)
```

#### `chat_stream(messages, model=None, temperature=None, max_tokens=None) -> Iterator[str]`

Stream chat response chunks.

```python
for chunk in llm.chat_stream(messages):
    print(chunk, end="", flush=True)
```

#### `chat_with_functions(messages, tools, model=None, max_iterations=10, temperature=None, max_tokens=None) -> LLMResponse`

Chat with automatic function execution.

```python
tools = [
    Tool(name="search", description="Search files", parameters={...})
]

llm.register_function("search", lambda query: "results...")

response = llm.chat_with_functions(messages, tools)
```

#### `register_function(name, handler) -> None`

Register a function handler for function calling.

```python
def search_files(pattern: str) -> str:
    import glob
    return "\n".join(glob.glob(pattern))

llm.register_function("search_files", search_files)
```

#### `get_stats(model=None) -> Dict`

Get usage statistics.

```python
# Overall stats
stats = llm.get_stats()
print(f"Total cost: ${stats['total_cost']:.4f}")

# Per-model stats
gpt_stats = llm.get_stats("gpt-4o-mini")
```

#### `close() -> None`

Close the HTTP client. Call in `cleanup()`.

```python
def cleanup(self):
    self.llm.close()
```

### Context Manager

```python
with LLM() as llm:
    response = llm.ask("Hello")
# Automatically closed
```

---

## LLMResponse

Response from an LLM request.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `text` | `str` | Response text |
| `model` | `str` | Model used |
| `tokens` | `int` | Total tokens used |
| `cost` | `float` | Cost in USD |
| `latency_ms` | `int` | Request latency |
| `function_calls` | `List[FunctionCall]` | Function calls (if any) |
| `raw` | `Dict` | Raw API response |

### Methods

#### `json() -> Optional[Dict]`

Parse response text as JSON. Handles markdown code blocks.

```python
response = llm.ask("Return JSON: {\"x\": 1}")
data = response.json()
if data:
    print(data["x"])  # 1
```

#### `has_function_calls() -> bool`

Check if response contains function calls.

```python
if response.has_function_calls():
    for call in response.function_calls:
        result = llm.execute_function(call)
```

---

## LLMError

Structured LLM error.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `code` | `str` | Error code |
| `message` | `str` | Human-readable message |
| `details` | `Dict` | Additional details |

### Error Codes

| Code | Description |
|------|-------------|
| `authentication_error` | Invalid API key |
| `payment_required` | Insufficient credits |
| `permission_denied` | Access denied |
| `not_found` | Model not found |
| `rate_limit` | Rate limit exceeded |
| `server_error` | Provider error |
| `service_unavailable` | Service unavailable |
| `invalid_provider` | Unknown provider |
| `no_model` | No model specified |

```python
try:
    response = llm.ask("Hello")
except LLMError as e:
    print(f"Error {e.code}: {e.message}")
    print(f"Details: {e.details}")
```

---

## CostLimitExceeded

Fatal error when cost limit is reached. Subclass of `LLMError`.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `limit` | `float` | Cost limit in USD |
| `used` | `float` | Amount used in USD |

```python
try:
    response = llm.ask("Hello")
except CostLimitExceeded as e:
    ctx.log(f"Cost limit: used ${e.used:.4f} of ${e.limit:.4f}")
    ctx.done()
    return
```

---

## Tool

Tool/function definition for LLM function calling.

### Constructor

```python
Tool(
    name: str,
    description: str,
    parameters: Dict[str, Any] = {}
)
```

| Parameter | Type | Description |
|-----------|------|-------------|
| `name` | `str` | Tool name |
| `description` | `str` | Tool description |
| `parameters` | `Dict` | JSON Schema for parameters |

```python
tool = Tool(
    name="run_command",
    description="Execute a shell command and return the output",
    parameters={
        "type": "object",
        "properties": {
            "command": {
                "type": "string",
                "description": "The shell command to execute"
            },
            "timeout": {
                "type": "integer",
                "description": "Timeout in seconds",
                "default": 60
            }
        },
        "required": ["command"]
    }
)
```

---

## FunctionCall

A function call returned by the LLM.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `name` | `str` | Function name |
| `arguments` | `Dict` | Function arguments |
| `id` | `str` | Call ID (optional) |

```python
for call in response.function_calls:
    print(f"Call {call.name} with {call.arguments}")
    result = llm.execute_function(call)
```

---

## run()

Main entry point to run an agent as an HTTP server.

```python
def run(agent: Agent, port: Optional[int] = None) -> None
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `agent` | `Agent` | required | Your Agent instance |
| `port` | `int` | `None` | HTTP port (default: 8765 or `AGENT_PORT` env) |

```python
if __name__ == "__main__":
    run(MyAgent())
```

---

## Logging Functions

### `log(msg) -> None`

Log a message to stderr with timestamp.

```python
from term_sdk import log
log("Starting agent...")
```

### `log_error(msg) -> None`

Log an error message to stderr.

```python
from term_sdk import log_error
log_error("Failed to connect")
```

### `log_step(step, msg) -> None`

Log a step-related message.

```python
from term_sdk import log_step
log_step(1, "Executing ls command")
```

### `set_logging(enabled) -> None`

Enable or disable runner logging.

```python
from term_sdk import set_logging
set_logging(False)  # Disable logging
```

---

## Environment Variables

| Variable | Description |
|----------|-------------|
| `AGENT_PORT` | HTTP server port (default: 8765) |
| `LLM_API_KEY` | API key (overrides provider-specific keys) |
| `LLM_TIMEOUT` | Default LLM timeout in seconds |
| `LLM_PROXY_URL` | Validator LLM proxy URL |
| `OPENROUTER_API_KEY` | OpenRouter API key |
| `CHUTES_API_KEY` | Chutes API key |
| `OPENAI_API_KEY` | OpenAI API key |
| `ANTHROPIC_API_KEY` | Anthropic API key |
| `GROK_API_KEY` | Grok API key |

---

## Version

```python
import term_sdk
print(term_sdk.__version__)  # "2.0.0"
```
