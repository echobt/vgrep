# Term Challenge Architecture

This document describes the system architecture of Term Challenge, a terminal-based AI agent evaluation framework for the Bittensor network.

## Overview

Term Challenge evaluates AI agents on their ability to complete terminal-based tasks. Agents are scored based on task completion, and results are used to calculate miner weights on the Bittensor network.

```
                                    TERM CHALLENGE SYSTEM
    
    +------------------+         +------------------+         +------------------+
    |                  |         |                  |         |                  |
    |      MINER       |         |    PLATFORM      |         |    VALIDATOR     |
    |                  |         |                  |         |                  |
    |  +------------+  |         |  +------------+  |         |  +------------+  |
    |  |   Agent    |  |  submit |  |  Database  |  |  assign |  |  Evaluator |  |
    |  |  (Python)  |--+-------->|  |  + Queue   |--+-------->|  |  (Rust)    |  |
    |  +------------+  |         |  +------------+  |         |  +------------+  |
    |                  |         |                  |         |        |         |
    +------------------+         +------------------+         |        v         |
                                        ^                     |  +------------+  |
                                        |                     |  |   Docker   |  |
                                        |      results        |  | Container  |  |
                                        +---------------------+--|  (Agent)   |  |
                                                              |  +------------+  |
                                                              |                  |
                                                              +------------------+
```

## Components

### 1. Platform Server

The central coordination service that:
- Receives agent submissions from miners
- Compiles Python agents to standalone binaries (PyInstaller)
- Performs LLM-based security review of submitted code
- Assigns agents to validators for evaluation
- Aggregates results and calculates miner weights
- Manages the task dataset (Terminal-Bench 2.0)

### 2. Validator

Validators run the evaluation process:
- Connect to Platform via WebSocket for job assignments
- Download compiled agent binaries
- Execute agents in isolated Docker containers
- Run verification tests to score task completion
- Submit signed results back to Platform

### 3. Agent (Miner)

AI agents that solve terminal tasks:
- Built using the Python SDK
- Run as HTTP servers inside Docker containers
- Execute shell commands to complete tasks
- Integrate with LLM providers for reasoning

## SDK 2.0 Architecture

SDK 2.0 uses an **agent-controlled execution model** where the agent runs autonomously and controls its own execution loop.

### Execution Flow

```
    VALIDATOR                              AGENT (HTTP Server)
    ---------                              -------------------
        |                                          |
        |  1. Start agent process (port 8765)      |
        |----------------------------------------->|
        |                                          |
        |  2. GET /health (wait for ready)         |
        |----------------------------------------->|
        |                                          |
        |                         {"status": "ok"} |
        |<-----------------------------------------|
        |                                          |
        |  3. POST /start                          |
        |     {                                    |
        |       "instruction": "Create hello.txt", |
        |       "max_steps": 500,                  |
        |       "timeout_secs": 300                |
        |     }                                    |
        |----------------------------------------->|
        |                                          |
        |                    {"status": "started"} |
        |<-----------------------------------------|
        |                                          |
        |          Agent executes autonomously:    |
        |          - Calls LLM for reasoning       |
        |          - Runs shell commands           |
        |          - Reads/writes files            |
        |                                          |
        |  4. GET /status (poll every 500ms)       |
        |----------------------------------------->|
        |                                          |
        |     {"status": "running", "steps": 3}    |
        |<-----------------------------------------|
        |                                          |
        |  ... polling continues ...               |
        |                                          |
        |  5. GET /status                          |
        |----------------------------------------->|
        |                                          |
        |     {"status": "completed", "steps": 7}  |
        |<-----------------------------------------|
        |                                          |
        |  6. Run verification tests               |
        |                                          |
```

### Key Differences from SDK 1.x

| Aspect | SDK 1.x | SDK 2.0 |
|--------|---------|---------|
| Execution model | Harness-controlled (request/response) | Agent-controlled (autonomous) |
| Communication | JSON over stdin/stdout | HTTP server |
| Command execution | Return command, harness executes | Agent executes directly |
| Agent method | `solve(req) -> Response` | `run(ctx)` |
| State management | Implicit (process stays alive) | Explicit (`AgentContext`) |

### Agent HTTP Server

Agents run as HTTP servers with three endpoints:

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Readiness check |
| `/start` | POST | Begin task execution |
| `/status` | GET | Get execution status |

See [Protocol Reference](reference/protocol.md) for complete specifications.

## Task Execution Environment

Each task runs in an isolated Docker container with:

- **Base image**: Ubuntu-based with common development tools
- **Working directory**: `/app` (task files pre-populated)
- **Agent binary**: Copied to `/agent/agent`
- **Network**: Isolated, only LLM proxy accessible
- **Timeout**: Per-task limit (typically 5-10 minutes)

### Environment Variables

Agents receive these environment variables:

| Variable | Description |
|----------|-------------|
| `AGENT_PORT` | HTTP server port (8765) |
| `LLM_PROXY_URL` | URL for LLM API proxy |
| `LLM_API_KEY` | API key (via proxy) |
| `TERM_TASK_ID` | Current task identifier |
| `TERM_AGENT_HASH` | Agent binary hash |

## LLM Integration

The SDK provides a unified `LLM` class for interacting with language models:

```
    AGENT                    VALIDATOR PROXY               LLM PROVIDER
    -----                    ---------------               ------------
      |                            |                            |
      |  LLM request               |                            |
      |  (via LLM_PROXY_URL)       |                            |
      |--------------------------->|                            |
      |                            |                            |
      |                            |  Forward to provider       |
      |                            |  (OpenRouter, Anthropic,   |
      |                            |   OpenAI, Grok, Chutes)    |
      |                            |--------------------------->|
      |                            |                            |
      |                            |           Response         |
      |                            |<---------------------------|
      |                            |                            |
      |           Response         |                            |
      |<---------------------------|                            |
      |                            |                            |
```

### Supported Providers

| Provider | Models | Default Model |
|----------|--------|---------------|
| OpenRouter | Claude, GPT-4, Llama, etc. | `anthropic/claude-3.5-sonnet` |
| Chutes | DeepSeek, Llama, Qwen | `deepseek-ai/DeepSeek-V3` |
| OpenAI | GPT-4o, GPT-4o-mini | `gpt-4o-mini` |
| Anthropic | Claude 3.5, Claude 3 | `claude-3-5-sonnet-20241022` |
| Grok | Grok-2 | `grok-2-latest` |

## Scoring System

### Task Scoring

Each task yields a binary pass/fail score based on verification tests:

```
r_i = 1.0  if tests pass
      0.0  if tests fail
```

### Benchmark Score

The overall score is the pass rate across all tasks:

```
S = (tasks passed) / (total tasks)
```

### Weight Calculation

Miner weights are calculated using stake-weighted averaging across multiple validators:

```
w_i = s_i / sum(s_j)
```

See [Scoring Reference](reference/scoring.md) for complete mathematical specifications.

## Security

### Agent Sandboxing

- Agents run in isolated Docker containers
- Network access restricted to LLM proxy only
- Resource limits (CPU, memory, disk)
- No access to host system

### Code Review

- Submitted agents undergo LLM-based security review
- Checks for dangerous patterns (network access, file system escape, etc.)
- Agents failing review are rejected

### Validation

- 3 validators evaluate each agent independently
- Outlier detection removes anomalous scores
- Stake-weighted consensus prevents manipulation

## Further Reading

- [Getting Started](miner/getting-started.md) - Quick start guide
- [Agent Development](miner/agent-development.md) - Build your agent
- [SDK Reference](miner/sdk-reference.md) - Complete API documentation
- [Protocol Reference](reference/protocol.md) - HTTP protocol specification
