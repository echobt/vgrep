//! Platform LLM Client - All LLM requests go through platform-server
//!
//! This module replaces direct LLM API calls with centralized requests
//! through platform-server, which handles:
//! - API key lookup per agent
//! - Cost tracking
//! - Provider routing

use anyhow::{anyhow, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tracing::{debug, error, info};

/// Platform LLM client configuration
#[derive(Debug, Clone)]
pub struct PlatformLlmConfig {
    /// Platform server URL
    pub platform_url: String,
    /// Agent hash (to identify which miner's API key to use)
    pub agent_hash: String,
    /// Validator hotkey (for audit)
    pub validator_hotkey: String,
    /// Model to use (optional)
    pub model: Option<String>,
    /// Max tokens
    pub max_tokens: u32,
    /// Temperature
    pub temperature: f32,
    /// Timeout in seconds
    pub timeout_secs: u64,
}

impl Default for PlatformLlmConfig {
    fn default() -> Self {
        Self {
            platform_url: std::env::var("PLATFORM_URL")
                .unwrap_or_else(|_| "https://chain.platform.network".to_string()),
            agent_hash: String::new(),
            validator_hotkey: String::new(),
            model: None,
            max_tokens: 4096,
            temperature: 0.7,
            timeout_secs: 120,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

impl ChatMessage {
    pub fn system(content: &str) -> Self {
        Self {
            role: "system".to_string(),
            content: content.to_string(),
        }
    }

    pub fn user(content: &str) -> Self {
        Self {
            role: "user".to_string(),
            content: content.to_string(),
        }
    }

    pub fn assistant(content: &str) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.to_string(),
        }
    }
}

#[derive(Debug, Serialize)]
struct PlatformLlmRequest {
    agent_hash: String,
    validator_hotkey: String,
    messages: Vec<ChatMessage>,
    model: Option<String>,
    max_tokens: Option<u32>,
    temperature: Option<f32>,
}

#[derive(Debug, Deserialize)]
pub struct PlatformLlmResponse {
    pub success: bool,
    pub content: Option<String>,
    pub model: Option<String>,
    pub usage: Option<LlmUsage>,
    pub cost_usd: Option<f64>,
    pub error: Option<String>,
}

#[derive(Debug, Clone, Deserialize)]
pub struct LlmUsage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

/// Platform LLM client - routes all requests through platform-server
pub struct PlatformLlmClient {
    client: Client,
    config: PlatformLlmConfig,
}

impl PlatformLlmClient {
    pub fn new(config: PlatformLlmConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_secs))
            .build()?;

        Ok(Self { client, config })
    }

    /// Create a new client for a specific agent evaluation
    pub fn for_agent(platform_url: &str, agent_hash: &str, validator_hotkey: &str) -> Result<Self> {
        Self::new(PlatformLlmConfig {
            platform_url: platform_url.to_string(),
            agent_hash: agent_hash.to_string(),
            validator_hotkey: validator_hotkey.to_string(),
            ..Default::default()
        })
    }

    /// Send a chat completion request through platform-server
    pub async fn chat(&self, messages: Vec<ChatMessage>) -> Result<String> {
        let url = format!("{}/api/v1/llm/chat", self.config.platform_url);

        let request = PlatformLlmRequest {
            agent_hash: self.config.agent_hash.clone(),
            validator_hotkey: self.config.validator_hotkey.clone(),
            messages,
            model: self.config.model.clone(),
            max_tokens: Some(self.config.max_tokens),
            temperature: Some(self.config.temperature),
        };

        debug!(
            "Platform LLM request for agent {} via {}",
            &self.config.agent_hash[..16.min(self.config.agent_hash.len())],
            self.config.platform_url
        );

        let response = self
            .client
            .post(&url)
            .json(&request)
            .send()
            .await
            .map_err(|e| anyhow!("Platform LLM request failed: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let text = response.text().await.unwrap_or_default();
            return Err(anyhow!("Platform LLM error {}: {}", status, text));
        }

        let result: PlatformLlmResponse = response
            .json()
            .await
            .map_err(|e| anyhow!("Invalid platform response: {}", e))?;

        if !result.success {
            return Err(anyhow!(
                "Platform LLM failed: {}",
                result.error.unwrap_or_else(|| "Unknown error".to_string())
            ));
        }

        let content = result
            .content
            .ok_or_else(|| anyhow!("No content in response"))?;

        if let Some(usage) = &result.usage {
            info!(
                "LLM response: {} tokens, cost: ${:.4}",
                usage.total_tokens,
                result.cost_usd.unwrap_or(0.0)
            );
        }

        Ok(content)
    }

    /// Send a chat completion and get full response with usage
    pub async fn chat_with_usage(&self, messages: Vec<ChatMessage>) -> Result<PlatformLlmResponse> {
        let url = format!("{}/api/v1/llm/chat", self.config.platform_url);

        let request = PlatformLlmRequest {
            agent_hash: self.config.agent_hash.clone(),
            validator_hotkey: self.config.validator_hotkey.clone(),
            messages,
            model: self.config.model.clone(),
            max_tokens: Some(self.config.max_tokens),
            temperature: Some(self.config.temperature),
        };

        let response = self
            .client
            .post(&url)
            .json(&request)
            .send()
            .await
            .map_err(|e| anyhow!("Platform LLM request failed: {}", e))?;

        if !response.status().is_success() {
            let status = response.status();
            let text = response.text().await.unwrap_or_default();
            return Err(anyhow!("Platform LLM error {}: {}", status, text));
        }

        let result: PlatformLlmResponse = response
            .json()
            .await
            .map_err(|e| anyhow!("Invalid platform response: {}", e))?;

        Ok(result)
    }

    /// Get agent hash
    pub fn agent_hash(&self) -> &str {
        &self.config.agent_hash
    }

    /// Get total cost so far (from last response)
    pub fn platform_url(&self) -> &str {
        &self.config.platform_url
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use httpmock::prelude::*;

    #[test]
    fn test_message_creation() {
        let sys = ChatMessage::system("You are helpful");
        assert_eq!(sys.role, "system");
        assert_eq!(sys.content, "You are helpful");

        let user = ChatMessage::user("Hello");
        assert_eq!(user.role, "user");
        assert_eq!(user.content, "Hello");

        let asst = ChatMessage::assistant("Hi there");
        assert_eq!(asst.role, "assistant");
        assert_eq!(asst.content, "Hi there");
    }

    #[test]
    fn test_config_default() {
        let config = PlatformLlmConfig::default();
        // platform_url uses PLATFORM_URL env var or fallback
        let expected_url = std::env::var("PLATFORM_URL")
            .unwrap_or_else(|_| "https://chain.platform.network".to_string());
        assert_eq!(config.platform_url, expected_url);
        assert_eq!(config.max_tokens, 4096);
        assert!((config.temperature - 0.7).abs() < 0.001);
        assert_eq!(config.timeout_secs, 120);
        assert!(config.agent_hash.is_empty());
        assert!(config.validator_hotkey.is_empty());
        assert!(config.model.is_none());
    }

    #[test]
    fn test_client_new() {
        let config = PlatformLlmConfig {
            platform_url: "http://localhost:8080".to_string(),
            agent_hash: "test_hash".to_string(),
            validator_hotkey: "test_validator".to_string(),
            model: Some("gpt-4".to_string()),
            max_tokens: 2048,
            temperature: 0.5,
            timeout_secs: 60,
        };
        let client = PlatformLlmClient::new(config).unwrap();
        assert_eq!(client.agent_hash(), "test_hash");
        assert_eq!(client.platform_url(), "http://localhost:8080");
    }

    #[test]
    fn test_for_agent() {
        let client =
            PlatformLlmClient::for_agent("http://test.example.com", "agent123", "validator456")
                .unwrap();
        assert_eq!(client.agent_hash(), "agent123");
        assert_eq!(client.platform_url(), "http://test.example.com");
    }

    #[test]
    fn test_agent_hash_getter() {
        let config = PlatformLlmConfig {
            agent_hash: "my_agent_hash".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();
        assert_eq!(client.agent_hash(), "my_agent_hash");
    }

    #[test]
    fn test_platform_url_getter() {
        let config = PlatformLlmConfig {
            platform_url: "http://custom.url".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();
        assert_eq!(client.platform_url(), "http://custom.url");
    }

    #[tokio::test]
    async fn test_chat_success() {
        let server = MockServer::start();

        let mock = server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": true,
                    "content": "Hello! How can I help you?",
                    "model": "gpt-4",
                    "usage": {
                        "prompt_tokens": 10,
                        "completion_tokens": 8,
                        "total_tokens": 18
                    },
                    "cost_usd": 0.0012
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent_hash_12345678".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let messages = vec![
            ChatMessage::system("You are a helpful assistant"),
            ChatMessage::user("Hello"),
        ];

        let result = client.chat(messages).await.unwrap();
        assert_eq!(result, "Hello! How can I help you?");
        mock.assert();
    }

    #[tokio::test]
    async fn test_chat_http_error() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(500).body("Internal Server Error");
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await;
        assert!(result.is_err());
        let err = result.unwrap_err().to_string();
        assert!(err.contains("Platform LLM error"));
        assert!(err.contains("500"));
    }

    #[tokio::test]
    async fn test_chat_invalid_json() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .body("not valid json");
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await;
        assert!(result.is_err());
        assert!(result
            .unwrap_err()
            .to_string()
            .contains("Invalid platform response"));
    }

    #[tokio::test]
    async fn test_chat_api_failure() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": false,
                    "error": "API key invalid"
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await;
        assert!(result.is_err());
        let err = result.unwrap_err().to_string();
        assert!(err.contains("Platform LLM failed"));
        assert!(err.contains("API key invalid"));
    }

    #[tokio::test]
    async fn test_chat_api_failure_unknown_error() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": false
                    // No error field - triggers unwrap_or_else
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await;
        assert!(result.is_err());
        let err = result.unwrap_err().to_string();
        assert!(err.contains("Unknown error"));
    }

    #[tokio::test]
    async fn test_chat_no_content() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": true
                    // No content field
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await;
        assert!(result.is_err());
        assert!(result
            .unwrap_err()
            .to_string()
            .contains("No content in response"));
    }

    #[tokio::test]
    async fn test_chat_with_usage_success() {
        let server = MockServer::start();

        let mock = server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": true,
                    "content": "Test response",
                    "model": "gpt-4",
                    "usage": {
                        "prompt_tokens": 20,
                        "completion_tokens": 15,
                        "total_tokens": 35
                    },
                    "cost_usd": 0.0025
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            model: Some("gpt-4".to_string()),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client
            .chat_with_usage(vec![ChatMessage::user("Test")])
            .await
            .unwrap();
        assert!(result.success);
        assert_eq!(result.content, Some("Test response".to_string()));
        assert_eq!(result.model, Some("gpt-4".to_string()));
        assert!(result.usage.is_some());
        let usage = result.usage.unwrap();
        assert_eq!(usage.prompt_tokens, 20);
        assert_eq!(usage.completion_tokens, 15);
        assert_eq!(usage.total_tokens, 35);
        assert!((result.cost_usd.unwrap() - 0.0025).abs() < 0.0001);
        mock.assert();
    }

    #[tokio::test]
    async fn test_chat_with_usage_http_error() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(403).body("Forbidden");
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client
            .chat_with_usage(vec![ChatMessage::user("Test")])
            .await;
        assert!(result.is_err());
        let err = result.unwrap_err().to_string();
        assert!(err.contains("Platform LLM error"));
        assert!(err.contains("403"));
    }

    #[tokio::test]
    async fn test_chat_with_usage_invalid_json() {
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .body("{broken json}}}");
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client
            .chat_with_usage(vec![ChatMessage::user("Test")])
            .await;
        assert!(result.is_err());
        assert!(result
            .unwrap_err()
            .to_string()
            .contains("Invalid platform response"));
    }

    #[tokio::test]
    async fn test_chat_without_usage_in_response() {
        // Test the branch where usage is None (no info! log)
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": true,
                    "content": "Response without usage"
                    // No usage field
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "test_agent".to_string(),
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await.unwrap();
        assert_eq!(result, "Response without usage");
    }

    #[tokio::test]
    async fn test_chat_with_short_agent_hash() {
        // Test the debug log with short agent hash (< 16 chars)
        let server = MockServer::start();

        server.mock(|when, then| {
            when.method(POST).path("/api/v1/llm/chat");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(serde_json::json!({
                    "success": true,
                    "content": "OK"
                }));
        });

        let config = PlatformLlmConfig {
            platform_url: server.url(""),
            agent_hash: "short".to_string(), // Less than 16 chars
            validator_hotkey: "test_validator".to_string(),
            ..Default::default()
        };
        let client = PlatformLlmClient::new(config).unwrap();

        let result = client.chat(vec![ChatMessage::user("Hi")]).await.unwrap();
        assert_eq!(result, "OK");
    }

    #[test]
    fn test_llm_usage_struct() {
        let usage = LlmUsage {
            prompt_tokens: 100,
            completion_tokens: 50,
            total_tokens: 150,
        };
        assert_eq!(usage.prompt_tokens, 100);
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.total_tokens, 150);

        // Test Clone
        let cloned = usage.clone();
        assert_eq!(cloned.total_tokens, 150);
    }

    #[test]
    fn test_platform_llm_response_struct() {
        let response = PlatformLlmResponse {
            success: true,
            content: Some("test content".to_string()),
            model: Some("gpt-4".to_string()),
            usage: Some(LlmUsage {
                prompt_tokens: 10,
                completion_tokens: 5,
                total_tokens: 15,
            }),
            cost_usd: Some(0.001),
            error: None,
        };
        assert!(response.success);
        assert_eq!(response.content.unwrap(), "test content");
    }

    #[test]
    fn test_chat_message_debug() {
        let msg = ChatMessage::user("test");
        // Test Debug derive
        let debug_str = format!("{:?}", msg);
        assert!(debug_str.contains("user"));
        assert!(debug_str.contains("test"));
    }

    #[test]
    fn test_chat_message_clone() {
        let msg = ChatMessage::system("original");
        let cloned = msg.clone();
        assert_eq!(cloned.role, "system");
        assert_eq!(cloned.content, "original");
    }

    #[test]
    fn test_platform_llm_config_clone() {
        let config = PlatformLlmConfig {
            platform_url: "http://test".to_string(),
            agent_hash: "hash".to_string(),
            validator_hotkey: "key".to_string(),
            model: Some("model".to_string()),
            max_tokens: 1000,
            temperature: 0.5,
            timeout_secs: 30,
        };
        let cloned = config.clone();
        assert_eq!(cloned.platform_url, "http://test");
        assert_eq!(cloned.agent_hash, "hash");
        assert_eq!(cloned.model, Some("model".to_string()));
    }

    #[test]
    fn test_platform_llm_config_debug() {
        let config = PlatformLlmConfig::default();
        let debug_str = format!("{:?}", config);
        assert!(debug_str.contains("PlatformLlmConfig"));
        assert!(debug_str.contains("platform_url"));
    }
}
