//! Direct LLM API client.
//!
//! Makes direct HTTP requests to LLM providers (OpenRouter, OpenAI).
//! Used for agent execution with configurable API endpoints.

use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tracing::{debug, info};

use crate::task::harness::{AgentRequest, AgentResponse};

/// LLM configuration
#[derive(Debug, Clone)]
pub struct LlmConfig {
    pub api_base: String,
    pub api_key: String,
    pub model: String,
    pub max_tokens: u32,
    pub temperature: f32,
    pub timeout_secs: u64,
}

impl Default for LlmConfig {
    fn default() -> Self {
        Self {
            api_base: std::env::var("LLM_API_BASE")
                .unwrap_or_else(|_| "https://openrouter.ai/api/v1".to_string()),
            api_key: std::env::var("OPENROUTER_API_KEY")
                .or_else(|_| std::env::var("LLM_API_KEY"))
                .or_else(|_| std::env::var("OPENAI_API_KEY"))
                .unwrap_or_default(),
            model: std::env::var("LLM_MODEL")
                .unwrap_or_else(|_| "anthropic/claude-3-haiku".to_string()),
            max_tokens: 2048,
            temperature: 0.3,
            timeout_secs: 120,
        }
    }
}

#[derive(Debug, Serialize)]
struct ChatRequest {
    model: String,
    messages: Vec<Message>,
    max_tokens: u32,
    temperature: f32,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Message {
    pub role: String,
    pub content: String,
}

impl Message {
    pub fn system(content: &str) -> Self {
        Self {
            role: "system".to_string(),
            content: content.to_string(),
        }
    }

    pub fn user(content: &str) -> Self {
        Self {
            role: "user".to_string(),
            content: content.to_string(),
        }
    }

    pub fn assistant(content: &str) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.to_string(),
        }
    }
}

#[derive(Debug, Deserialize)]
struct ChatResponse {
    choices: Vec<Choice>,
}

#[derive(Debug, Deserialize)]
struct Choice {
    message: Message,
}

/// LLM client for API calls
pub struct LlmClient {
    client: Client,
    config: LlmConfig,
}

impl LlmClient {
    pub fn new(config: LlmConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_secs))
            .build()?;

        info!(
            "LLM client initialized: model={}, api_base={}",
            config.model, config.api_base
        );
        Ok(Self { client, config })
    }

    pub fn from_env() -> Result<Self> {
        Self::new(LlmConfig::default())
    }

    fn system_prompt(&self) -> String {
        r#"You are a terminal agent. Execute shell commands to complete tasks.

RESPONSE FORMAT (JSON only):
{"command": "your shell command here", "task_complete": false}

When done:
{"command": null, "task_complete": true}

RULES:
- One command at a time
- You receive the output of each command
- Set task_complete=true only when finished
- Respond with valid JSON only, no other text"#
            .to_string()
    }

    fn build_user_message(&self, req: &AgentRequest) -> String {
        let mut msg = format!(
            "TASK: {}\n\nSTEP: {}\nCWD: {}",
            req.instruction, req.step, req.cwd
        );

        if let Some(cmd) = &req.last_command {
            msg.push_str(&format!("\n\nLAST COMMAND: {}", cmd));
        }
        if let Some(code) = req.exit_code {
            msg.push_str(&format!("\nEXIT CODE: {}", code));
        }
        if let Some(out) = &req.output {
            let truncated = if out.len() > 16000 {
                format!("{}...[truncated]", &out[..16000])
            } else {
                out.clone()
            };
            msg.push_str(&format!("\n\nOUTPUT:\n{}", truncated));
        }

        msg
    }

    /// Execute a single LLM call and get agent response
    pub async fn execute(&self, request: AgentRequest) -> Result<AgentResponse> {
        let messages = vec![
            Message::system(&self.system_prompt()),
            Message::user(&self.build_user_message(&request)),
        ];

        debug!("Calling LLM: step={}", request.step);

        let resp = self
            .client
            .post(format!("{}/chat/completions", self.config.api_base))
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .header("HTTP-Referer", "https://platform.network")
            .json(&ChatRequest {
                model: self.config.model.clone(),
                messages,
                max_tokens: self.config.max_tokens,
                temperature: self.config.temperature,
            })
            .send()
            .await
            .context("LLM request failed")?;

        if !resp.status().is_success() {
            let status = resp.status();
            let err = resp.text().await.unwrap_or_default();
            anyhow::bail!("LLM error ({}): {}", status, err);
        }

        let chat: ChatResponse = resp.json().await?;
        let content = chat
            .choices
            .first()
            .map(|c| c.message.content.clone())
            .unwrap_or_default();

        debug!("LLM response: {}", content);
        crate::task::harness::parse_agent_response(&content)
    }

    /// Chat with conversation history
    pub async fn chat(&self, messages: Vec<Message>) -> Result<String> {
        let resp = self
            .client
            .post(format!("{}/chat/completions", self.config.api_base))
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .header("HTTP-Referer", "https://platform.network")
            .json(&ChatRequest {
                model: self.config.model.clone(),
                messages,
                max_tokens: self.config.max_tokens,
                temperature: self.config.temperature,
            })
            .send()
            .await
            .context("LLM chat request failed")?;

        if !resp.status().is_success() {
            let status = resp.status();
            let err = resp.text().await.unwrap_or_default();
            anyhow::bail!("LLM chat error ({}): {}", status, err);
        }

        let chat: ChatResponse = resp.json().await?;
        Ok(chat
            .choices
            .first()
            .map(|c| c.message.content.clone())
            .unwrap_or_default())
    }
}

// ============================================================================
// REMOVED: SourceCodeAgent
// ============================================================================
// The SourceCodeAgent struct that executed Python on the host has been REMOVED
// for security reasons. All agent code now executes inside Docker containers
// via the evaluator module.
//
// If you need to run agent code, use:
// - TaskEvaluator::evaluate_task() for full task evaluation
// - ContainerRun::inject_agent_code() + start_agent() for direct container execution
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_llm_config_default() {
        let config = LlmConfig::default();
        assert!(!config.api_base.is_empty());
        assert_eq!(config.max_tokens, 2048);
        assert_eq!(config.temperature, 0.3);
        assert_eq!(config.timeout_secs, 120);
    }

    #[test]
    fn test_llm_config_custom() {
        let config = LlmConfig {
            api_base: "https://api.openai.com/v1".to_string(),
            api_key: "test_key".to_string(),
            model: "gpt-4".to_string(),
            max_tokens: 4096,
            temperature: 0.7,
            timeout_secs: 60,
        };

        assert_eq!(config.api_base, "https://api.openai.com/v1");
        assert_eq!(config.api_key, "test_key");
        assert_eq!(config.model, "gpt-4");
        assert_eq!(config.max_tokens, 4096);
        assert_eq!(config.temperature, 0.7);
        assert_eq!(config.timeout_secs, 60);
    }

    #[test]
    fn test_message_system() {
        let msg = Message::system("You are a helpful assistant");
        assert_eq!(msg.role, "system");
        assert_eq!(msg.content, "You are a helpful assistant");
    }

    #[test]
    fn test_message_user() {
        let msg = Message::user("Hello");
        assert_eq!(msg.role, "user");
        assert_eq!(msg.content, "Hello");
    }

    #[test]
    fn test_message_assistant() {
        let msg = Message::assistant("Hi there");
        assert_eq!(msg.role, "assistant");
        assert_eq!(msg.content, "Hi there");
    }

    #[test]
    fn test_message_clone() {
        let msg1 = Message::user("test");
        let msg2 = msg1.clone();
        assert_eq!(msg1.role, msg2.role);
        assert_eq!(msg1.content, msg2.content);
    }

    #[test]
    fn test_llm_client_new() {
        let config = LlmConfig {
            api_base: "https://api.test.com/v1".to_string(),
            api_key: "test_key".to_string(),
            model: "test-model".to_string(),
            max_tokens: 1000,
            temperature: 0.5,
            timeout_secs: 30,
        };

        let client = LlmClient::new(config.clone());
        assert!(client.is_ok());
    }

    #[test]
    fn test_system_prompt_format() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();
        let prompt = client.system_prompt();

        assert!(prompt.contains("terminal agent"));
        assert!(prompt.contains("JSON"));
        assert!(prompt.contains("command"));
        assert!(prompt.contains("task_complete"));
    }

    #[test]
    fn test_build_user_message_basic() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "List files".to_string(),
            step: 1,
            cwd: "/home/user".to_string(),
            last_command: None,
            exit_code: None,
            output: None,
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("List files"));
        assert!(msg.contains("STEP: 1"));
        assert!(msg.contains("/home/user"));
    }

    #[test]
    fn test_build_user_message_with_command() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "Check status".to_string(),
            step: 2,
            cwd: "/tmp".to_string(),
            last_command: Some("ls -la".to_string()),
            exit_code: Some(0),
            output: Some("total 0".to_string()),
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("Check status"));
        assert!(msg.contains("ls -la"));
        assert!(msg.contains("EXIT CODE: 0"));
        assert!(msg.contains("total 0"));
    }

    #[test]
    fn test_build_user_message_truncates_long_output() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let long_output = "x".repeat(20000);
        let req = AgentRequest {
            instruction: "Test".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: None,
            exit_code: None,
            output: Some(long_output),
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("[truncated]"));
        assert!(msg.len() < 20000);
    }

    #[test]
    fn test_chat_request_serialization() {
        let req = ChatRequest {
            model: "gpt-4".to_string(),
            messages: vec![Message::user("test")],
            max_tokens: 100,
            temperature: 0.5,
        };

        let json = serde_json::to_string(&req).unwrap();
        assert!(json.contains("gpt-4"));
        assert!(json.contains("test"));
    }

    #[test]
    fn test_message_serialization() {
        let msg = Message::user("Hello world");
        let json = serde_json::to_string(&msg).unwrap();
        assert!(json.contains("user"));
        assert!(json.contains("Hello world"));
    }

    #[test]
    fn test_message_deserialization() {
        let json = r#"{"role":"assistant","content":"Response"}"#;
        let msg: Message = serde_json::from_str(json).unwrap();
        assert_eq!(msg.role, "assistant");
        assert_eq!(msg.content, "Response");
    }

    #[test]
    fn test_config_debug() {
        let config = LlmConfig::default();
        let debug_str = format!("{:?}", config);
        assert!(debug_str.contains("LlmConfig"));
    }

    #[test]
    fn test_message_empty_content() {
        let msg = Message::user("");
        assert_eq!(msg.content, "");
        assert_eq!(msg.role, "user");
    }

    #[test]
    fn test_config_with_env_fallback() {
        // Test that default config uses environment variables
        let config = LlmConfig::default();
        // Should have some default value even if env vars aren't set
        assert!(!config.model.is_empty());
    }

    #[test]
    fn test_llm_client_from_env() {
        let client = LlmClient::from_env();
        assert!(client.is_ok());
    }

    #[test]
    fn test_llm_config_clone() {
        let config1 = LlmConfig {
            api_base: "https://api.test.com".to_string(),
            api_key: "key123".to_string(),
            model: "model-x".to_string(),
            max_tokens: 512,
            temperature: 0.8,
            timeout_secs: 45,
        };

        let config2 = config1.clone();
        assert_eq!(config1.api_base, config2.api_base);
        assert_eq!(config1.api_key, config2.api_key);
        assert_eq!(config1.model, config2.model);
        assert_eq!(config1.max_tokens, config2.max_tokens);
        assert_eq!(config1.temperature, config2.temperature);
        assert_eq!(config1.timeout_secs, config2.timeout_secs);
    }

    #[test]
    fn test_message_with_special_characters() {
        let msg = Message::user("Hello\nWorld\t\"quoted\"");
        assert_eq!(msg.content, "Hello\nWorld\t\"quoted\"");
        assert_eq!(msg.role, "user");
    }

    #[test]
    fn test_message_debug() {
        let msg = Message::system("test");
        let debug_str = format!("{:?}", msg);
        assert!(debug_str.contains("Message"));
        assert!(debug_str.contains("test"));
    }

    #[test]
    fn test_chat_request_debug() {
        let req = ChatRequest {
            model: "test-model".to_string(),
            messages: vec![],
            max_tokens: 100,
            temperature: 0.5,
        };
        let debug_str = format!("{:?}", req);
        assert!(debug_str.contains("ChatRequest"));
    }

    #[test]
    fn test_build_user_message_with_all_fields() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "Complete task".to_string(),
            step: 5,
            cwd: "/workspace".to_string(),
            last_command: Some("echo hello".to_string()),
            exit_code: Some(1),
            output: Some("error message".to_string()),
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("Complete task"));
        assert!(msg.contains("STEP: 5"));
        assert!(msg.contains("/workspace"));
        assert!(msg.contains("echo hello"));
        assert!(msg.contains("EXIT CODE: 1"));
        assert!(msg.contains("error message"));
    }

    #[test]
    fn test_build_user_message_exact_truncation_boundary() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        // Exactly 16000 characters - should not truncate
        let exact_output = "x".repeat(16000);
        let req = AgentRequest {
            instruction: "Test".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: None,
            exit_code: None,
            output: Some(exact_output.clone()),
        };

        let msg = client.build_user_message(&req);
        assert!(!msg.contains("[truncated]"));
        assert!(msg.contains(&exact_output));
    }

    #[test]
    fn test_build_user_message_just_over_truncation() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        // 16001 characters - should truncate
        let over_output = "x".repeat(16001);
        let req = AgentRequest {
            instruction: "Test".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: None,
            exit_code: None,
            output: Some(over_output),
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("[truncated]"));
    }

    #[test]
    fn test_build_user_message_with_none_exit_code() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "Task".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: Some("cmd".to_string()),
            exit_code: None,
            output: None,
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("LAST COMMAND: cmd"));
        assert!(!msg.contains("EXIT CODE"));
    }

    #[test]
    fn test_build_user_message_zero_exit_code() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "Task".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: Some("cmd".to_string()),
            exit_code: Some(0),
            output: None,
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("EXIT CODE: 0"));
    }

    #[test]
    fn test_system_prompt_contains_rules() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();
        let prompt = client.system_prompt();

        assert!(prompt.contains("RESPONSE FORMAT"));
        assert!(prompt.contains("RULES"));
        assert!(prompt.contains("One command at a time"));
        assert!(prompt.contains("valid JSON only"));
    }

    #[test]
    fn test_chat_request_with_multiple_messages() {
        let req = ChatRequest {
            model: "test".to_string(),
            messages: vec![
                Message::system("sys"),
                Message::user("user"),
                Message::assistant("assist"),
            ],
            max_tokens: 100,
            temperature: 0.5,
        };

        let json = serde_json::to_string(&req).unwrap();
        assert!(json.contains("sys"));
        assert!(json.contains("user"));
        assert!(json.contains("assist"));
    }

    #[test]
    fn test_chat_request_empty_messages() {
        let req = ChatRequest {
            model: "test".to_string(),
            messages: vec![],
            max_tokens: 100,
            temperature: 0.5,
        };

        let json = serde_json::to_string(&req).unwrap();
        assert!(json.contains("test"));
        assert!(json.contains("messages"));
    }

    #[test]
    fn test_message_role_variants() {
        let system = Message::system("s");
        let user = Message::user("u");
        let assistant = Message::assistant("a");

        assert_eq!(system.role, "system");
        assert_eq!(user.role, "user");
        assert_eq!(assistant.role, "assistant");
    }

    #[test]
    fn test_llm_config_default_values() {
        let config = LlmConfig::default();

        assert_eq!(config.max_tokens, 2048);
        assert_eq!(config.temperature, 0.3);
        assert_eq!(config.timeout_secs, 120);
        assert!(!config.api_base.is_empty());
    }

    #[test]
    fn test_llm_config_custom_timeout() {
        let config = LlmConfig {
            api_base: "https://api.test.com".to_string(),
            api_key: "key".to_string(),
            model: "model".to_string(),
            max_tokens: 1000,
            temperature: 0.5,
            timeout_secs: 180,
        };

        assert_eq!(config.timeout_secs, 180);
    }

    #[test]
    fn test_llm_config_zero_temperature() {
        let config = LlmConfig {
            api_base: "https://api.test.com".to_string(),
            api_key: "key".to_string(),
            model: "model".to_string(),
            max_tokens: 1000,
            temperature: 0.0,
            timeout_secs: 60,
        };

        assert_eq!(config.temperature, 0.0);
    }

    #[test]
    fn test_llm_config_high_temperature() {
        let config = LlmConfig {
            api_base: "https://api.test.com".to_string(),
            api_key: "key".to_string(),
            model: "model".to_string(),
            max_tokens: 1000,
            temperature: 1.0,
            timeout_secs: 60,
        };

        assert_eq!(config.temperature, 1.0);
    }

    #[test]
    fn test_message_serialization_format() {
        let msg = Message::user("test content");
        let json = serde_json::to_value(&msg).unwrap();

        assert_eq!(json["role"], "user");
        assert_eq!(json["content"], "test content");
    }

    #[test]
    fn test_message_deserialization_various_roles() {
        let system_json = r#"{"role":"system","content":"System message"}"#;
        let user_json = r#"{"role":"user","content":"User message"}"#;
        let assistant_json = r#"{"role":"assistant","content":"Assistant message"}"#;

        let system: Message = serde_json::from_str(system_json).unwrap();
        let user: Message = serde_json::from_str(user_json).unwrap();
        let assistant: Message = serde_json::from_str(assistant_json).unwrap();

        assert_eq!(system.role, "system");
        assert_eq!(user.role, "user");
        assert_eq!(assistant.role, "assistant");
    }

    #[test]
    fn test_chat_response_deserialization() {
        let json = r#"{
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": "Response text"
                    }
                }
            ]
        }"#;

        let response: ChatResponse = serde_json::from_str(json).unwrap();
        assert_eq!(response.choices.len(), 1);
        assert_eq!(response.choices[0].message.content, "Response text");
        assert_eq!(response.choices[0].message.role, "assistant");
    }

    #[test]
    fn test_chat_response_empty_choices() {
        let json = r#"{"choices": []}"#;
        let response: ChatResponse = serde_json::from_str(json).unwrap();
        assert_eq!(response.choices.len(), 0);
    }

    #[test]
    fn test_build_user_message_multiline_output() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let output = "line1\nline2\nline3";
        let req = AgentRequest {
            instruction: "Task".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: None,
            exit_code: None,
            output: Some(output.to_string()),
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("line1"));
        assert!(msg.contains("line2"));
        assert!(msg.contains("line3"));
    }

    #[test]
    fn test_build_user_message_formats_correctly() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "My task".to_string(),
            step: 3,
            cwd: "/home".to_string(),
            last_command: None,
            exit_code: None,
            output: None,
        };

        let msg = client.build_user_message(&req);
        assert!(msg.starts_with("TASK: My task"));
        assert!(msg.contains("\n\nSTEP: 3"));
        assert!(msg.contains("\nCWD: /home"));
    }

    #[test]
    fn test_message_long_content() {
        let long_content = "a".repeat(10000);
        let msg = Message::user(&long_content);
        assert_eq!(msg.content.len(), 10000);
    }

    #[test]
    fn test_llm_config_empty_api_key() {
        let config = LlmConfig {
            api_base: "https://api.test.com".to_string(),
            api_key: "".to_string(),
            model: "model".to_string(),
            max_tokens: 1000,
            temperature: 0.5,
            timeout_secs: 60,
        };

        assert_eq!(config.api_key, "");
    }

    #[test]
    fn test_llm_config_various_models() {
        let models = vec![
            "gpt-4",
            "claude-3-opus",
            "anthropic/claude-3.5-sonnet",
            "deepseek-ai/DeepSeek-V3",
        ];

        for model in models {
            let config = LlmConfig {
                api_base: "https://api.test.com".to_string(),
                api_key: "key".to_string(),
                model: model.to_string(),
                max_tokens: 1000,
                temperature: 0.5,
                timeout_secs: 60,
            };
            assert_eq!(config.model, model);
        }
    }

    #[test]
    fn test_build_user_message_negative_exit_code() {
        let config = LlmConfig::default();
        let client = LlmClient::new(config).unwrap();

        let req = AgentRequest {
            instruction: "Task".to_string(),
            step: 1,
            cwd: "/".to_string(),
            last_command: Some("cmd".to_string()),
            exit_code: Some(-1),
            output: None,
        };

        let msg = client.build_user_message(&req);
        assert!(msg.contains("EXIT CODE: -1"));
    }

    #[test]
    fn test_chat_request_with_max_tokens_edge_cases() {
        let small = ChatRequest {
            model: "test".to_string(),
            messages: vec![],
            max_tokens: 1,
            temperature: 0.5,
        };
        assert_eq!(small.max_tokens, 1);

        let large = ChatRequest {
            model: "test".to_string(),
            messages: vec![],
            max_tokens: 100000,
            temperature: 0.5,
        };
        assert_eq!(large.max_tokens, 100000);
    }

    #[test]
    fn test_message_unicode_content() {
        let unicode = "Hello ä¸–ç•Œ ðŸŒ ÐŸÑ€Ð¸Ð²ÐµÑ‚";
        let msg = Message::user(unicode);
        assert_eq!(msg.content, unicode);
    }
}
