//! LLM API types.
//!
//! Types specific to the LLM proxy API.

use serde::{Deserialize, Serialize};

/// LLM provider identifier.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum LlmProvider {
    OpenRouter,
    OpenAI,
    Anthropic,
    Chutes,
    Grok,
}

impl LlmProvider {
    /// Returns the default API endpoint for this provider.
    pub fn default_endpoint(&self) -> &'static str {
        match self {
            Self::OpenRouter => "https://openrouter.ai/api/v1",
            Self::OpenAI => "https://api.openai.com/v1",
            Self::Anthropic => "https://api.anthropic.com/v1",
            Self::Chutes => "https://api.chutes.ai/v1",
            Self::Grok => "https://api.x.ai/v1",
        }
    }

    /// Detects provider from model name.
    pub fn from_model(model: &str) -> Self {
        if model.starts_with("claude") {
            Self::Anthropic
        } else if model.starts_with("grok") {
            Self::Grok
        } else if model.contains("chutes") || model.contains("deepseek") {
            Self::Chutes
        } else if model.starts_with("gpt") || model.starts_with("o1") {
            Self::OpenAI
        } else {
            Self::OpenRouter
        }
    }
}

/// Error from LLM API.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LlmApiError {
    /// Error message.
    pub message: String,
    /// Error type.
    pub error_type: Option<String>,
    /// HTTP status code.
    pub status_code: Option<u16>,
}
