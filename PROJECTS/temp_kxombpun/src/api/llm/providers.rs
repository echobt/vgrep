//! LLM provider implementations.
//!
//! Provider-specific request/response transformations for
//! OpenRouter, Anthropic, OpenAI, Chutes, and Grok.

use serde::{Deserialize, Serialize};
use serde_json::Value;
use tracing::{debug, info, warn};

// =============================================================================
// Provider Enum and Configuration
// =============================================================================

/// LLM provider types
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Provider {
    OpenRouter,
    OpenAI,
    Anthropic,
    Chutes,
    Grok,
}

impl Provider {
    /// Detect provider from model name
    pub fn from_model(model: &str) -> Self {
        if model.starts_with("claude") || model.contains("anthropic") {
            Self::Anthropic
        } else if model.starts_with("grok") {
            Self::Grok
        } else if model.contains("chutes") || model.contains("deepseek") {
            Self::Chutes
        } else if model.starts_with("gpt") || model.starts_with("o1") || model.starts_with("o3") {
            Self::OpenAI
        } else {
            Self::OpenRouter
        }
    }

    /// Parse provider from string (case-insensitive)
    pub fn from_str(s: &str) -> Self {
        match s.to_lowercase().as_str() {
            "openrouter" => Self::OpenRouter,
            "openai" => Self::OpenAI,
            "anthropic" | "claude" => Self::Anthropic,
            "chutes" | "deepseek" => Self::Chutes,
            "grok" | "xai" => Self::Grok,
            _ => Self::OpenRouter, // Default fallback
        }
    }

    /// Get default API endpoint for chat completions
    pub fn endpoint(&self) -> &'static str {
        match self {
            Self::OpenRouter => "https://openrouter.ai/api/v1/chat/completions",
            Self::OpenAI => "https://api.openai.com/v1/chat/completions",
            Self::Anthropic => "https://api.anthropic.com/v1/messages",
            Self::Chutes => "https://llm.chutes.ai/v1/chat/completions",
            Self::Grok => "https://api.x.ai/v1/chat/completions",
        }
    }

    /// Get base API URL (without path)
    pub fn base_url(&self) -> &'static str {
        match self {
            Self::OpenRouter => "https://openrouter.ai/api/v1",
            Self::OpenAI => "https://api.openai.com/v1",
            Self::Anthropic => "https://api.anthropic.com/v1",
            Self::Chutes => "https://llm.chutes.ai/v1",
            Self::Grok => "https://api.x.ai/v1",
        }
    }

    /// Get default model for this provider
    pub fn default_model(&self) -> &'static str {
        match self {
            Self::OpenRouter => "anthropic/claude-3.5-sonnet",
            Self::OpenAI => "gpt-4o",
            Self::Anthropic => "claude-3-5-sonnet-20241022",
            Self::Chutes => "deepseek-ai/DeepSeek-V3",
            Self::Grok => "grok-2-latest",
        }
    }

    /// Build authorization header value
    pub fn auth_header(&self, api_key: &str) -> String {
        match self {
            Self::Anthropic => api_key.to_string(), // Uses x-api-key header instead
            _ => format!("Bearer {}", api_key),
        }
    }

    /// Check if provider uses OpenAI-compatible API format
    pub fn is_openai_compatible(&self) -> bool {
        match self {
            Self::Anthropic => false,
            _ => true,
        }
    }

    /// Check if provider supports streaming
    pub fn supports_streaming(&self) -> bool {
        true // All providers support streaming
    }
}

impl std::fmt::Display for Provider {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::OpenRouter => write!(f, "openrouter"),
            Self::OpenAI => write!(f, "openai"),
            Self::Anthropic => write!(f, "anthropic"),
            Self::Chutes => write!(f, "chutes"),
            Self::Grok => write!(f, "grok"),
        }
    }
}

// =============================================================================
// Provider Configuration
// =============================================================================

/// Configuration for a specific provider
pub struct ProviderConfig {
    pub provider: Provider,
    pub api_key: String,
    pub model: String,
}

impl ProviderConfig {
    pub fn new(provider: Provider, api_key: String, model: Option<String>) -> Self {
        Self {
            model: model.unwrap_or_else(|| provider.default_model().to_string()),
            provider,
            api_key,
        }
    }

    pub fn endpoint(&self) -> &'static str {
        self.provider.endpoint()
    }

    pub fn auth_header(&self) -> String {
        self.provider.auth_header(&self.api_key)
    }
}

// =============================================================================
// OpenAI Responses API Support (GPT-4.1+, GPT-5.x)
// =============================================================================

/// Check if model uses OpenAI's /v1/responses API instead of /v1/chat/completions
pub fn is_openai_responses_model(model: &str) -> bool {
    let model_lower = model.to_lowercase();
    model_lower.starts_with("gpt-4.1") || model_lower.starts_with("gpt-5")
}

/// Get the appropriate endpoint for OpenAI models
pub fn get_openai_endpoint(model: &str) -> &'static str {
    if is_openai_responses_model(model) {
        "https://api.openai.com/v1/responses"
    } else {
        "https://api.openai.com/v1/chat/completions"
    }
}

// =============================================================================
// Anthropic Request Transformation
// =============================================================================

/// Transform request body for Anthropic Messages API format
///
/// Anthropic's Messages API has specific requirements:
/// 1. System messages must be in a top-level `system` parameter, not in messages array
/// 2. Maximum of 4 cache_control blocks allowed
pub fn transform_for_anthropic(mut body: Value) -> Value {
    if let Some(messages) = body.get_mut("messages").and_then(|m| m.as_array_mut()) {
        // Extract system messages and combine into top-level system parameter
        let mut system_contents: Vec<Value> = Vec::new();
        let mut non_system_messages: Vec<Value> = Vec::new();

        for msg in messages.drain(..) {
            if msg.get("role").and_then(|r| r.as_str()) == Some("system") {
                // Extract content from system message
                if let Some(content) = msg.get("content") {
                    if let Some(text) = content.as_str() {
                        // Simple string content
                        system_contents.push(serde_json::json!({
                            "type": "text",
                            "text": text
                        }));
                    } else if let Some(arr) = content.as_array() {
                        // Array content (possibly with cache_control)
                        for item in arr {
                            system_contents.push(item.clone());
                        }
                    } else {
                        // Object content - pass through
                        system_contents.push(content.clone());
                    }
                }
            } else {
                non_system_messages.push(msg);
            }
        }

        // Replace messages with non-system messages only
        *messages = non_system_messages;

        // Add system parameter if we have system content
        if !system_contents.is_empty() {
            // Limit cache_control blocks to 4 (Anthropic limit)
            let mut cache_count = 0;
            for item in system_contents.iter_mut().rev() {
                if item.get("cache_control").is_some() {
                    cache_count += 1;
                    if cache_count > 4 {
                        // Remove excess cache_control
                        if let Some(obj) = item.as_object_mut() {
                            obj.remove("cache_control");
                        }
                    }
                }
            }

            // Also limit cache_control in messages
            for msg in messages.iter_mut() {
                if let Some(content) = msg.get_mut("content").and_then(|c| c.as_array_mut()) {
                    for item in content.iter_mut().rev() {
                        if item.get("cache_control").is_some() {
                            cache_count += 1;
                            if cache_count > 4 {
                                if let Some(obj) = item.as_object_mut() {
                                    obj.remove("cache_control");
                                }
                            }
                        }
                    }
                }
            }

            body["system"] = Value::Array(system_contents);
        }
    }

    body
}

// =============================================================================
// OpenAI Responses API Transformation
// =============================================================================

/// LLM message for transformation (simplified)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LlmMessage {
    pub role: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub content: Option<Value>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ToolCall>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tool_call_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// Tool call structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCall {
    pub id: String,
    #[serde(rename = "type")]
    pub call_type: String,
    pub function: FunctionCall,
}

/// Function call structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FunctionCall {
    pub name: String,
    pub arguments: String,
}

/// Transform chat messages to OpenAI Responses API input format
pub fn transform_to_responses_api(
    messages: &[LlmMessage],
    model: &str,
    max_tokens: Option<u32>,
    temperature: Option<f32>,
    extra_params: Option<&Value>,
) -> Value {
    let mut instructions: Option<String> = None;
    let mut input_items: Vec<Value> = Vec::new();

    for msg in messages {
        match msg.role.as_str() {
            "system" => {
                // System messages become 'instructions' parameter
                let content_str = msg.content.as_ref().and_then(|v| v.as_str()).unwrap_or("");
                if let Some(ref mut inst) = instructions {
                    inst.push_str("\n\n");
                    inst.push_str(content_str);
                } else {
                    instructions = Some(content_str.to_string());
                }
            }
            "user" => {
                // User messages become input items
                let content_str = msg.content.as_ref().and_then(|v| v.as_str()).unwrap_or("");
                input_items.push(serde_json::json!({
                    "type": "message",
                    "role": "user",
                    "content": [{"type": "input_text", "text": content_str}]
                }));
            }
            "assistant" => {
                // Check for tool_calls
                if let Some(ref tool_calls) = msg.tool_calls {
                    for tc in tool_calls {
                        input_items.push(serde_json::json!({
                            "type": "function_call",
                            "id": &tc.id,
                            "call_id": &tc.id,
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }));
                    }
                } else if let Some(ref content) = msg.content {
                    if let Some(text) = content.as_str() {
                        if !text.is_empty() {
                            input_items.push(serde_json::json!({
                                "type": "message",
                                "role": "assistant",
                                "content": [{"type": "output_text", "text": text}]
                            }));
                        }
                    }
                }
            }
            "tool" => {
                // Tool results become function_call_output items
                let content_str = msg.content.as_ref().and_then(|v| v.as_str()).unwrap_or("");
                input_items.push(serde_json::json!({
                    "type": "function_call_output",
                    "call_id": msg.tool_call_id.as_deref().unwrap_or(""),
                    "output": content_str
                }));
            }
            _ => {}
        }
    }

    let mut body = serde_json::json!({
        "model": model,
        "input": input_items,
        "max_output_tokens": max_tokens.unwrap_or(64000),
        "store": false,
    });

    // Only add temperature if explicitly provided
    if let Some(temp) = temperature {
        body["temperature"] = serde_json::json!(temp);
    }

    if let Some(inst) = instructions {
        body["instructions"] = Value::String(inst);
    }

    // Merge tools from extra_params if present
    if let Some(extra) = extra_params {
        if let Some(tools) = extra.get("tools") {
            // Transform tools to Responses API format
            if let Some(tools_array) = tools.as_array() {
                let mut transformed_tools: Vec<Value> = Vec::new();
                for tool in tools_array {
                    if tool.get("type").and_then(|t| t.as_str()) == Some("function") {
                        if let Some(func) = tool.get("function") {
                            transformed_tools.push(serde_json::json!({
                                "type": "function",
                                "name": func.get("name"),
                                "description": func.get("description"),
                                "parameters": func.get("parameters"),
                                "strict": true
                            }));
                        }
                    }
                }
                if !transformed_tools.is_empty() {
                    body["tools"] = Value::Array(transformed_tools);
                    body["tool_choice"] = serde_json::json!("auto");
                }
            }
        }

        // Copy other extra params (but not messages, model, etc.)
        if let Some(extra_obj) = extra.as_object() {
            for (key, value) in extra_obj {
                // Skip params that are handled elsewhere or not supported by Responses API
                if [
                    "tools",
                    "tool_choice",
                    "messages",
                    "model",
                    "max_tokens",
                    "temperature",
                    "max_completion_tokens",
                ]
                .contains(&key.as_str())
                {
                    continue;
                }
                body[key] = value.clone();
            }
            // Handle max_completion_tokens -> max_output_tokens conversion
            if let Some(mct) = extra_obj.get("max_completion_tokens") {
                body["max_output_tokens"] = mct.clone();
            }
        }
    }

    body
}

// =============================================================================
// Response Parsing
// =============================================================================

/// Parsed LLM response
#[derive(Debug, Clone)]
pub struct LlmResponse {
    pub content: Option<String>,
    pub model: Option<String>,
    pub usage: Option<LlmUsage>,
    pub cost_usd: Option<f64>,
    pub tool_calls: Option<Vec<LlmToolCall>>,
}

/// Token usage information
#[derive(Debug, Clone, Serialize)]
pub struct LlmUsage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_tokens_details: Option<Value>,
}

/// Tool call in response
#[derive(Debug, Clone, Serialize)]
pub struct LlmToolCall {
    pub id: Option<String>,
    #[serde(rename = "type")]
    pub call_type: String,
    pub function: LlmFunctionCall,
}

/// Function call in response
#[derive(Debug, Clone, Serialize)]
pub struct LlmFunctionCall {
    pub name: String,
    pub arguments: String,
}

/// Parse OpenAI Responses API response
pub fn parse_responses_api_response(json: &Value, model: &str) -> LlmResponse {
    let mut content = String::new();
    let mut tool_calls: Vec<LlmToolCall> = Vec::new();

    if let Some(output) = json.get("output").and_then(|o| o.as_array()) {
        for item in output {
            match item.get("type").and_then(|t| t.as_str()) {
                Some("message") => {
                    // Extract text from message content
                    if let Some(contents) = item.get("content").and_then(|c| c.as_array()) {
                        for c in contents {
                            if c.get("type").and_then(|t| t.as_str()) == Some("output_text") {
                                if let Some(text) = c.get("text").and_then(|t| t.as_str()) {
                                    content.push_str(text);
                                }
                            }
                        }
                    }
                }
                Some("function_call") => {
                    // Extract function calls
                    let name = item
                        .get("name")
                        .and_then(|n| n.as_str())
                        .unwrap_or("")
                        .to_string();
                    let arguments = item
                        .get("arguments")
                        .and_then(|a| a.as_str())
                        .unwrap_or("{}")
                        .to_string();
                    let id = item
                        .get("id")
                        .or_else(|| item.get("call_id"))
                        .and_then(|i| i.as_str())
                        .map(|s| s.to_string());

                    tool_calls.push(LlmToolCall {
                        id,
                        call_type: "function".to_string(),
                        function: LlmFunctionCall { name, arguments },
                    });
                }
                _ => {}
            }
        }
    }

    // Extract usage
    let usage = json.get("usage").map(|u| LlmUsage {
        prompt_tokens: u.get("input_tokens").and_then(|t| t.as_u64()).unwrap_or(0) as u32,
        completion_tokens: u.get("output_tokens").and_then(|t| t.as_u64()).unwrap_or(0) as u32,
        total_tokens: u.get("total_tokens").and_then(|t| t.as_u64()).unwrap_or(0) as u32,
        prompt_tokens_details: None,
    });

    LlmResponse {
        content: if content.is_empty() {
            None
        } else {
            Some(content)
        },
        model: json
            .get("model")
            .and_then(|m| m.as_str())
            .map(|s| s.to_string()),
        usage,
        cost_usd: None, // Responses API doesn't return cost
        tool_calls: if tool_calls.is_empty() {
            None
        } else {
            Some(tool_calls)
        },
    }
}

/// Parse OpenAI/OpenRouter chat completions response
pub fn parse_chat_completions_response(json: &Value) -> LlmResponse {
    // Extract content
    let content = json["choices"][0]["message"]["content"]
        .as_str()
        .map(|s| s.to_string());

    let response_model = json["model"].as_str().map(|s| s.to_string());

    let usage = json.get("usage").map(|usage_obj| LlmUsage {
        prompt_tokens: usage_obj["prompt_tokens"].as_u64().unwrap_or(0) as u32,
        completion_tokens: usage_obj["completion_tokens"].as_u64().unwrap_or(0) as u32,
        total_tokens: usage_obj["total_tokens"].as_u64().unwrap_or(0) as u32,
        prompt_tokens_details: usage_obj.get("prompt_tokens_details").cloned(),
    });

    // Try to use provider-reported cost
    let cost_usd = json["usage"]["cost"]
        .as_f64()
        .or_else(|| json["usage"]["total_cost"].as_f64())
        .or_else(|| json["cost"].as_f64());

    // Extract tool_calls if present
    let tool_calls = json["choices"][0]["message"]["tool_calls"]
        .as_array()
        .map(|calls| {
            calls
                .iter()
                .filter_map(|tc| {
                    let id = tc["id"].as_str().map(|s| s.to_string());
                    let call_type = tc["type"].as_str().unwrap_or("function").to_string();
                    let func = &tc["function"];
                    let name = func["name"].as_str()?.to_string();
                    let arguments = func["arguments"].as_str().unwrap_or("{}").to_string();
                    Some(LlmToolCall {
                        id,
                        call_type,
                        function: LlmFunctionCall { name, arguments },
                    })
                })
                .collect::<Vec<_>>()
        })
        .filter(|v| !v.is_empty());

    LlmResponse {
        content,
        model: response_model,
        usage,
        cost_usd,
        tool_calls,
    }
}

// =============================================================================
// Error Parsing
// =============================================================================

/// Parsed error from LLM provider
#[derive(Debug)]
pub struct ParsedError {
    pub message: String,
    pub error_type: Option<String>,
}

/// Parse error response from LLM providers (OpenRouter, OpenAI, Anthropic)
pub fn parse_error_response(response_text: &str) -> ParsedError {
    if let Ok(json) = serde_json::from_str::<Value>(response_text) {
        // OpenRouter/OpenAI format: {"error": {"message": "...", "type": "...", "code": "..."}}
        if let Some(error_obj) = json.get("error") {
            let message = error_obj
                .get("message")
                .and_then(|m| m.as_str())
                .unwrap_or("Unknown error")
                .to_string();
            let error_type = error_obj
                .get("type")
                .or_else(|| error_obj.get("code"))
                .and_then(|t| t.as_str())
                .map(|s| s.to_string());
            return ParsedError {
                message,
                error_type,
            };
        }

        // Simple format: {"message": "..."}
        if let Some(message) = json.get("message").and_then(|m| m.as_str()) {
            return ParsedError {
                message: message.to_string(),
                error_type: None,
            };
        }
    }

    // Fallback: return raw text (truncated)
    let truncated = if response_text.len() > 200 {
        format!("{}...", &response_text[..200])
    } else {
        response_text.to_string()
    };
    ParsedError {
        message: truncated,
        error_type: None,
    }
}

// =============================================================================
// Cost Calculation Helpers
// =============================================================================

/// Estimate cost for LLM code review based on provider
pub fn estimate_review_cost(provider: &str) -> f64 {
    match provider.to_lowercase().as_str() {
        "openrouter" | "anthropic" | "claude" => 0.003,
        "openai" => 0.002,
        "chutes" | "deepseek" => 0.0005,
        "grok" => 0.002,
        _ => 0.002,
    }
}

/// Log cache hit information if available
pub fn log_cache_info(json: &Value) {
    let cached_tokens = json["usage"]["prompt_tokens_details"]["cached_tokens"]
        .as_u64()
        .unwrap_or(0);
    if cached_tokens > 0 {
        let prompt_tokens = json["usage"]["prompt_tokens"].as_u64().unwrap_or(0);
        let cache_hit_ratio = if prompt_tokens > 0 {
            (cached_tokens as f64 / prompt_tokens as f64) * 100.0
        } else {
            0.0
        };
        info!(
            "LLM cache hit: {} cached of {} prompt tokens ({:.1}% hit rate)",
            cached_tokens, prompt_tokens, cache_hit_ratio
        );
    }
}

// =============================================================================
// Request Building Helpers
// =============================================================================

/// Add OpenRouter-specific request options
pub fn add_openrouter_options(body: &mut Value) {
    // Add usage: {include: true} to get cost and cache info
    if let Some(base) = body.as_object_mut() {
        base.insert("usage".to_string(), serde_json::json!({"include": true}));
    }
}

/// Build standard request body for chat completions
pub fn build_chat_request(
    model: &str,
    messages: &[LlmMessage],
    max_tokens: Option<u32>,
    temperature: Option<f32>,
    extra_params: Option<&Value>,
) -> Value {
    // Check if max_completion_tokens is in extra_params (for o-series models)
    let has_max_completion_tokens = extra_params
        .as_ref()
        .and_then(|e| e.as_object())
        .map(|o| o.contains_key("max_completion_tokens"))
        .unwrap_or(false);

    let mut body = serde_json::json!({
        "model": model,
        "messages": messages,
    });

    // Only add temperature if explicitly provided
    if let Some(temp) = temperature {
        body["temperature"] = serde_json::json!(temp);
    }

    // Use max_completion_tokens if provided (for o-series), otherwise max_tokens
    if !has_max_completion_tokens {
        body["max_tokens"] = serde_json::json!(max_tokens.unwrap_or(64000));
    }

    // Merge extra_params
    if let Some(extra) = extra_params {
        if let (Some(base), Some(extra_obj)) = (body.as_object_mut(), extra.as_object()) {
            for (key, value) in extra_obj {
                base.insert(key.clone(), value.clone());
            }
        }
    }

    body
}

// =============================================================================
// HTTP Status Code Mapping
// =============================================================================

/// Map LLM provider HTTP status code to appropriate response status
pub fn map_status_code(status_code: u16) -> u16 {
    match status_code {
        400 => 400, // Bad Request
        401 => 401, // Unauthorized
        402 => 402, // Payment Required
        403 => 403, // Forbidden
        404 => 404, // Not Found
        429 => 429, // Too Many Requests
        500 => 502, // Provider internal error -> Bad Gateway
        502 => 502, // Provider upstream error -> Bad Gateway
        503 => 503, // Service Unavailable
        504 => 504, // Gateway Timeout
        _ => 502,   // Default to Bad Gateway
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_provider_from_model() {
        assert_eq!(Provider::from_model("claude-3"), Provider::Anthropic);
        assert_eq!(
            Provider::from_model("anthropic/claude-3"),
            Provider::Anthropic
        );
        assert_eq!(Provider::from_model("grok-2"), Provider::Grok);
        assert_eq!(Provider::from_model("deepseek-v3"), Provider::Chutes);
        assert_eq!(Provider::from_model("gpt-4o"), Provider::OpenAI);
        assert_eq!(Provider::from_model("o1-preview"), Provider::OpenAI);
        assert_eq!(Provider::from_model("o3-mini"), Provider::OpenAI);
        assert_eq!(
            Provider::from_model("some-other-model"),
            Provider::OpenRouter
        );
    }

    #[test]
    fn test_provider_from_str() {
        assert_eq!(Provider::from_str("openrouter"), Provider::OpenRouter);
        assert_eq!(Provider::from_str("OPENAI"), Provider::OpenAI);
        assert_eq!(Provider::from_str("Anthropic"), Provider::Anthropic);
        assert_eq!(Provider::from_str("claude"), Provider::Anthropic);
        assert_eq!(Provider::from_str("chutes"), Provider::Chutes);
        assert_eq!(Provider::from_str("deepseek"), Provider::Chutes);
        assert_eq!(Provider::from_str("grok"), Provider::Grok);
        assert_eq!(Provider::from_str("xai"), Provider::Grok);
        assert_eq!(Provider::from_str("unknown"), Provider::OpenRouter);
    }

    #[test]
    fn test_is_openai_responses_model() {
        assert!(is_openai_responses_model("gpt-4.1"));
        assert!(is_openai_responses_model("GPT-4.1-turbo"));
        assert!(is_openai_responses_model("gpt-5"));
        assert!(is_openai_responses_model("GPT-5-preview"));
        assert!(!is_openai_responses_model("gpt-4o"));
        assert!(!is_openai_responses_model("gpt-4-turbo"));
        assert!(!is_openai_responses_model("o1-preview"));
    }

    #[test]
    fn test_parse_error_response() {
        // OpenAI format
        let openai_error =
            r#"{"error": {"message": "Invalid API key", "type": "invalid_request_error"}}"#;
        let parsed = parse_error_response(openai_error);
        assert_eq!(parsed.message, "Invalid API key");
        assert_eq!(parsed.error_type, Some("invalid_request_error".to_string()));

        // Simple format
        let simple_error = r#"{"message": "Rate limited"}"#;
        let parsed = parse_error_response(simple_error);
        assert_eq!(parsed.message, "Rate limited");
        assert!(parsed.error_type.is_none());

        // Plain text
        let plain_error = "Something went wrong";
        let parsed = parse_error_response(plain_error);
        assert_eq!(parsed.message, "Something went wrong");
        assert!(parsed.error_type.is_none());
    }

    #[test]
    fn test_transform_for_anthropic() {
        let body = serde_json::json!({
            "model": "claude-3",
            "messages": [
                {"role": "system", "content": "You are helpful"},
                {"role": "user", "content": "Hello"}
            ]
        });

        let transformed = transform_for_anthropic(body);

        // System message should be moved to top-level
        assert!(transformed.get("system").is_some());
        let system = transformed.get("system").unwrap();
        assert!(system.is_array());

        // Messages should only contain user message
        let messages = transformed.get("messages").unwrap().as_array().unwrap();
        assert_eq!(messages.len(), 1);
        assert_eq!(messages[0]["role"], "user");
    }

    #[test]
    fn test_estimate_review_cost() {
        assert_eq!(estimate_review_cost("openrouter"), 0.003);
        assert_eq!(estimate_review_cost("anthropic"), 0.003);
        assert_eq!(estimate_review_cost("openai"), 0.002);
        assert_eq!(estimate_review_cost("chutes"), 0.0005);
        assert_eq!(estimate_review_cost("deepseek"), 0.0005);
        assert_eq!(estimate_review_cost("grok"), 0.002);
        assert_eq!(estimate_review_cost("unknown"), 0.002);
    }

    #[test]
    fn test_map_status_code() {
        assert_eq!(map_status_code(400), 400);
        assert_eq!(map_status_code(401), 401);
        assert_eq!(map_status_code(429), 429);
        assert_eq!(map_status_code(500), 502);
        assert_eq!(map_status_code(999), 502);
    }
}
