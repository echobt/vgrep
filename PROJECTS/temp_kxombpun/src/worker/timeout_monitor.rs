//! Timeout Retry Monitor
//!
//! Background service that monitors task logs for timeout errors and reassigns
//! failed tasks to different validators for a second attempt.
//!
//! Flow:
//! 1. Poll DB every 5 minutes for tasks with timeout errors (retry_count < 1)
//! 2. For each timeout task:
//!    a. Find an available validator (not the one that timed out)
//!    b. Create a new evaluation_task for the new validator
//!    c. Increment retry_count to prevent infinite retries
//!    d. Log the reassignment
//!
//! This complements the local retry in validator_worker.rs:
//! - Validator retries locally once on timeout
//! - If still fails, server reassigns to a different validator

use crate::storage::pg::{PgStorage, TimeoutTask};
use std::sync::Arc;
use std::time::Duration;
use tokio::time::interval;
use tracing::{debug, error, info, warn};

/// Configuration for the timeout retry monitor
pub struct TimeoutRetryMonitorConfig {
    /// How often to check for timeout tasks (default: 5 minutes)
    pub poll_interval_secs: u64,
    /// Maximum retry count per task (default: 1 - only retry once on server side)
    pub max_retry_count: i32,
}

impl Default for TimeoutRetryMonitorConfig {
    fn default() -> Self {
        Self {
            poll_interval_secs: 300, // 5 minutes
            max_retry_count: 1,      // Only retry each task once on server side
        }
    }
}

/// Background worker that monitors timeout tasks and reassigns them
pub struct TimeoutRetryMonitor {
    storage: Arc<PgStorage>,
    config: TimeoutRetryMonitorConfig,
}

impl TimeoutRetryMonitor {
    pub fn new(storage: Arc<PgStorage>, config: TimeoutRetryMonitorConfig) -> Self {
        Self { storage, config }
    }

    /// Start the monitor (runs forever)
    pub async fn run(&self) {
        info!(
            "Timeout retry monitor started (poll={}s, max_retry={})",
            self.config.poll_interval_secs, self.config.max_retry_count
        );

        let mut ticker = interval(Duration::from_secs(self.config.poll_interval_secs));

        loop {
            ticker.tick().await;

            if let Err(e) = self.check_and_reassign_timeouts().await {
                error!("Error checking timeout tasks: {}", e);
            }
        }
    }

    /// Check for timeout tasks and reassign to new validators
    async fn check_and_reassign_timeouts(&self) -> anyhow::Result<()> {
        // Get tasks with timeout errors that haven't been retried
        let timeout_tasks = self
            .storage
            .get_tasks_with_timeout_errors(self.config.max_retry_count)
            .await?;

        if timeout_tasks.is_empty() {
            debug!("No timeout tasks found for retry");
            return Ok(());
        }

        info!(
            "Found {} timeout tasks eligible for retry",
            timeout_tasks.len()
        );

        let mut reassigned_count = 0;
        let mut skipped_count = 0;

        for task in timeout_tasks {
            let short_agent = &task.agent_hash[..16.min(task.agent_hash.len())];
            let short_task = &task.task_id[..16.min(task.task_id.len())];
            let short_validator = &task.validator_hotkey[..16.min(task.validator_hotkey.len())];

            // Find available validators who haven't tried this task
            let available_validators = match self
                .storage
                .get_validators_without_task(&task.agent_hash, &task.task_id)
                .await
            {
                Ok(v) => v,
                Err(e) => {
                    warn!(
                        "Failed to get available validators for task {}: {}",
                        short_task, e
                    );
                    continue;
                }
            };

            if available_validators.is_empty() {
                debug!(
                    "No available validators for task {} (agent {}), marking as retried",
                    short_task, short_agent
                );
                // Mark as retried anyway to prevent checking again
                if let Err(e) = self
                    .storage
                    .mark_task_for_retry(&task.agent_hash, &task.task_id, &task.validator_hotkey)
                    .await
                {
                    warn!("Failed to mark task {} as retried: {}", short_task, e);
                }
                skipped_count += 1;
                continue;
            }

            // Select the first available validator
            let new_validator = &available_validators[0];
            let short_new = &new_validator[..16.min(new_validator.len())];

            // Reassign the task
            match self
                .storage
                .reassign_task_for_retry(
                    &task.agent_hash,
                    &task.task_id,
                    &task.validator_hotkey,
                    new_validator,
                )
                .await
            {
                Ok(()) => {
                    info!(
                        "Reassigned timeout task {} (agent {}) from {} to {}",
                        short_task, short_agent, short_validator, short_new
                    );
                    reassigned_count += 1;
                }
                Err(e) => {
                    error!(
                        "Failed to reassign task {} from {} to {}: {}",
                        short_task, short_validator, short_new, e
                    );
                }
            }
        }

        if reassigned_count > 0 || skipped_count > 0 {
            info!(
                "Timeout retry check complete: {} reassigned, {} skipped (no validators available)",
                reassigned_count, skipped_count
            );
        }

        Ok(())
    }
}

/// Start the timeout retry monitor in background
pub fn spawn_timeout_retry_monitor(storage: Arc<PgStorage>, config: TimeoutRetryMonitorConfig) {
    tokio::spawn(async move {
        let monitor = TimeoutRetryMonitor::new(storage, config);
        monitor.run().await;
    });
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_defaults() {
        let config = TimeoutRetryMonitorConfig::default();
        assert_eq!(config.poll_interval_secs, 300);
        assert_eq!(config.max_retry_count, 1);
    }

    #[test]
    fn test_config_custom() {
        let config = TimeoutRetryMonitorConfig {
            poll_interval_secs: 60,
            max_retry_count: 2,
        };
        assert_eq!(config.poll_interval_secs, 60);
        assert_eq!(config.max_retry_count, 2);
    }
}
