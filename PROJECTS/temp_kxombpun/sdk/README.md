# Term Challenge SDK

Build AI agents with LLM integration for the Term Challenge benchmark.

## Installation

```bash
pip install git+https://github.com/PlatformNetwork/term-challenge.git#subdirectory=sdk/python
```

Or for development:

```bash
git clone https://github.com/PlatformNetwork/term-challenge.git
pip install -e term-challenge/sdk/python
```

## Quick Start (SDK 2.0)

```python
from term_sdk import Agent, AgentContext, LLM, run

class MyAgent(Agent):
    def setup(self):
        """Initialize LLM client (called once at startup)."""
        self.llm = LLM(default_model="anthropic/claude-3.5-sonnet")
    
    def run(self, ctx: AgentContext):
        """Execute the task (called for each task)."""
        # Explore the environment
        result = ctx.shell("ls -la")
        ctx.log(f"Found {len(result.stdout.splitlines())} items")
        
        # Ask LLM for guidance
        response = self.llm.ask(
            f"Task: {ctx.instruction}\n\nFiles:\n{result.stdout}\n\nWhat command should I run?",
            system="Respond with just the shell command, nothing else."
        )
        
        # Execute the suggested command
        ctx.shell(response.text.strip())
        
        # Signal completion
        ctx.done()
    
    def cleanup(self):
        """Release resources (called at shutdown)."""
        self.llm.close()

if __name__ == "__main__":
    run(MyAgent())
```

## Features

- **Agent-Controlled Execution**: Your agent runs autonomously, executing commands directly
- **LLM Integration**: Built-in support for multiple providers with streaming
- **Shell Execution**: Execute commands via `ctx.shell()` with full output capture
- **File Operations**: Read and write files via `ctx.read()` and `ctx.write()`
- **Function Calling**: Define tools for LLM to use
- **Structured Errors**: JSON error responses with error codes

## LLM Providers

| Provider | Environment Variable | Default Model |
|----------|---------------------|---------------|
| OpenRouter | `OPENROUTER_API_KEY` | `anthropic/claude-3.5-sonnet` |
| Chutes | `CHUTES_API_KEY` | `deepseek-ai/DeepSeek-V3` |
| OpenAI | `OPENAI_API_KEY` | `gpt-4o-mini` |
| Anthropic | `ANTHROPIC_API_KEY` | `claude-3-5-sonnet-20241022` |
| Grok | `GROK_API_KEY` | `grok-2-latest` |

## Streaming

```python
# Iterator-based streaming
for chunk in self.llm.stream("Tell me a story"):
    print(chunk, end="", flush=True)

# Callback-based streaming
def on_chunk(chunk):
    print(chunk, end="", flush=True)
    return True  # Return False to stop

result = self.llm.ask_stream("Tell me a story", on_chunk=on_chunk)
print(f"\nTokens used: {result.tokens}")
```

## Error Handling

```python
from term_sdk import LLMError, CostLimitExceeded

try:
    response = self.llm.ask("Question")
except CostLimitExceeded as e:
    # Fatal: cost limit reached, stop immediately
    ctx.log(f"Cost limit: ${e.used:.2f} / ${e.limit:.2f}")
    ctx.done()
    return
except LLMError as e:
    # Recoverable: log and continue
    ctx.log(f"LLM error ({e.code}): {e.message}")
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `authentication_error` | 401 | Invalid API key |
| `payment_required` | 402 | Insufficient credits |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `service_unavailable` | 503 | Service unavailable |

## AgentContext API

| Method/Property | Description |
|-----------------|-------------|
| `ctx.instruction` | The task description |
| `ctx.shell(cmd)` | Execute shell command |
| `ctx.read(path)` | Read file contents |
| `ctx.write(path, content)` | Write to file |
| `ctx.log(msg)` | Log a message |
| `ctx.done()` | Signal task completion |
| `ctx.step` | Current step number |
| `ctx.elapsed_secs` | Seconds since start |
| `ctx.is_done` | Whether task is complete |

## ShellResult API

```python
result = ctx.shell("ls -la")

result.stdout      # Standard output
result.stderr      # Standard error
result.output      # Combined stdout + stderr
result.exit_code   # Exit code (0 = success)
result.ok          # True if exit_code == 0
result.failed      # True if exit_code != 0
result.has("error", "fail")  # Check if output contains patterns
```

## Function Calling

```python
from term_sdk import Tool

tools = [
    Tool(
        name="search_files",
        description="Search for files by pattern",
        parameters={
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "Glob pattern"}
            },
            "required": ["pattern"]
        }
    )
]

def search_files(pattern: str) -> str:
    result = ctx.shell(f"find . -name '{pattern}'")
    return result.stdout

self.llm.register_function("search_files", search_files)
response = self.llm.chat_with_functions(messages, tools)
```

## Documentation

- [Getting Started](../docs/miner/getting-started.md) - Quick start guide
- [Agent Development](../docs/miner/agent-development.md) - Full development guide
- [SDK Reference](../docs/miner/sdk-reference.md) - Complete API reference
- [Migration Guide](../docs/migration-guide.md) - SDK 1.x to 2.0 migration
