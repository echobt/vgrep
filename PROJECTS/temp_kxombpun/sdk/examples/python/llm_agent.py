#!/usr/bin/env python3
"""
LLM-powered agent for Terminal Benchmark Challenge (SDK 2.0).

This agent uses an LLM to solve terminal tasks. It runs autonomously,
executing commands directly and maintaining conversation context.

Usage:
    term bench agent -a llm_agent.py -t <task> --api-key "sk-..."
    term bench agent -a llm_agent.py -d terminal-bench@2.0 --api-key "sk-..."
"""
import os
from term_sdk import Agent, AgentContext, LLM, LLMError, run


SYSTEM_PROMPT = """You are a terminal expert. You solve tasks by executing shell commands.

IMPORTANT RULES:
1. Execute ONE command at a time
2. Analyze the output and decide what to do next
3. When the task is complete, set task_complete to true
4. Keep commands simple and portable (bash on Linux)

Output format (JSON):
- To run a command: {"command": "your command here", "task_complete": false}
- When done: {"command": null, "task_complete": true}

ONLY output valid JSON, nothing else."""


class LLMAgent(Agent):
    """
    LLM agent that solves terminal tasks autonomously.
    
    Uses conversation history to maintain context across steps.
    """
    
    def setup(self):
        """Initialize LLM client."""
        self.model = os.environ.get("LLM_MODEL", "anthropic/claude-3.5-sonnet")
        self.llm = LLM(default_model=self.model)
        ctx_log = lambda msg: print(f"[setup] {msg}")
        ctx_log(f"Using model: {self.model}")
    
    def run(self, ctx: AgentContext):
        """Execute the task using LLM reasoning."""
        ctx.log(f"Task: {ctx.instruction[:100]}...")
        
        # Initialize conversation with system prompt and task
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": self._build_initial_prompt(ctx)}
        ]
        
        # Main execution loop
        while ctx.step < 100:  # Limit to 100 steps
            try:
                # Get LLM response
                result = self.llm.ask(
                    messages[-1]["content"] if messages[-1]["role"] == "user" else ctx.instruction,
                    system=SYSTEM_PROMPT,
                    temperature=0.1
                )
                ctx.log(f"LLM response: {result.text[:100]}...")
                
                # Parse response
                data = result.json()
                if not data:
                    ctx.log("Failed to parse LLM response as JSON")
                    break
                
                # Check if task is complete
                if data.get("task_complete"):
                    ctx.log("LLM marked task as complete")
                    break
                
                # Execute command
                cmd = data.get("command")
                if cmd:
                    ctx.log(f"Executing: {cmd}")
                    shell_result = ctx.shell(cmd)
                    
                    # Add exchange to conversation
                    messages.append({"role": "assistant", "content": result.text})
                    messages.append({
                        "role": "user",
                        "content": self._build_follow_up_prompt(ctx, cmd, shell_result)
                    })
                else:
                    ctx.log("No command in response, stopping")
                    break
                    
            except LLMError as e:
                ctx.log(f"LLM error: {e.code} - {e.message}")
                break
            except Exception as e:
                ctx.log(f"Error: {e}")
                break
        
        ctx.done()
    
    def _build_initial_prompt(self, ctx: AgentContext) -> str:
        """Build the initial prompt with task info."""
        return f"""TASK: {ctx.instruction}

WORKING DIRECTORY: /app

What single command should I run first? (JSON only)"""
    
    def _build_follow_up_prompt(self, ctx: AgentContext, cmd: str, result) -> str:
        """Build follow-up prompt with command output."""
        output = result.output[-2000:] if len(result.output) > 2000 else result.output
        
        status = "SUCCESS" if result.ok else f"FAILED (exit code {result.exit_code})"
        
        return f"""Command executed: {cmd}
Status: {status}
Output:
```
{output}
```

What command should I run next? (JSON only)
If the task is complete, respond with: {{"command": null, "task_complete": true}}"""
    
    def cleanup(self):
        """Report final stats."""
        stats = self.llm.get_stats()
        print(f"[cleanup] Total cost: ${stats['total_cost']:.4f}")
        print(f"[cleanup] Total tokens: {stats['total_tokens']}")
        self.llm.close()


if __name__ == "__main__":
    run(LLMAgent())
