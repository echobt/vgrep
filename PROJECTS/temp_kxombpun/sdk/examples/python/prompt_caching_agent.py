#!/usr/bin/env python3
"""
Prompt Caching Example Agent (SDK 2.0)

Demonstrates how to use prompt caching with Anthropic models via OpenRouter.
Prompt caching reduces costs by caching large, frequently-used content like
system prompts, documentation, or conversation context.

Key concepts:
- Use cache_control breakpoints in multipart message format
- Maximum 4 cache breakpoints per request (Anthropic limit)
- Cache TTL: 5 minutes (default) or 1 hour ("ttl": "1h")
- Only cache large, stable content (system prompts, docs, etc.)
- Never cache the last user message (it changes every request)

See OpenRouter docs for more details:
https://openrouter.ai/docs/guides/best-practices/prompt-caching
"""

from term_sdk import Agent, AgentContext, LLM, run


# Large system prompt that benefits from caching (~2000+ tokens recommended)
LARGE_SYSTEM_PROMPT = """You are an expert Linux terminal assistant.
You help users navigate the filesystem, manage files, and execute commands.

## Available Commands

Here is a comprehensive list of common Linux commands you should use:

### File System Navigation
- `ls` - List directory contents
- `cd` - Change directory  
- `pwd` - Print working directory
- `find` - Search for files
- `locate` - Find files by name
- `tree` - Display directory tree

### File Operations
- `cat` - Display file contents
- `head` - Display first lines of file
- `tail` - Display last lines of file
- `less` - Page through file
- `more` - Page through file
- `cp` - Copy files
- `mv` - Move/rename files
- `rm` - Remove files
- `mkdir` - Create directories
- `rmdir` - Remove directories
- `touch` - Create empty file or update timestamp
- `chmod` - Change file permissions
- `chown` - Change file ownership

### Text Processing
- `grep` - Search text patterns
- `sed` - Stream editor
- `awk` - Pattern scanning
- `sort` - Sort lines
- `uniq` - Filter duplicate lines
- `wc` - Word, line, character count
- `cut` - Remove sections from lines
- `paste` - Merge lines of files
- `tr` - Translate characters
- `diff` - Compare files

### System Information
- `uname` - System information
- `hostname` - Show hostname
- `uptime` - System uptime
- `whoami` - Current user
- `id` - User identity
- `df` - Disk space usage
- `du` - Directory space usage
- `free` - Memory usage
- `top` - Process monitor
- `ps` - Process status

### Network
- `ping` - Test connectivity
- `curl` - Transfer data
- `wget` - Download files
- `netstat` - Network statistics
- `ss` - Socket statistics
- `ip` - Network configuration

### Archives
- `tar` - Archive files
- `gzip` - Compress files
- `gunzip` - Decompress files
- `zip` - Create zip archive
- `unzip` - Extract zip archive

## Response Format

Always respond with a single, executable shell command.
Do not include explanations unless specifically asked.
If multiple commands are needed, chain them with && or use a script.

## Error Handling

If a command fails:
1. Analyze the error message
2. Suggest a corrected command
3. Explain why the original failed

## Safety Guidelines

- Never run destructive commands without confirmation
- Always use -i flag for interactive deletion when possible
- Prefer non-destructive alternatives when available
- Warn about irreversible operations

Remember: You are helping a user accomplish tasks efficiently and safely.
""" * 3  # Repeat to make it large enough for caching (~6000+ chars)


class PromptCachingAgent(Agent):
    """
    Agent demonstrating prompt caching for cost optimization.
    
    Uses multipart message format with cache_control to cache large prompts.
    """
    
    def setup(self):
        """Initialize agent with LLM client."""
        self.llm = LLM()
        self.conversation_history = []
    
    def run(self, ctx: AgentContext):
        """Execute the task using cached prompts."""
        ctx.log(f"Task: {ctx.instruction[:100]}...")
        
        # Initial exploration
        result = ctx.shell("ls -la")
        
        # Build messages with caching for the large system prompt
        # The system prompt is cached to reduce costs on subsequent requests
        messages = self._build_messages_with_cache(
            user_message=f"Task: {ctx.instruction}\n\nCurrent directory contents:\n{result.output}",
            last_user=True
        )
        
        # Make LLM call - the system prompt will be cached
        response = self.llm.chat(
            messages=messages,
            model="anthropic/claude-3.5-sonnet"
        )
        
        ctx.log(f"Response: {response.text[:200]}...")
        
        # Execute the suggested command
        cmd = response.text.strip().split('\n')[0].strip('`').strip()
        if cmd:
            ctx.log(f"Executing: {cmd}")
            result = ctx.shell(cmd)
            
            # Add to conversation history for context
            self.conversation_history.append({
                "role": "user",
                "content": f"Task: {ctx.instruction}"
            })
            self.conversation_history.append({
                "role": "assistant", 
                "content": response.text
            })
            
            # Follow-up if needed
            if result.failed:
                ctx.log(f"Command failed, asking for help...")
                
                # Build messages again - system prompt hits cache, saving tokens!
                messages = self._build_messages_with_cache(
                    user_message=f"The command failed with error:\n{result.output}\n\nPlease suggest a fix.",
                    last_user=True
                )
                
                response = self.llm.chat(
                    messages=messages,
                    model="anthropic/claude-3.5-sonnet"
                )
                ctx.log(f"Fix suggestion: {response.text[:200]}...")
        
        # Report cache savings
        stats = self.llm.get_stats()
        ctx.log(f"Total cost: ${stats['total_cost']:.4f}")
        ctx.log(f"Total tokens: {stats['total_tokens']}")
        
        ctx.done()
    
    def _build_messages_with_cache(
        self, 
        user_message: str,
        last_user: bool = False,
        cache_ttl: str = "5min"  # or "1h" for 1 hour
    ) -> list:
        """
        Build messages array with cache_control for the system prompt.
        
        Args:
            user_message: The current user message
            last_user: If True, this is the last user message (don't cache it)
            cache_ttl: Cache TTL - "5min" (default) or "1h"
        
        Returns:
            List of messages in multipart format with cache_control
        """
        # Build cache_control object
        cache_control = {"type": "ephemeral"}
        if cache_ttl == "1h":
            cache_control["ttl"] = "1h"
        
        messages = [
            # System prompt with cache_control - this gets cached!
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": LARGE_SYSTEM_PROMPT,
                        "cache_control": cache_control
                    }
                ]
            }
        ]
        
        # Add conversation history (could also cache if large and stable)
        for msg in self.conversation_history:
            messages.append(msg)
        
        # Add current user message (never cache the last user message)
        messages.append({
            "role": "user",
            "content": user_message  # Simple string, no cache_control
        })
        
        return messages
    
    def cleanup(self):
        """Report final stats."""
        stats = self.llm.get_stats()
        print("\n=== Caching Stats ===")
        print(f"Total requests: {stats['request_count']}")
        print(f"Total tokens: {stats['total_tokens']}")
        print(f"Total cost: ${stats['total_cost']:.4f}")
        print("\nNote: With caching enabled, the second request should")
        print("show reduced input tokens due to cached system prompt.")
        self.llm.close()


def example_manual_caching():
    """
    Standalone example showing manual cache_control usage.
    
    This example shows the message format without the agent framework.
    """
    from term_sdk import LLM
    
    llm = LLM()
    
    # Large content to cache (documentation, context, etc.)
    large_context = "..." * 1000  # Your large content here
    
    # Format 1: Cache in system message
    messages_with_system_cache = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": "You are a helpful assistant.",
                },
                {
                    "type": "text",
                    "text": large_context,
                    "cache_control": {"type": "ephemeral"}  # 5 min TTL
                }
            ]
        },
        {
            "role": "user",
            "content": "Summarize the context above."
        }
    ]
    
    # Format 2: Cache in user message (for RAG, uploaded docs, etc.)
    messages_with_user_cache = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Here is a document to analyze:"
                },
                {
                    "type": "text",
                    "text": large_context,
                    "cache_control": {"type": "ephemeral", "ttl": "1h"}  # 1 hour TTL
                },
                {
                    "type": "text",
                    "text": "What are the key points?"
                }
            ]
        }
    ]
    
    # Make the request - cache_control is preserved and sent to OpenRouter
    response = llm.chat(
        messages=messages_with_system_cache,
        model="anthropic/claude-3.5-sonnet"
    )
    
    print(f"Response: {response.text[:200]}...")
    print(f"Tokens: {response.tokens}, Cost: ${response.cost:.4f}")
    
    llm.close()


if __name__ == "__main__":
    run(PromptCachingAgent())
