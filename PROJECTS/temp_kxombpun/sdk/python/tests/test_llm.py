"""Tests for term_sdk LLM client."""

import pytest
from unittest.mock import patch, MagicMock
from term_sdk import LLM, LLMResponse, LLMError, Tool, FunctionCall


class TestLLMError:
    def test_basic_error(self):
        err = LLMError("rate_limit", "Too many requests")
        assert err.code == "rate_limit"
        assert err.message == "Too many requests"
        assert err.details == {}
    
    def test_error_with_details(self):
        err = LLMError("invalid_model", "Model not found", {"model": "gpt-5"})
        assert err.details["model"] == "gpt-5"
    
    def test_to_dict(self):
        err = LLMError("test", "Test error")
        d = err.to_dict()
        assert d["error"]["code"] == "test"
        assert d["error"]["message"] == "Test error"
    
    def test_to_json(self):
        err = LLMError("test", "Test")
        import json
        data = json.loads(err.to_json())
        assert "error" in data
    
    def test_str(self):
        err = LLMError("code", "message")
        assert "code" in str(err)
        assert "message" in str(err)


class TestLLMResponse:
    def test_basic_response(self):
        resp = LLMResponse(
            text="Hello",
            model="gpt-4",
            tokens=10,
            cost=0.001,
            latency_ms=100
        )
        assert resp.text == "Hello"
        assert resp.model == "gpt-4"
        assert resp.tokens == 10
    
    def test_json_parsing(self):
        resp = LLMResponse(
            text='{"command": "ls", "task_complete": false}',
            model="gpt-4"
        )
        data = resp.json()
        assert data["command"] == "ls"
    
    def test_json_from_markdown(self):
        resp = LLMResponse(
            text='```json\n{"key": "value"}\n```',
            model="gpt-4"
        )
        data = resp.json()
        assert data["key"] == "value"
    
    def test_has_function_calls(self):
        resp = LLMResponse(text="", model="gpt-4")
        assert resp.has_function_calls() is False
        
        resp.function_calls = [FunctionCall(name="test", arguments={})]
        assert resp.has_function_calls() is True


class TestLLM:
    def test_invalid_provider(self):
        with pytest.raises(LLMError) as exc:
            LLM(provider="invalid_provider")
        assert exc.value.code == "invalid_provider"
    
    def test_no_model_error(self):
        llm = LLM()
        with pytest.raises(LLMError) as exc:
            llm._get_model(None)
        assert exc.value.code == "no_model"
    
    def test_default_model(self):
        llm = LLM(default_model="gpt-4")
        model = llm._get_model(None)
        assert model == "gpt-4"
    
    def test_override_model(self):
        llm = LLM(default_model="gpt-4")
        model = llm._get_model("claude-3-haiku")
        assert model == "claude-3-haiku"
    
    def test_register_function(self):
        llm = LLM()
        
        def my_func(x: int) -> int:
            return x * 2
        
        llm.register_function("double", my_func)
        assert "double" in llm._function_handlers
    
    def test_execute_function(self):
        llm = LLM()
        llm.register_function("add", lambda a, b: a + b)
        
        call = FunctionCall(name="add", arguments={"a": 1, "b": 2})
        result = llm.execute_function(call)
        assert result == 3
    
    def test_execute_unknown_function(self):
        llm = LLM()
        call = FunctionCall(name="unknown", arguments={})
        with pytest.raises(LLMError) as exc:
            llm.execute_function(call)
        assert exc.value.code == "unknown_function"
    
    def test_get_stats_empty(self):
        llm = LLM()
        stats = llm.get_stats()
        assert stats["total_tokens"] == 0
        assert stats["total_cost"] == 0.0
    
    def test_get_stats_per_model(self):
        llm = LLM()
        llm._update_model_stats("gpt-4", 100, 0.01)
        llm._update_model_stats("gpt-4", 50, 0.005)
        
        stats = llm.get_stats("gpt-4")
        assert stats["tokens"] == 150
        assert stats["cost"] == 0.015
        assert stats["requests"] == 2
    
    def test_calculate_cost(self):
        llm = LLM()
        # gpt-4o: $5/1M input, $15/1M output
        cost = llm._calculate_cost("gpt-4o", 1000, 1000)
        expected = (1000 * 5 + 1000 * 15) / 1_000_000
        assert abs(cost - expected) < 0.0001
    
    def test_context_manager(self):
        with LLM() as llm:
            assert llm is not None
        # Should not raise after exit


class TestTool:
    def test_tool_to_dict(self):
        tool = Tool(
            name="search",
            description="Search files",
            parameters={
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                },
                "required": ["query"]
            }
        )
        d = tool.to_dict()
        assert d["type"] == "function"
        assert d["function"]["name"] == "search"
        assert d["function"]["parameters"]["required"] == ["query"]
