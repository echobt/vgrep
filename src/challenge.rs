//! Terminal Benchmark Challenge implementation for platform

use crate::evaluator::{AgentInfo, TaskEvaluator};
use crate::scoring::{Leaderboard, ScoreCalculator};
use crate::task::{Task, TaskRegistry, TaskResult};
use async_trait::async_trait;
use platform_challenge_sdk::prelude::*;
use platform_challenge_sdk::{ChallengeError, ChallengeRoute, Result, RouteRequest, RouteResponse};
use platform_core::Hotkey;
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::info;

/// Terminal Benchmark Challenge
///
/// This challenge evaluates AI agents on terminal-based tasks.
/// Agents compete by solving tasks in isolated Docker containers.
/// Scores are based on task completion rate and execution time.
pub struct TerminalBenchChallenge {
    /// Challenge ID
    id: ChallengeId,
    /// Challenge name
    name: String,
    /// Mechanism ID on Bittensor
    mechanism_id: u8,
    /// Emission weight
    emission_weight: f64,
    /// Task registry
    task_registry: Arc<RwLock<Option<TaskRegistry>>>,
    /// Score calculator
    score_calculator: ScoreCalculator,
    /// Leaderboard
    leaderboard: Arc<RwLock<Leaderboard>>,
    /// Tasks directory
    tasks_dir: PathBuf,
    /// Results cache (agent_hash -> results)
    results_cache: Arc<RwLock<HashMap<String, Vec<TaskResult>>>>,
    /// Number of tasks per evaluation
    tasks_per_evaluation: usize,
    /// Max concurrent evaluations
    max_concurrent: usize,
}

impl TerminalBenchChallenge {
    /// Get default routes (static method for registration without instance)
    pub fn default_routes() -> Vec<ChallengeRoute> {
        vec![
            // Agent submission
            ChallengeRoute::post("/submit", "Submit an agent (Python source code)"),
            ChallengeRoute::get("/can_submit", "Check if miner can submit"),
            // Agent status
            ChallengeRoute::get("/status/:hash", "Get agent submission status"),
            ChallengeRoute::get("/agent/:hash", "Get agent details"),
            ChallengeRoute::get("/agents/miner/:hotkey", "List agents for a miner"),
            ChallengeRoute::get("/agents/pending", "List pending agents"),
            ChallengeRoute::get("/agents/active", "List active agents"),
            // Configuration
            ChallengeRoute::get("/config", "Get challenge configuration"),
            ChallengeRoute::get("/whitelist", "Get module whitelist"),
            ChallengeRoute::get("/whitelist/modules", "Get allowed modules"),
            ChallengeRoute::get("/whitelist/models", "Get allowed LLM models"),
            ChallengeRoute::get("/pricing", "Get pricing limits"),
            // Stats and leaderboard
            ChallengeRoute::get("/stats", "Get submission statistics"),
            ChallengeRoute::get("/leaderboard", "Get current leaderboard"),
            // Progress tracking
            ChallengeRoute::get("/progress/:evaluation_id", "Get evaluation progress"),
            ChallengeRoute::get("/progress/agent/:hash", "Get agent's evaluation history"),
        ]
    }

    /// Create a new Terminal Benchmark Challenge
    pub fn new(
        name: impl Into<String>,
        mechanism_id: u8,
        emission_weight: f64,
        tasks_dir: PathBuf,
    ) -> Self {
        // Use a deterministic ID for development/testing
        // In production this might come from configuration or be randomized
        let id_str = "00000000-0000-0000-0000-000000000001";
        let id = ChallengeId::from_str(id_str).unwrap_or_default();

        Self {
            id,
            name: name.into(),
            mechanism_id,
            emission_weight,
            task_registry: Arc::new(RwLock::new(None)),
            score_calculator: ScoreCalculator,
            leaderboard: Arc::new(RwLock::new(Leaderboard::default())),
            tasks_dir,
            results_cache: Arc::new(RwLock::new(HashMap::new())),
            tasks_per_evaluation: 10, // Evaluate on 10 random tasks by default
            max_concurrent: 4,
        }
    }

    /// Set the number of tasks per evaluation
    pub fn with_tasks_per_evaluation(mut self, n: usize) -> Self {
        self.tasks_per_evaluation = n;
        self
    }

    /// Set max concurrent evaluations
    pub fn with_max_concurrent(mut self, n: usize) -> Self {
        self.max_concurrent = n;
        self
    }

    /// Get the task registry
    async fn registry(
        &self,
    ) -> anyhow::Result<tokio::sync::RwLockReadGuard<'_, Option<TaskRegistry>>> {
        let guard = self.task_registry.read().await;
        if guard.is_none() {
            drop(guard);
            self.load_tasks().await?;
            return Ok(self.task_registry.read().await);
        }
        Ok(guard)
    }

    /// Load tasks from directory
    async fn load_tasks(&self) -> anyhow::Result<()> {
        let registry = TaskRegistry::new(self.tasks_dir.clone())?;
        info!("Loaded {} tasks for Terminal Benchmark", registry.count());

        let mut guard = self.task_registry.write().await;
        *guard = Some(registry);
        Ok(())
    }

    /// Record evaluation results from external source
    pub async fn record_evaluation_result(
        &self,
        agent_hash: String,
        miner_hotkey: String,
        results: Vec<TaskResult>,
    ) {
        // Cache results
        {
            let mut cache = self.results_cache.write().await;
            cache.insert(agent_hash.clone(), results.clone());
        }

        // Update leaderboard
        // We need to fetch tasks to calculate aggregate
        if let Ok(registry_guard) = self.registry().await {
            if let Some(registry) = registry_guard.as_ref() {
                let tasks: Vec<&Task> = results
                    .iter()
                    .filter_map(|r| registry.get(&r.task_id))
                    .collect();

                let aggregate = self.score_calculator.calculate_aggregate(&tasks, &results);
                {
                    let mut lb = self.leaderboard.write().await;
                    lb.update(agent_hash, miner_hotkey, aggregate);
                }
            }
        }
    }

    /// Run evaluation for an agent
    async fn run_evaluation(&self, agent: &AgentInfo) -> anyhow::Result<Vec<TaskResult>> {
        let registry_guard = self.registry().await?;
        let registry = registry_guard
            .as_ref()
            .ok_or_else(|| anyhow::anyhow!("Task registry not loaded"))?;

        // Get random tasks for evaluation
        let tasks = registry.random_tasks(self.tasks_per_evaluation);

        if tasks.is_empty() {
            return Err(anyhow::anyhow!("No tasks available for evaluation"));
        }

        info!(
            "Running evaluation on {} tasks for agent {}",
            tasks.len(),
            agent.hash
        );

        // Create evaluator
        let evaluator = TaskEvaluator::new(self.max_concurrent).await?;

        // Run evaluation
        let results = evaluator.evaluate_tasks(&tasks, agent).await;

        // Cache results
        {
            let mut cache = self.results_cache.write().await;
            cache.insert(agent.hash.clone(), results.clone());
        }

        // Update leaderboard
        let aggregate = self.score_calculator.calculate_aggregate(&tasks, &results);
        {
            let mut lb = self.leaderboard.write().await;
            lb.update(agent.hash.clone(), agent.miner_hotkey.clone(), aggregate);
        }

        Ok(results)
    }

    /// Get cached results for an agent (for future use in weight calculations)
    #[allow(dead_code)]
    async fn get_cached_results(&self, agent_hash: &str) -> Option<Vec<TaskResult>> {
        let cache = self.results_cache.read().await;
        cache.get(agent_hash).cloned()
    }

    /// Calculate weights from leaderboard
    async fn calculate_weights_from_leaderboard(&self) -> Vec<WeightAssignment> {
        let leaderboard = self.leaderboard.read().await;
        let entries = leaderboard.all();

        if entries.is_empty() {
            return Vec::new();
        }

        // Calculate total normalized score
        let total_score: f64 = entries.iter().map(|e| e.score.normalized_score).sum();

        if total_score == 0.0 {
            return Vec::new();
        }

        // Assign weights proportional to normalized scores
        // Use miner_hotkey (SS58 address) for weight assignment
        entries
            .iter()
            .map(|entry| {
                let weight = entry.score.normalized_score / total_score;
                WeightAssignment::new(entry.miner_hotkey.clone(), weight)
            })
            .collect()
    }
}

#[async_trait]
impl Challenge for TerminalBenchChallenge {
    fn id(&self) -> ChallengeId {
        self.id
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn description(&self) -> &str {
        "Terminal Benchmark Challenge - AI agents compete on terminal-based tasks"
    }

    fn version(&self) -> &str {
        env!("CARGO_PKG_VERSION")
    }

    fn emission_weight(&self) -> f64 {
        self.emission_weight
    }

    async fn on_startup(&self, _ctx: &ChallengeContext) -> Result<()> {
        info!("Terminal Benchmark Challenge starting up");
        self.load_tasks()
            .await
            .map_err(|e| ChallengeError::Internal(e.to_string()))?;
        Ok(())
    }

    async fn evaluate(
        &self,
        ctx: &ChallengeContext,
        agent: &platform_challenge_sdk::AgentInfo,
        payload: serde_json::Value,
    ) -> Result<EvaluationResult> {
        info!("Evaluating agent {} for Terminal Benchmark", agent.hash);

        // Extract agent image from payload or metadata
        let agent_image = payload
            .get("image")
            .and_then(|v| v.as_str())
            .unwrap_or(&agent.hash);

        // Get miner hotkey from agent owner
        let miner_hotkey = agent
            .owner
            .as_ref()
            .map(|h| h.to_ss58())
            .unwrap_or_default();

        let agent_info = AgentInfo {
            hash: agent.hash.clone(),
            miner_hotkey,
            image: agent_image.to_string(),
            endpoint: payload
                .get("endpoint")
                .and_then(|v| v.as_str())
                .map(String::from),
            source_code: None,
            language: None,
            env_vars: Vec::new(),
        };

        // Run evaluation
        let results = self
            .run_evaluation(&agent_info)
            .await
            .map_err(|e| ChallengeError::Evaluation(e.to_string()))?;

        // Calculate aggregate score
        let registry_guard = self
            .registry()
            .await
            .map_err(|e| ChallengeError::Internal(e.to_string()))?;
        let registry = registry_guard
            .as_ref()
            .ok_or_else(|| ChallengeError::Internal("Registry not loaded".to_string()))?;

        let tasks: Vec<&Task> = results
            .iter()
            .filter_map(|r| registry.get(&r.task_id))
            .collect();

        let aggregate = self.score_calculator.calculate_aggregate(&tasks, &results);
        let score = self.score_calculator.to_weight(&aggregate);

        // Build metrics
        let mut metrics = HashMap::new();
        metrics.insert("tasks_passed".to_string(), aggregate.tasks_passed as f64);
        metrics.insert("tasks_failed".to_string(), aggregate.tasks_failed as f64);
        metrics.insert("pass_rate".to_string(), aggregate.pass_rate);
        metrics.insert("normalized_score".to_string(), aggregate.normalized_score);

        info!(
            "Agent {} evaluation complete: score={:.4}, passed={}/{}",
            agent.hash,
            score,
            aggregate.tasks_passed,
            aggregate.total_tasks()
        );

        Ok(EvaluationResult::new(ctx.job_id(), agent.hash.clone(), score).with_metrics(metrics))
    }

    async fn calculate_weights(&self, _ctx: &ChallengeContext) -> Result<Vec<WeightAssignment>> {
        info!("Calculating weights for Terminal Benchmark");

        let weights = self.calculate_weights_from_leaderboard().await;

        info!("Calculated {} weight assignments", weights.len());
        Ok(weights)
    }

    async fn validate_agent(
        &self,
        _ctx: &ChallengeContext,
        agent: &platform_challenge_sdk::AgentInfo,
    ) -> Result<bool> {
        // Basic validation: agent hash should be valid
        if agent.hash.is_empty() {
            return Ok(false);
        }

        // Check if agent has required metadata (optional)
        // In production, you might validate the Docker image exists, etc.
        Ok(true)
    }

    fn metadata(&self) -> ChallengeMetadata {
        ChallengeMetadata {
            id: self.id,
            name: self.name.clone(),
            description: self.description().to_string(),
            version: self.version().to_string(),
            owner: Hotkey([0u8; 32]), // Will be set by runtime
            emission_weight: self.emission_weight,
            config: ChallengeConfig::with_mechanism(self.mechanism_id),
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            is_active: true,
        }
    }

    /// Custom routes for agent submission and status
    fn routes(&self) -> Vec<ChallengeRoute> {
        vec![
            // Agent submission
            ChallengeRoute::post("/submit", "Submit an agent (Python source code)"),
            ChallengeRoute::get("/can_submit", "Check if miner can submit"),
            // Agent status
            ChallengeRoute::get("/status/:hash", "Get agent submission status"),
            ChallengeRoute::get("/agent/:hash", "Get agent details"),
            ChallengeRoute::get("/agents/miner/:hotkey", "List agents for a miner"),
            ChallengeRoute::get("/agents/pending", "List pending agents"),
            ChallengeRoute::get("/agents/active", "List active agents"),
            // Configuration
            ChallengeRoute::get("/config", "Get challenge configuration"),
            ChallengeRoute::get("/whitelist", "Get module whitelist"),
            ChallengeRoute::get("/whitelist/modules", "Get allowed modules"),
            ChallengeRoute::get("/whitelist/models", "Get allowed LLM models"),
            ChallengeRoute::get("/pricing", "Get pricing limits"),
            // Stats and leaderboard
            ChallengeRoute::get("/stats", "Get submission statistics"),
            ChallengeRoute::get("/leaderboard", "Get current leaderboard"),
            // Progress tracking
            ChallengeRoute::get("/progress/:evaluation_id", "Get evaluation progress"),
            ChallengeRoute::get("/progress/agent/:hash", "Get agent's evaluation history"),
        ]
    }

    /// Handle incoming requests to custom routes
    async fn handle_route(&self, _ctx: &ChallengeContext, req: RouteRequest) -> RouteResponse {
        match (req.method.as_str(), req.path.as_str()) {
            // Leaderboard
            ("GET", "/leaderboard") => {
                let leaderboard = self.leaderboard.read().await;
                let entries = leaderboard.all();
                RouteResponse::json(entries)
            }

            // Stats
            ("GET", "/stats") => {
                let leaderboard = self.leaderboard.read().await;
                let entries = leaderboard.all();
                RouteResponse::json(serde_json::json!({
                    "total_agents": entries.len(),
                    "active_agents": entries.iter().filter(|e| e.score.pass_rate > 0.0).count(),
                    "tasks_available": self.tasks_per_evaluation,
                }))
            }

            // Configuration
            ("GET", "/config") => RouteResponse::json(serde_json::json!({
                "name": self.name,
                "mechanism_id": self.mechanism_id,
                "emission_weight": self.emission_weight,
                "tasks_per_evaluation": self.tasks_per_evaluation,
                "max_concurrent": self.max_concurrent,
            })),

            // Whitelist
            ("GET", "/whitelist") | ("GET", "/whitelist/modules") => {
                let config = crate::ChallengeConfig::default();
                RouteResponse::json(config.module_whitelist)
            }

            ("GET", "/whitelist/models") => {
                let config = crate::ChallengeConfig::default();
                RouteResponse::json(config.model_whitelist)
            }

            ("GET", "/pricing") => {
                let config = crate::ChallengeConfig::default();
                RouteResponse::json(config.pricing)
            }

            // Agent details by hash
            ("GET", path) if path.starts_with("/agent/") => {
                let hash = req.param("hash").unwrap_or_default();
                let leaderboard = self.leaderboard.read().await;
                if let Some(entry) = leaderboard.get(hash) {
                    RouteResponse::json(entry)
                } else {
                    RouteResponse::not_found()
                }
            }

            // Pending/active agents (simplified - would use registry in production)
            ("GET", "/agents/pending") | ("GET", "/agents/active") => {
                let leaderboard = self.leaderboard.read().await;
                RouteResponse::json(leaderboard.all())
            }

            // Submit agent - delegates to AgentSubmissionHandler in production
            ("POST", "/submit") => {
                // In production, this would validate and register the agent
                // For now, return instructions
                RouteResponse::json(serde_json::json!({
                    "message": "Agent submission endpoint",
                    "required_fields": {
                        "source_code": "Python source code",
                        "miner_hotkey": "Hex-encoded miner hotkey",
                        "signature": "Hex-encoded signature",
                        "stake": "Stake in RAO"
                    }
                }))
            }

            // Can submit check
            ("GET", "/can_submit") => {
                let hotkey = req.query_param("miner_hotkey").unwrap_or_default();
                let stake: u64 = req
                    .query_param("stake")
                    .and_then(|s| s.parse().ok())
                    .unwrap_or(0);

                let min_stake = 1000 * 1_000_000_000u64; // 1000 TAO
                let allowed = stake >= min_stake;

                RouteResponse::json(serde_json::json!({
                    "allowed": allowed,
                    "reason": if allowed { None } else { Some("Insufficient stake") },
                    "min_stake_tao": 1000,
                    "your_stake_tao": stake as f64 / 1_000_000_000.0,
                }))
            }

            _ => RouteResponse::not_found(),
        }
    }
}

/// Create the Terminal Benchmark challenge with default settings
pub fn create_terminal_bench_challenge(
    mechanism_id: u8,
    emission_weight: f64,
    tasks_dir: PathBuf,
) -> TerminalBenchChallenge {
    TerminalBenchChallenge::new(
        "Terminal Benchmark",
        mechanism_id,
        emission_weight,
        tasks_dir,
    )
    .with_tasks_per_evaluation(10)
    .with_max_concurrent(4)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_challenge_creation() {
        let challenge = create_terminal_bench_challenge(1, 0.5, PathBuf::from("./tasks"));

        assert_eq!(challenge.name(), "Terminal Benchmark");
        assert_eq!(challenge.emission_weight(), 0.5);
    }
}
