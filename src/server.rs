//! Always-On Challenge Server - Production Ready
//!
//! This module implements the challenge container server for Terminal-Bench evaluations.
//!
//! Architecture:
//! ```text
//! Challenge Container (always-on)
//!  ├── Service Mode (continuous)
//!  │   └── POST /evaluate → Run agent on real tasks → Return results
//!  └── Weights Mode (epoch-triggered)
//!      └── GET /get_weights → Read-only, deterministic
//! ```
//!
//! Datasets:
//! - Production: terminal-bench 2.0 (89 tasks)
//! - Testing: hello-world (1 task)

use crate::bench::external_agent::ExternalAgent;
use crate::bench::registry::{Dataset, RegistryClient, TaskSource};
use crate::bench::runner::{TrialConfig, TrialRunner};
use crate::bench::task::Task;
use crate::central_client::PlatformClient;
use crate::config::ChallengeConfig;
use crate::llm_review::{LlmConfig, LlmProvider, LlmReviewManager};
use crate::pg_storage::PgStorage;
use crate::python_whitelist::{PythonWhitelist, WhitelistConfig};
use axum::{
    extract::{Query, State},
    http::StatusCode,
    routing::{get, post},
    Json, Router,
};
use rand::seq::SliceRandom;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::RwLock;
use tower_http::cors::{Any, CorsLayer};
use tower_http::trace::TraceLayer;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

// ============================================================================
// CONSTANTS
// ============================================================================

/// Default dataset for production evaluations
pub const DEFAULT_DATASET: &str = "terminal-bench";
pub const DEFAULT_DATASET_VERSION: &str = "2.0";

/// Test dataset for quick validation
pub const TEST_DATASET: &str = "hello-world";
pub const TEST_DATASET_VERSION: &str = "head";

/// Registry URL
pub const REGISTRY_URL: &str = "https://raw.githubusercontent.com/laude-institute/harbor/83745559edb7b1e6f21483a90604f83e201c4a10/registry.json";

// ============================================================================
// SERVER STATE
// ============================================================================

pub struct ChallengeServerState {
    pub config: RwLock<ChallengeConfig>,
    pub platform_client: PlatformClient,
    pub challenge_id: String,
    pub whitelist: PythonWhitelist,
    pub llm_manager: RwLock<Option<LlmReviewManager>>,
    pub registry_client: RwLock<RegistryClient>,
    pub cached_tasks: RwLock<HashMap<String, Vec<PathBuf>>>,
    pub test_mode: bool,
    /// PostgreSQL storage for server mode (subnet owner)
    /// None = validator mode (uses platform API), Some = server mode (local PostgreSQL)
    pub pg_storage: Option<PgStorage>,
}

impl ChallengeServerState {
    pub fn new(config: ChallengeConfig, platform_url: &str, challenge_id: &str) -> Self {
        Self::with_options(config, platform_url, challenge_id, false, None)
    }

    pub fn with_mode(
        config: ChallengeConfig,
        platform_url: &str,
        challenge_id: &str,
        test_mode: bool,
    ) -> Self {
        Self::with_options(config, platform_url, challenge_id, test_mode, None)
    }

    pub fn with_options(
        config: ChallengeConfig,
        platform_url: &str,
        challenge_id: &str,
        test_mode: bool,
        pg_storage: Option<PgStorage>,
    ) -> Self {
        let whitelist_config = WhitelistConfig {
            allowed_stdlib: config.module_whitelist.allowed_stdlib.clone(),
            allowed_third_party: config.module_whitelist.allowed_third_party.clone(),
            ..Default::default()
        };
        let whitelist = PythonWhitelist::new(whitelist_config);

        Self {
            config: RwLock::new(config),
            platform_client: PlatformClient::new(platform_url),
            challenge_id: challenge_id.to_string(),
            whitelist,
            llm_manager: RwLock::new(None),
            registry_client: RwLock::new(RegistryClient::with_url(REGISTRY_URL)),
            cached_tasks: RwLock::new(HashMap::new()),
            test_mode,
            pg_storage,
        }
    }

    /// Check if running in server mode (with PostgreSQL storage)
    pub fn is_server_mode(&self) -> bool {
        self.pg_storage.is_some()
    }

    /// Create LLM review manager with miner's API key
    pub fn create_llm_manager(&self, api_key: &str, provider: &str) -> LlmReviewManager {
        let llm_provider = LlmProvider::parse(provider);
        let llm_config = LlmConfig::for_provider(llm_provider, api_key.to_string());
        LlmReviewManager::new(llm_config, self.challenge_id.clone())
    }

    /// Get dataset name based on mode
    pub fn dataset_name(&self) -> &str {
        if self.test_mode {
            TEST_DATASET
        } else {
            DEFAULT_DATASET
        }
    }

    /// Get dataset version based on mode
    pub fn dataset_version(&self) -> &str {
        if self.test_mode {
            TEST_DATASET_VERSION
        } else {
            DEFAULT_DATASET_VERSION
        }
    }

    /// Download and cache tasks for the current dataset
    pub async fn ensure_tasks_cached(&self) -> anyhow::Result<Vec<PathBuf>> {
        let dataset_key = format!("{}@{}", self.dataset_name(), self.dataset_version());

        // Check cache first
        {
            let cache = self.cached_tasks.read().await;
            if let Some(tasks) = cache.get(&dataset_key) {
                return Ok(tasks.clone());
            }
        }

        // Download tasks
        info!("Downloading tasks for dataset: {}", dataset_key);
        let mut registry = self.registry_client.write().await;

        let task_paths = registry
            .download_dataset(self.dataset_name(), self.dataset_version(), false)
            .await?;
        info!("Downloaded {} tasks", task_paths.len());

        // Cache tasks
        {
            let mut cache = self.cached_tasks.write().await;
            cache.insert(dataset_key, task_paths.clone());
        }

        Ok(task_paths)
    }
}

// ============================================================================
// /get_weights ENDPOINT
// ============================================================================

#[derive(Debug, Deserialize)]
pub struct GetWeightsQuery {
    pub epoch: Option<u64>,
}

#[derive(Debug, Serialize)]
pub struct GetWeightsResponse {
    pub epoch: u64,
    pub weights: Vec<WeightEntry>,
}

#[derive(Debug, Serialize)]
pub struct WeightEntry {
    pub hotkey: String,
    pub weight: f64,
}

/// GET /get_weights - Deterministic weight calculation
pub async fn get_weights(
    State(state): State<Arc<ChallengeServerState>>,
    Query(query): Query<GetWeightsQuery>,
) -> Result<Json<GetWeightsResponse>, (StatusCode, String)> {
    let snapshot = state
        .platform_client
        .get_snapshot(query.epoch)
        .await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;

    let epoch = snapshot.epoch;
    let mut weights = Vec::new();
    let total_score: f64 = snapshot
        .leaderboard
        .iter()
        .map(|e| e.consensus_score.max(0.0))
        .sum();

    if total_score > 0.0 {
        for entry in &snapshot.leaderboard {
            if entry.consensus_score > 0.0 {
                let weight = (entry.consensus_score / total_score) * 0.9;
                weights.push(WeightEntry {
                    hotkey: entry.miner_hotkey.clone(),
                    weight,
                });
            }
        }
    }

    let total_weight: f64 = weights.iter().map(|w| w.weight).sum();
    info!(
        "Computed weights for epoch {}: {} miners, total weight: {:.4}",
        epoch,
        weights.len(),
        total_weight
    );

    Ok(Json(GetWeightsResponse { epoch, weights }))
}

// ============================================================================
// /evaluate ENDPOINT - Production Ready
// ============================================================================

#[derive(Debug, Deserialize)]
pub struct EvaluateRequest {
    pub submission_id: String,
    pub agent_hash: String,
    pub miner_hotkey: String,
    pub validator_hotkey: String,
    pub name: Option<String>,
    pub source_code: String,
    /// Deprecated: API key is now looked up from platform-server
    #[serde(default)]
    pub api_key: Option<String>,
    /// Deprecated: Provider is now looked up from platform-server
    #[serde(default)]
    pub api_provider: Option<String>,
    pub epoch: u64,
}

#[derive(Debug, Serialize)]
pub struct EvaluateResponse {
    pub success: bool,
    pub error: Option<String>,
    pub score: f64,
    pub tasks_passed: u32,
    pub tasks_total: u32,
    pub tasks_failed: u32,
    pub total_cost_usd: f64,
    pub execution_time_ms: i64,
    pub task_results: Option<Vec<TaskResultResponse>>,
    pub execution_log: Option<String>,
}

#[derive(Debug, Clone, Serialize)]
pub struct TaskResultResponse {
    pub task_id: String,
    pub task_name: String,
    pub passed: bool,
    pub score: f64,
    pub execution_time_ms: i64,
    pub steps: u32,
    pub error: Option<String>,
}

/// POST /evaluate - Evaluate agent on real Terminal-Bench tasks
pub async fn evaluate_agent(
    State(state): State<Arc<ChallengeServerState>>,
    Json(req): Json<EvaluateRequest>,
) -> Result<Json<EvaluateResponse>, (StatusCode, String)> {
    let start = std::time::Instant::now();
    let config = state.config.read().await;

    let agent_name = req.name.as_deref().unwrap_or("unnamed");
    let agent_hash_short = &req.agent_hash[..16.min(req.agent_hash.len())];

    info!(
        "Evaluating agent: {} (hash: {}) from {} [dataset: {}]",
        agent_name,
        agent_hash_short,
        &req.miner_hotkey[..16.min(req.miner_hotkey.len())],
        state.dataset_name()
    );

    // Step 1: Whitelist validation
    let verification = state.whitelist.verify(&req.source_code);
    if !verification.valid {
        warn!(
            "Agent {} failed whitelist validation: {:?}",
            agent_hash_short, verification.errors
        );
        return Ok(Json(EvaluateResponse {
            success: false,
            error: Some(format!("Whitelist violations: {:?}", verification.errors)),
            score: 0.0,
            tasks_passed: 0,
            tasks_total: 0,
            tasks_failed: 0,
            total_cost_usd: 0.0,
            execution_time_ms: start.elapsed().as_millis() as i64,
            task_results: None,
            execution_log: Some(format!("Rejected: {:?}", verification.errors)),
        }));
    }

    // Step 2: LLM Code Review via centralized platform-server
    let mut total_cost_usd = 0.0;
    let platform_llm = crate::platform_llm::PlatformLlmClient::for_agent(
        state.platform_client.base_url(),
        &req.agent_hash,
        &req.validator_hotkey,
    );

    if let Ok(llm_client) = platform_llm {
        // Create review prompt
        let review_prompt = format!(
            "Review this Python agent code for security and compliance. \
             Check for: dangerous imports, network access, file system access, \
             code injection, infinite loops, resource abuse. \
             Respond with JSON: {{\"approved\": true/false, \"reason\": \"...\", \"violations\": []}}\n\n\
             Code:\n```python\n{}\n```",
            &req.source_code
        );

        let messages = vec![
            crate::platform_llm::ChatMessage::system(
                "You are a security reviewer for AI agent code. Be strict about security.",
            ),
            crate::platform_llm::ChatMessage::user(&review_prompt),
        ];

        match llm_client.chat_with_usage(messages).await {
            Ok(response) => {
                total_cost_usd += response.cost_usd.unwrap_or(0.0);

                if let Some(content) = &response.content {
                    // Parse review result
                    if let Ok(review) = serde_json::from_str::<serde_json::Value>(content) {
                        let approved = review["approved"].as_bool().unwrap_or(true);
                        let reason = review["reason"].as_str().unwrap_or("Unknown");

                        if !approved {
                            warn!("Agent {} failed LLM review: {}", agent_hash_short, reason);
                            return Ok(Json(EvaluateResponse {
                                success: false,
                                error: Some(format!("LLM Review rejected: {}", reason)),
                                score: 0.0,
                                tasks_passed: 0,
                                tasks_total: 0,
                                tasks_failed: 0,
                                total_cost_usd,
                                execution_time_ms: start.elapsed().as_millis() as i64,
                                task_results: None,
                                execution_log: Some(format!("LLM Review: {}", reason)),
                            }));
                        }
                        info!("Agent {} passed LLM review", agent_hash_short);
                    }
                }
            }
            Err(e) => {
                warn!("LLM review failed (continuing): {}", e);
                // Continue without review on error (graceful degradation)
            }
        }
    } else {
        warn!("Could not create platform LLM client, skipping review");
    }

    // Step 3: Download/cache tasks
    let task_paths = match state.ensure_tasks_cached().await {
        Ok(paths) => paths,
        Err(e) => {
            error!("Failed to download tasks: {}", e);
            return Ok(Json(EvaluateResponse {
                success: false,
                error: Some(format!("Failed to download tasks: {}", e)),
                score: 0.0,
                tasks_passed: 0,
                tasks_total: 0,
                tasks_failed: 0,
                total_cost_usd,
                execution_time_ms: start.elapsed().as_millis() as i64,
                task_results: None,
                execution_log: None,
            }));
        }
    };

    // Step 4: Select tasks for evaluation
    let tasks_per_eval = config.evaluation.tasks_per_evaluation.min(task_paths.len());
    let selected_tasks: Vec<_> = if task_paths.len() <= tasks_per_eval {
        task_paths.clone()
    } else {
        let mut rng = rand::thread_rng();
        let mut shuffled = task_paths.clone();
        shuffled.shuffle(&mut rng);
        shuffled.into_iter().take(tasks_per_eval).collect()
    };

    info!(
        "Running {} tasks for agent {}",
        selected_tasks.len(),
        agent_hash_short
    );

    // Step 5: Execute agent on each task
    let mut task_results = Vec::new();
    let mut tasks_passed = 0u32;
    let mut tasks_failed = 0u32;
    let mut execution_log = String::new();

    // Create output directory for this evaluation
    let output_dir = PathBuf::from("/tmp/term-challenge-evals")
        .join(&req.submission_id)
        .join(&req.agent_hash[..16.min(req.agent_hash.len())]);

    for task_path in &selected_tasks {
        let task_start = std::time::Instant::now();
        let task_name = task_path
            .file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("unknown")
            .to_string();

        info!("Running task: {}", task_name);

        // Load task
        let task = match Task::from_path(task_path) {
            Ok(t) => t,
            Err(e) => {
                error!("Failed to load task {}: {}", task_name, e);
                task_results.push(TaskResultResponse {
                    task_id: Uuid::new_v4().to_string(),
                    task_name: task_name.clone(),
                    passed: false,
                    score: 0.0,
                    execution_time_ms: task_start.elapsed().as_millis() as i64,
                    steps: 0,
                    error: Some(format!("Failed to load task: {}", e)),
                });
                tasks_failed += 1;
                continue;
            }
        };

        // Create external agent from source code
        let agent = match ExternalAgent::from_source(
            &req.source_code,
            agent_name.to_string(),
            req.api_key.clone(),
            req.api_provider.clone(),
        )
        .await
        {
            Ok(a) => a,
            Err(e) => {
                error!("Failed to create agent for task {}: {}", task_name, e);
                task_results.push(TaskResultResponse {
                    task_id: Uuid::new_v4().to_string(),
                    task_name: task_name.clone(),
                    passed: false,
                    score: 0.0,
                    execution_time_ms: task_start.elapsed().as_millis() as i64,
                    steps: 0,
                    error: Some(format!("Failed to create agent: {}", e)),
                });
                tasks_failed += 1;
                continue;
            }
        };

        // Configure trial
        let trial_config = TrialConfig {
            trial_name: format!(
                "{}-{}",
                &req.agent_hash[..8.min(req.agent_hash.len())],
                task_name
            ),
            output_dir: output_dir.clone(),
            max_steps: config.evaluation.max_steps_per_task.unwrap_or(100),
            timeout_multiplier: 1.0,
            force_build: false,
            delete_container: true,
            agent_provider: req.api_provider.clone(),
            model_name: None,
        };

        // Run trial
        let runner = TrialRunner::new(trial_config);
        match runner.run(&task, &agent).await {
            Ok(result) => {
                let passed = result.success();
                let score = result.reward();
                let task_time = task_start.elapsed().as_millis() as i64;

                execution_log.push_str(&format!(
                    "Task {}: {} (score: {:.2}, steps: {}, time: {}ms)\n",
                    task_name,
                    if passed { "PASS" } else { "FAIL" },
                    score,
                    result.steps,
                    task_time
                ));

                if passed {
                    tasks_passed += 1;
                } else {
                    tasks_failed += 1;
                }

                task_results.push(TaskResultResponse {
                    task_id: Uuid::new_v4().to_string(),
                    task_name,
                    passed,
                    score,
                    execution_time_ms: task_time,
                    steps: result.steps,
                    error: result.error,
                });

                // Add LLM cost if agent used API
                if req.api_key.is_some() {
                    total_cost_usd += estimate_task_cost(result.steps);
                }
            }
            Err(e) => {
                error!("Task {} failed: {}", task_name, e);
                execution_log.push_str(&format!("Task {}: ERROR - {}\n", task_name, e));
                tasks_failed += 1;
                task_results.push(TaskResultResponse {
                    task_id: Uuid::new_v4().to_string(),
                    task_name,
                    passed: false,
                    score: 0.0,
                    execution_time_ms: task_start.elapsed().as_millis() as i64,
                    steps: 0,
                    error: Some(e.to_string()),
                });
            }
        }

        // Cleanup agent container
        if let Err(e) = agent.cleanup().await {
            warn!("Failed to cleanup agent container: {}", e);
        }
    }

    // Calculate final score
    let tasks_total = selected_tasks.len() as u32;
    let score = if tasks_total > 0 {
        tasks_passed as f64 / tasks_total as f64
    } else {
        0.0
    };

    let execution_time_ms = start.elapsed().as_millis() as i64;

    info!(
        "Evaluation complete for {}: score={:.2}, passed={}/{}, cost=${:.4}, time={}ms",
        agent_hash_short, score, tasks_passed, tasks_total, total_cost_usd, execution_time_ms
    );

    // Store evaluation in PostgreSQL if in server mode
    if let Some(pg) = &state.pg_storage {
        let eval_record = crate::pg_storage::EvaluationRecord {
            id: Uuid::new_v4().to_string(),
            submission_id: req.submission_id.clone(),
            agent_hash: req.agent_hash.clone(),
            miner_hotkey: req.miner_hotkey.clone(),
            score,
            tasks_passed: tasks_passed as i32,
            tasks_total: tasks_total as i32,
            tasks_failed: tasks_failed as i32,
            total_cost_usd,
            execution_time_ms: Some(execution_time_ms),
            task_results: Some(serde_json::to_value(&task_results).unwrap_or_default()),
            created_at: chrono::Utc::now().timestamp(),
        };

        if let Err(e) = pg.store_evaluation(&eval_record).await {
            error!("Failed to store evaluation in PostgreSQL: {}", e);
        } else {
            debug!("Stored evaluation {} in PostgreSQL", eval_record.id);
        }
    }

    Ok(Json(EvaluateResponse {
        success: true,
        error: None,
        score,
        tasks_passed,
        tasks_total,
        tasks_failed,
        total_cost_usd,
        execution_time_ms,
        task_results: Some(task_results),
        execution_log: Some(execution_log),
    }))
}

/// Estimate cost for LLM code review based on provider
fn estimate_review_cost(provider: &str) -> f64 {
    match provider.to_lowercase().as_str() {
        "openrouter" | "anthropic" | "claude" => 0.003,
        "openai" => 0.002,
        "chutes" | "deepseek" => 0.0005,
        "grok" => 0.002,
        _ => 0.002,
    }
}

/// Estimate cost per task step (LLM calls)
fn estimate_task_cost(steps: u32) -> f64 {
    // Average ~$0.002 per step for LLM calls
    (steps as f64) * 0.002
}

// ============================================================================
// /validate ENDPOINT
// ============================================================================

#[derive(Debug, Deserialize)]
pub struct ValidateRequest {
    pub source_code: String,
}

#[derive(Debug, Serialize)]
pub struct ValidateResponse {
    pub valid: bool,
    pub errors: Vec<String>,
}

pub async fn validate_source(
    State(state): State<Arc<ChallengeServerState>>,
    Json(req): Json<ValidateRequest>,
) -> Json<ValidateResponse> {
    let verification = state.whitelist.verify(&req.source_code);
    Json(ValidateResponse {
        valid: verification.valid,
        errors: verification.errors,
    })
}

// ============================================================================
// /config ENDPOINT
// ============================================================================

pub async fn get_config(State(state): State<Arc<ChallengeServerState>>) -> Json<serde_json::Value> {
    let config = state.config.read().await;
    Json(serde_json::json!({
        "challenge_id": state.challenge_id,
        "dataset": state.dataset_name(),
        "dataset_version": state.dataset_version(),
        "test_mode": state.test_mode,
        "tasks_per_evaluation": config.evaluation.tasks_per_evaluation,
        "max_steps_per_task": config.evaluation.max_steps_per_task,
        "max_concurrent_tasks": config.evaluation.max_concurrent_tasks_per_agent,
        "max_cost_per_task_usd": config.pricing.max_cost_per_task_usd,
        "max_total_cost_usd": config.pricing.max_total_cost_usd,
        "min_stake_tao": config.min_stake_tao,
    }))
}

// ============================================================================
// /leaderboard ENDPOINT
// ============================================================================

#[derive(Debug, Deserialize)]
pub struct LeaderboardQuery {
    pub limit: Option<usize>,
}

#[derive(Debug, Serialize)]
pub struct LeaderboardResponse {
    pub challenge_id: String,
    pub entries: Vec<LeaderboardEntryResponse>,
    pub total_count: usize,
}

#[derive(Debug, Serialize)]
pub struct LeaderboardEntryResponse {
    pub rank: u32,
    pub agent_hash: String,
    pub miner_hotkey: String,
    pub name: Option<String>,
    pub consensus_score: f64,
    pub evaluation_count: u32,
}

pub async fn get_leaderboard(
    State(state): State<Arc<ChallengeServerState>>,
    Query(query): Query<LeaderboardQuery>,
) -> Result<Json<LeaderboardResponse>, (StatusCode, String)> {
    let limit = query.limit.unwrap_or(100);

    // Use PostgreSQL if in server mode, otherwise fetch from platform-server API
    let entries: Vec<LeaderboardEntryResponse> = if let Some(pg) = &state.pg_storage {
        // Server mode: read from local PostgreSQL
        let lb = pg
            .get_leaderboard(limit as i64)
            .await
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;

        lb.iter()
            .map(|e| LeaderboardEntryResponse {
                rank: e.rank.unwrap_or(0) as u32,
                agent_hash: e.agent_hash.clone(),
                miner_hotkey: e.miner_hotkey.clone(),
                name: e.name.clone(),
                consensus_score: e.best_score,
                evaluation_count: e.evaluation_count as u32,
            })
            .collect()
    } else {
        // Validator mode: fetch from platform-server via snapshot
        let snapshot = state
            .platform_client
            .get_snapshot(None)
            .await
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;

        snapshot
            .leaderboard
            .iter()
            .take(limit)
            .enumerate()
            .map(|(i, e)| LeaderboardEntryResponse {
                rank: (i + 1) as u32,
                agent_hash: e.agent_hash.clone(),
                miner_hotkey: e.miner_hotkey.clone(),
                name: e.name.clone(),
                consensus_score: e.consensus_score,
                evaluation_count: e.evaluation_count,
            })
            .collect()
    };

    let total_count = entries.len();

    Ok(Json(LeaderboardResponse {
        challenge_id: state.challenge_id.clone(),
        entries,
        total_count,
    }))
}

// ============================================================================
// /health ENDPOINT
// ============================================================================

pub async fn health_check() -> &'static str {
    "OK"
}

// ============================================================================
// SERVER STARTUP
// ============================================================================

pub async fn run_server(
    config: ChallengeConfig,
    platform_url: &str,
    challenge_id: &str,
    host: &str,
    port: u16,
) -> anyhow::Result<()> {
    run_server_with_mode(config, platform_url, challenge_id, host, port, false).await
}

pub async fn run_server_with_mode(
    config: ChallengeConfig,
    platform_url: &str,
    challenge_id: &str,
    host: &str,
    port: u16,
    test_mode: bool,
) -> anyhow::Result<()> {
    // Initialize PostgreSQL if DATABASE_URL is set (server mode)
    let pg_storage = if let Ok(database_url) = std::env::var("DATABASE_URL") {
        info!("DATABASE_URL found, initializing PostgreSQL storage (server mode)");
        match PgStorage::new(&database_url).await {
            Ok(pg) => {
                info!("PostgreSQL storage initialized successfully");
                Some(pg)
            }
            Err(e) => {
                error!("Failed to initialize PostgreSQL: {}", e);
                warn!("Continuing in validator mode (no persistent storage)");
                None
            }
        }
    } else {
        debug!("No DATABASE_URL, running in validator mode");
        None
    };

    let state = Arc::new(ChallengeServerState::with_options(
        config,
        platform_url,
        challenge_id,
        test_mode,
        pg_storage,
    ));

    // Pre-download tasks at startup
    info!(
        "Pre-downloading tasks for dataset: {}",
        state.dataset_name()
    );
    match state.ensure_tasks_cached().await {
        Ok(tasks) => info!("Cached {} tasks", tasks.len()),
        Err(e) => warn!(
            "Failed to pre-download tasks: {} (will retry on first evaluation)",
            e
        ),
    }

    let cors = CorsLayer::new()
        .allow_origin(Any)
        .allow_methods(Any)
        .allow_headers(Any);

    let app = Router::new()
        .route("/health", get(health_check))
        .route("/get_weights", get(get_weights))
        .route("/evaluate", post(evaluate_agent))
        .route("/validate", post(validate_source))
        .route("/config", get(get_config))
        .route("/leaderboard", get(get_leaderboard))
        .layer(cors)
        .layer(TraceLayer::new_for_http())
        .with_state(state.clone());

    let addr = format!("{}:{}", host, port);
    let listener = tokio::net::TcpListener::bind(&addr).await?;

    info!("╔══════════════════════════════════════════════════════════════╗");
    info!("║     Terminal Benchmark Challenge - Production Server        ║");
    info!("╠══════════════════════════════════════════════════════════════╣");
    info!("║  Challenge ID: {:<45} ║", challenge_id);
    info!("║  Platform URL: {:<45} ║", platform_url);
    info!("║  Listening on: {:<45} ║", addr);
    info!(
        "║  Dataset: {:<50} ║",
        format!(
            "{}@{}",
            if test_mode {
                TEST_DATASET
            } else {
                DEFAULT_DATASET
            },
            if test_mode {
                TEST_DATASET_VERSION
            } else {
                DEFAULT_DATASET_VERSION
            }
        )
    );
    info!(
        "║  Dataset Mode: {:<45} ║",
        if test_mode { "TEST" } else { "PRODUCTION" }
    );
    info!(
        "║  Storage Mode: {:<45} ║",
        if state.is_server_mode() {
            "SERVER (PostgreSQL)"
        } else {
            "VALIDATOR (API only)"
        }
    );
    info!("╠══════════════════════════════════════════════════════════════╣");
    info!("║  Endpoints:                                                  ║");
    info!("║    GET  /health      - Health check                          ║");
    info!("║    GET  /get_weights - Deterministic weights (epoch)         ║");
    info!("║    POST /evaluate    - Run agent on real tasks               ║");
    info!("║    POST /validate    - Whitelist validation                  ║");
    info!("║    GET  /config      - Challenge configuration               ║");
    info!("║    GET  /leaderboard - Challenge leaderboard                 ║");
    info!("╚══════════════════════════════════════════════════════════════╝");

    axum::serve(listener, app).await?;
    Ok(())
}
