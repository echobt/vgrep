//! Term - Terminal Benchmark Challenge CLI
//!
//! A command-line interface for the Terminal Benchmark Challenge.
//! Test, submit, and monitor AI agents competing on terminal tasks.

mod commands;
mod style;
mod tui;
mod tui_runner;
mod wizard;

use clap::{Parser, Subcommand};
use style::*;

const BANNER: &str = r#"
  ████████╗███████╗██████╗ ███╗   ███╗
  ╚══██╔══╝██╔════╝██╔══██╗████╗ ████║
     ██║   █████╗  ██████╔╝██╔████╔██║
     ██║   ██╔══╝  ██╔══██╗██║╚██╔╝██║
     ██║   ███████╗██║  ██║██║ ╚═╝ ██║
     ╚═╝   ╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝
"#;

const VERSION: &str = env!("CARGO_PKG_VERSION");

#[derive(Parser)]
#[command(name = "term")]
#[command(author = "Platform Network")]
#[command(version)]
#[command(about = "Terminal Benchmark Challenge - Test and submit AI agents", long_about = None)]
#[command(propagate_version = true)]
struct Cli {
    /// Validator RPC endpoint
    #[arg(
        short,
        long,
        env = "VALIDATOR_RPC",
        default_value = "https://chain.platform.network",
        global = true
    )]
    rpc: String,

    /// Enable verbose output
    #[arg(short, long, global = true)]
    verbose: bool,

    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Interactive submission wizard (recommended for first-time users)
    #[command(visible_alias = "w")]
    Wizard,

    /// Submit an agent to the network
    #[command(visible_alias = "s")]
    Submit {
        /// Path to the agent Python file
        #[arg(short, long)]
        agent: std::path::PathBuf,

        /// Your miner secret key (hex or mnemonic)
        #[arg(short, long, env = "MINER_SECRET_KEY")]
        key: String,

        /// Agent name (optional)
        #[arg(long)]
        name: Option<String>,

        /// LLM API key to encrypt for validators (OpenAI, Anthropic, etc.)
        #[arg(long, env = "LLM_API_KEY")]
        api_key: Option<String>,

        /// Use per-validator API keys (more secure, requires --api-keys-file)
        #[arg(long)]
        per_validator: bool,

        /// JSON file with per-validator API keys: {"validator_hotkey": "api_key", ...}
        #[arg(long)]
        api_keys_file: Option<std::path::PathBuf>,
    },

    /// Check agent status and results
    #[command(visible_alias = "st")]
    Status {
        /// Agent hash
        #[arg(short = 'H', long)]
        hash: String,

        /// Watch for updates (refresh every 5s)
        #[arg(short, long)]
        watch: bool,
    },

    /// View the leaderboard
    #[command(visible_alias = "lb")]
    Leaderboard {
        /// Number of entries to show
        #[arg(short, long, default_value = "20")]
        limit: usize,
    },

    /// Validate an agent locally (syntax & security checks)
    #[command(visible_alias = "v")]
    Validate {
        /// Path to the agent Python file
        #[arg(short, long)]
        agent: std::path::PathBuf,
    },

    /// LLM review - validate agent against blockchain rules using LLM
    #[command(visible_alias = "r")]
    Review {
        /// Path to the agent Python file
        #[arg(short, long)]
        agent: std::path::PathBuf,

        /// Challenge RPC endpoint (for fetching rules)
        #[arg(short, long, env = "TERM_CHALLENGE_URL")]
        endpoint: Option<String>,

        /// LLM API key (OpenRouter or Chutes)
        #[arg(long, env = "LLM_API_KEY")]
        api_key: Option<String>,

        /// LLM provider: openrouter, chutes
        #[arg(short, long)]
        provider: Option<String>,

        /// LLM model name
        #[arg(short, long)]
        model: Option<String>,
    },

    /// Show challenge configuration
    Config,

    /// Show allowed Python modules
    Modules,

    /// Show LLM models and pricing
    Models,

    /// Show network status and quick commands
    #[command(visible_alias = "ui")]
    Dashboard {
        /// Your miner secret key (hex or mnemonic)
        #[arg(short, long, env = "MINER_SECRET_KEY")]
        key: Option<String>,
    },

    /// Show network statistics
    Stats,

    /// Terminal-Bench benchmark commands
    #[command(visible_alias = "b")]
    Bench {
        #[command(subcommand)]
        command: BenchCommands,
    },

    /// Subnet owner control commands (owner only)
    #[command(visible_alias = "sn")]
    Subnet(commands::subnet::SubnetArgs),
}

#[derive(Subcommand)]
enum BenchCommands {
    /// List available datasets
    #[command(visible_alias = "ls")]
    List,

    /// Download a dataset
    #[command(visible_alias = "dl")]
    Download {
        /// Dataset specifier (e.g., terminal-bench@2.0)
        dataset: String,

        /// Force re-download even if cached
        #[arg(short, long)]
        force: bool,
    },

    /// Show cache info
    Cache,

    /// Clear downloaded datasets cache
    ClearCache,

    /// Run a single task with LLM agent
    #[command(visible_alias = "r")]
    Run {
        /// Path to task directory
        #[arg(short, long)]
        task: std::path::PathBuf,

        /// LLM provider: openrouter, chutes
        #[arg(short, long, default_value = "openrouter")]
        provider: String,

        /// Model name (provider-specific)
        #[arg(short, long)]
        model: Option<String>,

        /// API key (or set OPENROUTER_API_KEY / CHUTES_API_KEY)
        #[arg(long, env = "LLM_API_KEY")]
        api_key: Option<String>,

        /// Maximum cost budget in USD
        #[arg(long, default_value = "10.0")]
        budget: f64,

        /// Output directory for results
        #[arg(short, long)]
        output: Option<std::path::PathBuf>,

        /// Timeout multiplier (default: 1.0)
        #[arg(long, default_value = "1.0")]
        timeout_mult: f64,

        /// Maximum agent steps
        #[arg(long, default_value = "500")]
        max_steps: u32,
    },

    /// Run agent on task(s) - single task or full dataset benchmark
    #[command(visible_alias = "a")]
    Agent {
        /// Path to agent script (*.py, *.js, *.ts, *.rs)
        #[arg(short, long)]
        agent: std::path::PathBuf,

        /// Single task directory (mutually exclusive with --dataset)
        #[arg(short, long, conflicts_with = "dataset")]
        task: Option<std::path::PathBuf>,

        /// Dataset specifier for benchmark (e.g., terminal-bench@2.0)
        #[arg(short, long, conflicts_with = "task")]
        dataset: Option<String>,

        /// API key for LLM provider (REQUIRED)
        #[arg(long, env = "LLM_API_KEY")]
        api_key: String,

        /// LLM provider: openrouter, chutes (passed as env var to agent)
        #[arg(short, long, default_value = "openrouter")]
        provider: String,

        /// Model name (passed as env var to agent)
        #[arg(short, long)]
        model: Option<String>,

        /// Output directory for results
        #[arg(short, long)]
        output: Option<std::path::PathBuf>,

        /// Maximum number of tasks (only for dataset benchmark)
        #[arg(short = 'n', long)]
        max_tasks: Option<usize>,

        /// Number of concurrent tasks (only for dataset benchmark)
        #[arg(short, long, default_value = "1")]
        concurrent: usize,

        /// Timeout multiplier (default: 1.0)
        #[arg(long, default_value = "1.0")]
        timeout_mult: f64,

        /// Maximum agent steps per task
        #[arg(long, default_value = "500")]
        max_steps: u32,
    },
}

#[tokio::main]
async fn main() {
    let cli = Cli::parse();

    if cli.verbose {
        tracing_subscriber::fmt().with_env_filter("info").init();
    }

    let result = match cli.command {
        Commands::Wizard => wizard::run_submit_wizard(&cli.rpc).await,
        Commands::Submit {
            agent,
            key,
            name,
            api_key,
            per_validator,
            api_keys_file,
        } => {
            commands::submit::run(
                &cli.rpc,
                agent,
                key,
                name,
                api_key,
                per_validator,
                api_keys_file,
            )
            .await
        }
        Commands::Status { hash, watch } => commands::status::run(&cli.rpc, hash, watch).await,
        Commands::Leaderboard { limit } => commands::leaderboard::run(&cli.rpc, limit).await,
        Commands::Validate { agent } => commands::validate::run(agent).await,
        Commands::Review {
            agent,
            endpoint,
            api_key,
            provider,
            model,
        } => commands::review::run(agent, endpoint, api_key, provider, model).await,
        Commands::Config => commands::config::run(&cli.rpc).await,
        Commands::Modules => commands::modules::run().await,
        Commands::Models => commands::models::run().await,
        Commands::Dashboard { key } => tui::run(&cli.rpc, key).await,
        Commands::Stats => commands::stats::run(&cli.rpc).await,
        Commands::Bench { command } => match command {
            BenchCommands::List => commands::bench::list_datasets().await,
            BenchCommands::Download { dataset, force } => {
                commands::bench::download_dataset(&dataset, force).await
            }
            BenchCommands::Cache => commands::bench::show_cache(),
            BenchCommands::ClearCache => commands::bench::clear_cache(),
            BenchCommands::Run {
                task,
                provider,
                model,
                api_key,
                budget,
                output,
                timeout_mult,
                max_steps,
            } => {
                commands::bench::run_task(
                    task,
                    &provider,
                    model.as_deref(),
                    api_key.as_deref(),
                    budget,
                    output,
                    timeout_mult,
                    max_steps,
                )
                .await
            }
            BenchCommands::Agent {
                agent,
                task,
                dataset,
                api_key,
                provider,
                model,
                output,
                max_tasks,
                concurrent,
                timeout_mult,
                max_steps,
            } => match (task, dataset) {
                (Some(task_path), None) => {
                    commands::bench::run_external_agent(
                        agent,
                        task_path,
                        Some(&provider),
                        model.as_deref(),
                        Some(&api_key),
                        output,
                        timeout_mult,
                        max_steps,
                    )
                    .await
                }
                (None, Some(dataset_spec)) => {
                    commands::bench::run_benchmark(
                        &dataset_spec,
                        agent,
                        Some(&provider),
                        model.as_deref(),
                        Some(&api_key),
                        output,
                        max_tasks,
                        timeout_mult,
                        concurrent,
                        max_steps,
                    )
                    .await
                }
                (None, None) => Err(anyhow::anyhow!("Either --task or --dataset is required")),
                (Some(_), Some(_)) => {
                    Err(anyhow::anyhow!("Cannot specify both --task and --dataset"))
                }
            },
        },
        Commands::Subnet(args) => commands::subnet::run(args).await,
    };

    if let Err(e) = result {
        print_error(&format!("{}", e));
        std::process::exit(1);
    }
}

/// Print the welcome banner
pub fn print_banner() {
    println!("{}", style_cyan(BANNER));
    println!(
        "  {} {}",
        style_dim("Terminal Benchmark Challenge"),
        style_dim(&format!("v{}", VERSION))
    );
    println!();
}
