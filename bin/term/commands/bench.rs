//! Terminal-Bench benchmark commands

use anyhow::Result;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{Mutex, Semaphore};
use term_challenge::bench::{
    create_agent,
    llm::Provider,
    registry::{cache_dir, RegistryClient},
    results::{print_results, BenchmarkResults, ResultExporter, TaskResult},
    runner::{Agent, TrialConfig, TrialRunner},
    task::Task,
};
use tracing::{error, info, warn};
use uuid::Uuid;

/// Global container tracker for cleanup on Ctrl+C
static ACTIVE_CONTAINERS: std::sync::OnceLock<Arc<Mutex<Vec<String>>>> = std::sync::OnceLock::new();

fn get_container_tracker() -> Arc<Mutex<Vec<String>>> {
    ACTIVE_CONTAINERS
        .get_or_init(|| Arc::new(Mutex::new(Vec::new())))
        .clone()
}

/// Cleanup all active containers
async fn cleanup_containers() {
    let tracker = get_container_tracker();
    let containers = tracker.lock().await;
    
    if containers.is_empty() {
        return;
    }
    
    eprintln!("\n\n  üßπ Cleaning up {} container(s)...", containers.len());
    
    let docker = match bollard::Docker::connect_with_local_defaults() {
        Ok(d) => d,
        Err(e) => {
            eprintln!("  ‚ö†Ô∏è  Failed to connect to Docker: {}", e);
            return;
        }
    };
    
    for container_id in containers.iter() {
        // Stop with 5 second timeout
        let options = bollard::container::StopContainerOptions { t: 5 };
        if let Err(e) = docker.stop_container(container_id, Some(options)).await {
            warn!("Failed to stop container {}: {}", container_id, e);
        }
        
        // Remove container
        let rm_options = bollard::container::RemoveContainerOptions {
            force: true,
            ..Default::default()
        };
        if let Err(e) = docker.remove_container(container_id, Some(rm_options)).await {
            warn!("Failed to remove container {}: {}", container_id, e);
        } else {
            eprintln!("  ‚úì Removed: {}", &container_id[..12]);
        }
    }
}

/// List available datasets
pub async fn list_datasets() -> Result<()> {
    println!("\n  üì¶ Available Datasets\n");

    let mut client = RegistryClient::new();
    let datasets = client.list_datasets().await?;

    if datasets.is_empty() {
        println!("  No datasets found in registry.\n");
        return Ok(());
    }

    println!("  {:<30} {:<10} {}", "Name", "Version", "Description");
    println!("  {}", "-".repeat(70));

    for (name, version, desc) in datasets {
        let desc_short = if desc.len() > 30 {
            format!("{}...", &desc[..27])
        } else {
            desc
        };
        println!("  {:<30} {:<10} {}", name, version, desc_short);
    }

    println!("\n  Use: term bench download <name>@<version>\n");
    Ok(())
}

/// Download a dataset
pub async fn download_dataset(spec: &str, force: bool) -> Result<()> {
    let (name, version) = RegistryClient::parse_dataset_spec(spec);

    println!("\n  üì• Downloading dataset: {}@{}\n", name, version);

    let mut client = RegistryClient::new();
    let paths = client.download_dataset(&name, &version, force).await?;

    println!("  Downloaded {} tasks to:", paths.len());
    println!("  {}\n", cache_dir().display());

    for path in &paths {
        if let Some(name) = path.file_name() {
            println!("    ‚úì {}", name.to_string_lossy());
        }
    }

    println!();
    Ok(())
}

/// Show cache info
pub fn show_cache() -> Result<()> {
    let cache = cache_dir();

    println!("\n  üìÅ Cache Directory\n");
    println!("  Path: {}\n", cache.display());

    if !cache.exists() {
        println!("  Cache is empty.\n");
        return Ok(());
    }

    let mut count = 0;
    let mut total_size = 0u64;

    for entry in std::fs::read_dir(&cache)? {
        let entry = entry?;
        let meta = entry.metadata()?;

        if meta.is_dir() {
            count += 1;
            // Calculate directory size
            for file in walkdir(&entry.path()) {
                if let Ok(m) = file.metadata() {
                    total_size += m.len();
                }
            }
            println!("    {}", entry.file_name().to_string_lossy());
        }
    }

    println!(
        "\n  {} tasks cached ({:.1} MB)\n",
        count,
        total_size as f64 / 1_000_000.0
    );
    Ok(())
}

/// Clear cache
pub fn clear_cache() -> Result<()> {
    let cache = cache_dir();

    if cache.exists() {
        std::fs::remove_dir_all(&cache)?;
        println!("\n  ‚úì Cache cleared\n");
    } else {
        println!("\n  Cache is already empty\n");
    }

    Ok(())
}

/// Run a single task with LLM agent
pub async fn run_task(
    task_path: PathBuf,
    provider_str: &str,
    model: Option<&str>,
    api_key: Option<&str>,
    budget: f64,
    output_dir: Option<PathBuf>,
    timeout_multiplier: f64,
    max_steps: u32,
) -> Result<()> {
    let task = Task::from_path(&task_path)?;
    let provider = Provider::parse(provider_str)?;

    println!("\n  üöÄ Running task: {}\n", task.name);
    println!("  Provider: {}", provider);
    println!("  Model:    {}", model.unwrap_or(provider.default_model()));
    println!("  Budget:   ${:.2}", budget);
    println!("\n  Instruction:");
    println!(
        "  {}\n",
        task.instruction()?
            .lines()
            .take(5)
            .collect::<Vec<_>>()
            .join("\n  ")
    );

    // Create LLM agent
    let agent = create_agent(provider, model, api_key, budget)?;
    info!("Created agent: {}", agent.name());

    let output = output_dir.unwrap_or_else(|| PathBuf::from("./benchmark_results"));
    let trial_name = format!("trial-{}", Uuid::new_v4().as_simple());

    let config = TrialConfig {
        trial_name: trial_name.clone(),
        output_dir: output.clone(),
        max_steps,
        timeout_multiplier,
        force_build: false,
        delete_container: true,
        agent_provider: Some(provider.to_string()),
        model_name: model.map(String::from),
    };

    let runner = TrialRunner::new(config);
    let result = runner.run(&task, &agent).await.map_err(|e| {
        error!("Trial failed: {:?}", e);
        e
    })?;

    // Print cost info
    let cost = agent.cost_tracker();

    println!("\n  üìä Results\n");
    println!("  Task:     {}", result.task_name);
    println!("  Success:  {}", if result.success() { "‚úì" } else { "‚úó" });
    println!("  Reward:   {:.4}", result.reward());
    println!("  Steps:    {}", result.steps);
    println!("  Duration: {:.1}s", result.duration_sec);
    println!("\n  üí∞ Cost");
    println!(
        "  Tokens:   {} prompt + {} completion",
        cost.total_prompt_tokens, cost.total_completion_tokens
    );
    println!("  Total:    ${:.4}", cost.total_cost_usd);

    if let Some(err) = &result.error {
        println!("\n  ‚ö†Ô∏è  Error: {}", err);
    }

    println!("\n  üìÅ Logs: {}\n", result.logs_path.display());

    Ok(())
}

/// Run benchmark on a dataset with LLM agent
pub async fn run_benchmark(
    dataset_spec: &str,
    provider_str: &str,
    model: Option<&str>,
    api_key: Option<&str>,
    budget: f64,
    output_dir: Option<PathBuf>,
    max_tasks: Option<usize>,
    timeout_multiplier: f64,
    concurrent: usize,
    max_steps: u32,
) -> Result<()> {
    let (name, version) = RegistryClient::parse_dataset_spec(dataset_spec);
    let provider = Provider::parse(provider_str)?;

    println!("\n  üèÅ Starting benchmark: {}@{}\n", name, version);
    println!("  Provider:   {}", provider);
    println!(
        "  Model:      {}",
        model.unwrap_or(provider.default_model())
    );
    println!("  Budget:     ${:.2} per task", budget);

    // Download dataset if needed
    let mut client = RegistryClient::new();
    let task_paths = client.get_task_paths(&name, &version).await?;

    let task_paths: Vec<_> = if let Some(max) = max_tasks {
        task_paths.into_iter().take(max).collect()
    } else {
        task_paths
    };

    let total_tasks = task_paths.len();
    println!("  Tasks:      {}", total_tasks);
    println!("  Concurrent: {}", concurrent);
    println!("  Max steps:  {}", max_steps);
    println!("  Timeout:    {}x\n", timeout_multiplier);

    let output = output_dir.unwrap_or_else(|| PathBuf::from("./benchmark_results"));
    let model_short = model
        .unwrap_or(provider.default_model())
        .split('/')
        .last()
        .unwrap_or("unknown");
    let bench_name = format!(
        "bench-{}-{}@{}-{}",
        model_short,
        name,
        version,
        &Uuid::new_v4().as_simple().to_string()[..8]
    );

    let bench_dir = output.join(&bench_name);
    std::fs::create_dir_all(&bench_dir)?;

    let model_name = model.unwrap_or(provider.default_model());
    
    // Setup Ctrl+C handler
    let cleanup_flag = Arc::new(std::sync::atomic::AtomicBool::new(false));
    let cleanup_flag_clone = cleanup_flag.clone();
    
    tokio::spawn(async move {
        if tokio::signal::ctrl_c().await.is_ok() {
            cleanup_flag_clone.store(true, std::sync::atomic::Ordering::SeqCst);
            cleanup_containers().await;
            std::process::exit(130);
        }
    });

    // Shared state for concurrent execution
    let results = Arc::new(Mutex::new(BenchmarkResults::new(
        &bench_name,
        &format!("{}@{}", name, version),
        &format!("{}/{}", provider, model_name),
        Some(model_name),
    )));
    let total_cost = Arc::new(Mutex::new(0.0f64));
    let completed = Arc::new(std::sync::atomic::AtomicUsize::new(0));
    let semaphore = Arc::new(Semaphore::new(concurrent));
    let container_tracker = get_container_tracker();

    // Spawn concurrent tasks
    let mut handles = Vec::new();
    
    for (i, task_path) in task_paths.into_iter().enumerate() {
        let semaphore = semaphore.clone();
        let results = results.clone();
        let total_cost = total_cost.clone();
        let completed = completed.clone();
        let bench_name = bench_name.clone();
        let bench_dir = bench_dir.clone();
        let api_key = api_key.map(String::from);
        let model = model.map(String::from);
        let container_tracker = container_tracker.clone();
        let cleanup_flag = cleanup_flag.clone();
        
        let handle = tokio::spawn(async move {
            // Acquire semaphore permit
            let _permit = semaphore.acquire().await.unwrap();
            
            // Check if cleanup was requested
            if cleanup_flag.load(std::sync::atomic::Ordering::SeqCst) {
                return;
            }
            
            let task = match Task::from_path(&task_path) {
                Ok(t) => t,
                Err(e) => {
                    error!("Failed to load task {:?}: {}", task_path, e);
                    return;
                }
            };

            if !task.is_valid() {
                error!("Task {} is missing required files", task.name);
                return;
            }

            let task_num = completed.fetch_add(1, std::sync::atomic::Ordering::SeqCst) + 1;
            println!("  [{}/{}] Running: {}", task_num, total_tasks, task.name);

            // Create fresh agent for each task
            let agent = match create_agent(provider, model.as_deref(), api_key.as_deref(), budget) {
                Ok(a) => a,
                Err(e) => {
                    error!("Failed to create agent: {}", e);
                    let mut results = results.lock().await;
                    results.add_result(TaskResult {
                        task_name: task.name.clone(),
                        success: false,
                        reward: 0.0,
                        duration_sec: 0.0,
                        steps: 0,
                        error: Some(format!("Agent creation failed: {}", e)),
                        trial_name: bench_name.clone(),
                    });
                    return;
                }
            };

            let trial_name = format!("{}-{}", bench_name, task.name);
            let config = TrialConfig {
                trial_name: trial_name.clone(),
                output_dir: bench_dir.clone(),
                max_steps,
                timeout_multiplier,
                force_build: false,
                delete_container: true,
                agent_provider: Some(provider.to_string()),
                model_name: model.clone(),
            };

            let runner = TrialRunner::new(config);
            
            // Track container for cleanup (runner will set this)
            // Note: The runner's environment will be tracked internally
            
            match runner.run(&task, &agent).await {
                Ok(trial_result) => {
                    let status = if trial_result.success() { "‚úì" } else { "‚úó" };
                    let cost = agent.cost_tracker();
                    
                    {
                        let mut tc = total_cost.lock().await;
                        *tc += cost.total_cost_usd;
                    }

                    println!(
                        "  [{}/{}] {} {} reward={:.4} steps={} time={:.1}s cost=${:.4}",
                        task_num, total_tasks,
                        status,
                        task.name,
                        trial_result.reward(),
                        trial_result.steps,
                        trial_result.duration_sec,
                        cost.total_cost_usd
                    );
                    
                    let mut results = results.lock().await;
                    results.add_result(TaskResult::from(trial_result));
                }
                Err(e) => {
                    println!("  [{}/{}] ‚úó {} error: {}", task_num, total_tasks, task.name, e);
                    let mut results = results.lock().await;
                    results.add_result(TaskResult {
                        task_name: task.name.clone(),
                        success: false,
                        reward: 0.0,
                        duration_sec: 0.0,
                        steps: 0,
                        error: Some(e.to_string()),
                        trial_name: trial_name.clone(),
                    });
                }
            }
        });
        
        handles.push(handle);
    }

    // Wait for all tasks to complete
    for handle in handles {
        let _ = handle.await;
    }

    // Finalize results
    {
        let mut results_guard = results.lock().await;
        results_guard.complete();

        // Export results
        let exporter = ResultExporter::new(&bench_dir);
        exporter.export_all(&results_guard)?;

        // Print summary
        print_results(&results_guard);
    }

    let final_cost = *total_cost.lock().await;
    println!("\n  üí∞ Total Cost: ${:.4}", final_cost);
    println!("  üìÅ Results saved to: {}\n", bench_dir.display());

    Ok(())
}

/// Run external agent (Python/JavaScript/Rust) on a task
pub async fn run_external_agent(
    agent_path: PathBuf,
    task_path: PathBuf,
    provider: Option<&str>,
    model: Option<&str>,
    api_key: Option<&str>,
    output_dir: Option<PathBuf>,
    timeout_multiplier: f64,
    max_steps: u32,
) -> Result<()> {
    use crate::tui_runner::Spinner;
    use term_challenge::bench::create_external_agent;

    let task = Task::from_path(&task_path)?;

    // Detect language from extension
    let lang = agent_path
        .extension()
        .and_then(|e| e.to_str())
        .map(|e| match e {
            "py" => "Python",
            "js" | "mjs" | "ts" => "JavaScript",
            "rs" => "Rust",
            _ => "Binary",
        })
        .unwrap_or("Binary");

    // Print header
    println!();
    println!("  \x1b[1m\x1b[36mTerm Challenge\x1b[0m");
    println!("  \x1b[90m{}\x1b[0m", "‚îÄ".repeat(50));
    println!();
    println!(
        "  \x1b[90mAgent:\x1b[0m    {} \x1b[90m({})\x1b[0m",
        agent_path.display(),
        lang
    );
    println!("  \x1b[90mTask:\x1b[0m     \x1b[1m{}\x1b[0m", task.name);
    if let Some(p) = provider {
        println!("  \x1b[90mProvider:\x1b[0m {}", p);
    }
    if let Some(m) = model {
        println!("  \x1b[90mModel:\x1b[0m    {}", m);
    }
    println!();
    println!("  \x1b[90mInstruction:\x1b[0m");
    for line in task.instruction()?.lines().take(3) {
        println!("  \x1b[37m{}\x1b[0m", line);
    }
    println!();
    println!("  \x1b[90m{}\x1b[0m", "‚îÄ".repeat(50));

    // Create agent with spinner
    let mut spinner = Spinner::new("Initializing agent...");
    spinner.start();

    let agent = match create_external_agent(&agent_path, provider, api_key, model) {
        Ok(a) => {
            spinner.stop(true, Some("Agent initialized"));
            a
        }
        Err(e) => {
            spinner.stop(false, Some(&format!("Failed: {}", e)));
            return Err(e);
        }
    };

    let output = output_dir.unwrap_or_else(|| PathBuf::from("./benchmark_results"));
    let short_id = &Uuid::new_v4().as_simple().to_string()[..12];
    let trial_name = format!("ext-{}", short_id);

    let config = TrialConfig {
        trial_name: trial_name.clone(),
        output_dir: output.clone(),
        max_steps,
        timeout_multiplier,
        force_build: false,
        delete_container: true,
        agent_provider: provider.map(String::from),
        model_name: model.map(String::from),
    };

    let runner = TrialRunner::new(config);
    info!("Created external agent: {}", agent.name());

    // Run with spinner (evaluation happens inside Docker)
    let mut spinner = Spinner::new(&format!("Running {}...", task.name));
    spinner.start();

    let start = std::time::Instant::now();
    let result = runner.run(&task, &agent).await;
    let elapsed = start.elapsed().as_secs_f64();

    match &result {
        Ok(r) => {
            let status = if r.success() {
                format!("{} completed", task.name)
            } else {
                format!("{} failed", task.name)
            };
            spinner.stop(r.success(), Some(&status));

            // Print results
            println!();
            let icon = if r.success() {
                "\x1b[32m‚úì\x1b[0m"
            } else {
                "\x1b[31m‚úó\x1b[0m"
            };
            println!("  {} \x1b[1m{}\x1b[0m", icon, r.task_name);
            println!(
                "    Reward: \x1b[{}m{:.4}\x1b[0m  Steps: {}  Time: {:.1}s",
                if r.reward() > 0.0 { "32" } else { "90" },
                r.reward(),
                r.steps,
                elapsed
            );

            if let Some(ref err) = r.error {
                println!();
                println!("    \x1b[33m‚ö† Error:\x1b[0m");
                for line in err.lines().take(15) {
                    println!("      \x1b[90m{}\x1b[0m", line);
                }
            }

            println!();
            println!("  \x1b[90müìÅ Logs:\x1b[0m {}", r.logs_path.display());
        }
        Err(e) => {
            spinner.stop(false, Some(&format!("Failed: {}", e)));
            error!("Trial failed: {:?}", e);
        }
    }

    println!();

    Ok(())
}

/// Simple directory walker
fn walkdir(path: &std::path::Path) -> Vec<std::fs::DirEntry> {
    let mut files = vec![];
    if let Ok(entries) = std::fs::read_dir(path) {
        for entry in entries.flatten() {
            if entry.path().is_dir() {
                files.extend(walkdir(&entry.path()));
            } else {
                files.push(entry);
            }
        }
    }
    files
}
