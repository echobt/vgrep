//! Terminal-Bench benchmark commands

use anyhow::{Context, Result};
use std::path::PathBuf;
use std::sync::Arc;
use term_challenge::bench::{
    create_agent,
    llm::Provider,
    registry::{cache_dir, RegistryClient},
    results::{print_results, BenchmarkResults, ResultExporter, TaskResult},
    runner::{Agent, TrialConfig, TrialRunner},
    task::Task,
};
use tokio::sync::{Mutex, Semaphore};
use tracing::{error, info};
use uuid::Uuid;

/// Cleanup all bench containers on Ctrl+C
async fn cleanup_containers() {
    use bollard::container::ListContainersOptions;
    use std::collections::HashMap;

    eprintln!("\n\n  üßπ Cleaning up bench containers...");

    let docker = match bollard::Docker::connect_with_local_defaults() {
        Ok(d) => d,
        Err(e) => {
            eprintln!("  ‚ö†Ô∏è  Failed to connect to Docker: {}", e);
            return;
        }
    };

    // List all containers with term-bench prefix
    let mut filters = HashMap::new();
    filters.insert("name", vec!["term-bench-"]);

    let options = ListContainersOptions {
        all: true,
        filters,
        ..Default::default()
    };

    let containers = match docker.list_containers(Some(options)).await {
        Ok(c) => c,
        Err(e) => {
            eprintln!("  ‚ö†Ô∏è  Failed to list containers: {}", e);
            return;
        }
    };

    if containers.is_empty() {
        eprintln!("  No bench containers to clean up.");
        return;
    }

    eprintln!("  Found {} container(s) to clean up", containers.len());

    for container in containers {
        if let Some(id) = container.id {
            let name = container
                .names
                .as_ref()
                .and_then(|n| n.first())
                .map(|s| s.trim_start_matches('/'))
                .unwrap_or(&id[..12]);

            // Stop with 5 second timeout
            let options = bollard::container::StopContainerOptions { t: 5 };
            let _ = docker.stop_container(&id, Some(options)).await;

            // Remove container
            let rm_options = bollard::container::RemoveContainerOptions {
                force: true,
                ..Default::default()
            };
            if docker.remove_container(&id, Some(rm_options)).await.is_ok() {
                eprintln!("  ‚úì Removed: {}", name);
            }
        }
    }
}

/// List available datasets
pub async fn list_datasets() -> Result<()> {
    println!("\n  üì¶ Available Datasets\n");

    let mut client = RegistryClient::new();
    let datasets = client.list_datasets().await?;

    if datasets.is_empty() {
        println!("  No datasets found in registry.\n");
        return Ok(());
    }

    println!("  {:<30} {:<10} Description", "Name", "Version");
    println!("  {}", "-".repeat(70));

    for (name, version, desc) in datasets {
        let desc_short = if desc.len() > 30 {
            format!("{}...", &desc[..27])
        } else {
            desc
        };
        println!("  {:<30} {:<10} {}", name, version, desc_short);
    }

    println!("\n  Use: term bench download <name>@<version>\n");
    Ok(())
}

/// Download a dataset
pub async fn download_dataset(spec: &str, force: bool) -> Result<()> {
    let (name, version) = RegistryClient::parse_dataset_spec(spec);

    println!("\n  üì• Downloading dataset: {}@{}\n", name, version);

    let mut client = RegistryClient::new();
    let paths = client.download_dataset(&name, &version, force).await?;

    println!("  Downloaded {} tasks to:", paths.len());
    println!("  {}\n", cache_dir().display());

    for path in &paths {
        if let Some(name) = path.file_name() {
            println!("    ‚úì {}", name.to_string_lossy());
        }
    }

    println!();
    Ok(())
}

/// Show cache info
pub fn show_cache() -> Result<()> {
    let cache = cache_dir();

    println!("\n  üìÅ Cache Directory\n");
    println!("  Path: {}\n", cache.display());

    if !cache.exists() {
        println!("  Cache is empty.\n");
        return Ok(());
    }

    let mut count = 0;
    let mut total_size = 0u64;

    for entry in std::fs::read_dir(&cache)? {
        let entry = entry?;
        let meta = entry.metadata()?;

        if meta.is_dir() {
            count += 1;
            // Calculate directory size
            for file in walkdir(&entry.path()) {
                if let Ok(m) = file.metadata() {
                    total_size += m.len();
                }
            }
            println!("    {}", entry.file_name().to_string_lossy());
        }
    }

    println!(
        "\n  {} tasks cached ({:.1} MB)\n",
        count,
        total_size as f64 / 1_000_000.0
    );
    Ok(())
}

/// Clear cache
pub fn clear_cache() -> Result<()> {
    let cache = cache_dir();

    if cache.exists() {
        std::fs::remove_dir_all(&cache)?;
        println!("\n  ‚úì Cache cleared\n");
    } else {
        println!("\n  Cache is already empty\n");
    }

    Ok(())
}

/// Run a single task with LLM agent
#[allow(clippy::too_many_arguments)]
pub async fn run_task(
    task_path: PathBuf,
    provider_str: &str,
    model: Option<&str>,
    api_key: Option<&str>,
    budget: f64,
    output_dir: Option<PathBuf>,
    timeout_multiplier: f64,
    max_steps: u32,
) -> Result<()> {
    let task = Task::from_path(&task_path)?;
    let provider = Provider::parse(provider_str)?;

    println!("\n  üöÄ Running task: {}\n", task.name);
    println!("  Provider: {}", provider);
    println!("  Model:    {}", model.unwrap_or(provider.default_model()));
    println!("  Budget:   ${:.2}", budget);
    println!("\n  Instruction:");
    println!(
        "  {}\n",
        task.instruction()?
            .lines()
            .take(5)
            .collect::<Vec<_>>()
            .join("\n  ")
    );

    // Create LLM agent
    let agent = create_agent(provider, model, api_key, budget)?;
    info!("Created agent: {}", agent.name());

    let output = output_dir.unwrap_or_else(|| PathBuf::from("./benchmark_results"));
    let trial_name = format!("trial-{}", Uuid::new_v4().as_simple());

    let config = TrialConfig {
        trial_name: trial_name.clone(),
        output_dir: output.clone(),
        max_steps,
        timeout_multiplier,
        force_build: false,
        delete_container: true,
        agent_provider: Some(provider.to_string()),
        model_name: model.map(String::from),
    };

    let runner = TrialRunner::new(config);
    let result = runner.run(&task, &agent).await.map_err(|e| {
        error!("Trial failed: {:?}", e);
        e
    })?;

    // Print cost info
    let cost = agent.cost_tracker();

    println!("\n  üìä Results\n");
    println!("  Task:     {}", result.task_name);
    println!("  Success:  {}", if result.success() { "‚úì" } else { "‚úó" });
    println!("  Reward:   {:.4}", result.reward());
    println!("  Steps:    {}", result.steps);
    println!("  Duration: {:.1}s", result.duration_sec);
    println!("\n  üí∞ Cost");
    println!(
        "  Tokens:   {} prompt + {} completion",
        cost.total_prompt_tokens, cost.total_completion_tokens
    );
    println!("  Total:    ${:.4}", cost.total_cost_usd);

    if let Some(err) = &result.error {
        println!("\n  ‚ö†Ô∏è  Error: {}", err);
    }

    println!("\n  üìÅ Logs: {}\n", result.logs_path.display());

    Ok(())
}

/// Run benchmark on a dataset with your external agent
#[allow(clippy::too_many_arguments)]
pub async fn run_benchmark(
    dataset_spec: &str,
    agent_path: PathBuf,
    provider: Option<&str>,
    model: Option<&str>,
    api_key: Option<&str>,
    output_dir: Option<PathBuf>,
    max_tasks: Option<usize>,
    timeout_multiplier: f64,
    concurrent: usize,
    max_steps: u32,
) -> Result<()> {
    use term_challenge::bench::create_external_agent;

    let (name, version) = RegistryClient::parse_dataset_spec(dataset_spec);

    // Detect language from extension
    let lang = agent_path
        .extension()
        .and_then(|e| e.to_str())
        .map(|e| match e {
            "py" => "Python",
            "js" | "mjs" | "ts" => "JavaScript",
            "rs" => "Rust",
            _ => "Binary",
        })
        .unwrap_or("Binary");

    println!("\n  üèÅ Starting benchmark: {}@{}\n", name, version);
    println!("  Agent:      {} ({})", agent_path.display(), lang);
    if let Some(p) = provider {
        println!("  Provider:   {}", p);
    }
    if let Some(m) = model {
        println!("  Model:      {}", m);
    }

    // Download dataset if needed
    let mut client = RegistryClient::new();
    let task_paths = client.get_task_paths(&name, &version).await?;

    let task_paths: Vec<_> = if let Some(max) = max_tasks {
        task_paths.into_iter().take(max).collect()
    } else {
        task_paths
    };

    let total_tasks = task_paths.len();
    println!("  Tasks:      {}", total_tasks);
    println!("  Concurrent: {}", concurrent);
    println!("  Max steps:  {}", max_steps);
    println!("  Timeout:    {}x\n", timeout_multiplier);

    let output = output_dir.unwrap_or_else(|| PathBuf::from("./benchmark_results"));
    let agent_name = agent_path
        .file_stem()
        .and_then(|s| s.to_str())
        .unwrap_or("agent");
    let bench_name = format!(
        "bench-{}-{}@{}-{}",
        agent_name,
        name,
        version,
        &Uuid::new_v4().as_simple().to_string()[..8]
    );

    let bench_dir = output.join(&bench_name);
    std::fs::create_dir_all(&bench_dir)?;

    let model_name = model.unwrap_or("external");

    // Setup Ctrl+C handler - force kill immediately
    tokio::spawn(async move {
        if tokio::signal::ctrl_c().await.is_ok() {
            eprintln!("\n\n  ‚ö†Ô∏è  Interrupted! Force killing...");
            // Spawn cleanup in background and exit immediately
            tokio::spawn(cleanup_containers());
            // Give a tiny moment for the message to print
            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
            std::process::exit(130);
        }
    });

    // Shared state for concurrent execution
    let results = Arc::new(Mutex::new(BenchmarkResults::new(
        &bench_name,
        &format!("{}@{}", name, version),
        agent_name,
        Some(model_name),
    )));
    let completed = Arc::new(std::sync::atomic::AtomicUsize::new(0));
    let semaphore = Arc::new(Semaphore::new(concurrent));

    // Spawn concurrent tasks
    let mut handles = Vec::new();

    for task_path in task_paths.into_iter() {
        let semaphore = semaphore.clone();
        let results = results.clone();
        let completed = completed.clone();
        let bench_name = bench_name.clone();
        let bench_dir = bench_dir.clone();
        let agent_path = agent_path.clone();
        let api_key = api_key.map(String::from);
        let model = model.map(String::from);
        let provider = provider.map(String::from);

        let handle = tokio::spawn(async move {
            // Acquire semaphore permit
            let _permit = semaphore.acquire().await.unwrap();

            let task = match Task::from_path(&task_path) {
                Ok(t) => t,
                Err(e) => {
                    error!("Failed to load task {:?}: {}", task_path, e);
                    return;
                }
            };

            if !task.is_valid() {
                error!("Task {} is missing required files", task.name);
                return;
            }

            let task_num = completed.fetch_add(1, std::sync::atomic::Ordering::SeqCst) + 1;
            println!("  [{}/{}] Running: {}", task_num, total_tasks, task.name);

            // Create fresh external agent for each task (runs in Docker)
            let agent = match create_external_agent(
                &agent_path,
                provider.as_deref(),
                api_key.as_deref(),
                model.as_deref(),
            )
            .await
            {
                Ok(a) => a,
                Err(e) => {
                    error!("Failed to create agent: {}", e);
                    let mut results = results.lock().await;
                    results.add_result(TaskResult {
                        task_name: task.name.clone(),
                        success: false,
                        reward: 0.0,
                        duration_sec: 0.0,
                        steps: 0,
                        error: Some(format!("Agent creation failed: {}", e)),
                        trial_name: bench_name.clone(),
                    });
                    return;
                }
            };

            let trial_name = format!("{}-{}", bench_name, task.name);
            let config = TrialConfig {
                trial_name: trial_name.clone(),
                output_dir: bench_dir.clone(),
                max_steps,
                timeout_multiplier,
                force_build: false,
                delete_container: true,
                agent_provider: provider.clone(),
                model_name: model.clone(),
            };

            let runner = TrialRunner::new(config);

            let run_result = runner.run(&task, &agent).await;

            // Cleanup agent container
            let _ = agent.stop().await;

            match run_result {
                Ok(trial_result) => {
                    let status = if trial_result.success() { "‚úì" } else { "‚úó" };

                    println!(
                        "  [{}/{}] {} {} reward={:.4} steps={} time={:.1}s",
                        task_num,
                        total_tasks,
                        status,
                        task.name,
                        trial_result.reward(),
                        trial_result.steps,
                        trial_result.duration_sec,
                    );

                    let mut results = results.lock().await;
                    results.add_result(TaskResult::from(trial_result));
                }
                Err(e) => {
                    println!(
                        "  [{}/{}] ‚úó {} error: {}",
                        task_num, total_tasks, task.name, e
                    );
                    let mut results = results.lock().await;
                    results.add_result(TaskResult {
                        task_name: task.name.clone(),
                        success: false,
                        reward: 0.0,
                        duration_sec: 0.0,
                        steps: 0,
                        error: Some(e.to_string()),
                        trial_name: trial_name.clone(),
                    });
                }
            }
        });

        handles.push(handle);
    }

    // Wait for all tasks to complete
    for handle in handles {
        let _ = handle.await;
    }

    // Finalize results
    {
        let mut results_guard = results.lock().await;
        results_guard.complete();

        // Export results
        let exporter = ResultExporter::new(&bench_dir);
        exporter.export_all(&results_guard)?;

        // Print summary
        print_results(&results_guard);
    }

    println!("\n  üìÅ Results saved to: {}\n", bench_dir.display());

    Ok(())
}

/// Run external agent (Python/JavaScript/Rust) on a task
///
/// This compiles the agent to a binary and runs it in the task container,
/// exactly like production validators do.
#[allow(clippy::too_many_arguments)]
pub async fn run_external_agent(
    agent_path: PathBuf,
    task_path: PathBuf,
    provider: Option<&str>,
    model: Option<&str>,
    api_key: Option<&str>,
    output_dir: Option<PathBuf>,
    timeout_multiplier: f64,
    _max_steps: u32,
) -> Result<()> {
    use term_challenge::bench::{run_binary_agent, BinaryAgentConfig};

    let task = Task::from_path(&task_path)?;

    // Only Python is supported for now (compiled to binary)
    let ext = agent_path
        .extension()
        .and_then(|e| e.to_str())
        .unwrap_or("");
    if ext != "py" {
        anyhow::bail!("Only Python agents (.py) are supported. Got: .{}", ext);
    }

    // Print header
    println!();
    println!("  \x1b[1m\x1b[36mTerm Challenge\x1b[0m");
    println!("  \x1b[90m{}\x1b[0m", "‚îÄ".repeat(50));
    println!();
    println!(
        "  \x1b[90mAgent:\x1b[0m    {} \x1b[90m(Python ‚Üí Binary)\x1b[0m",
        agent_path.display()
    );
    println!("  \x1b[90mTask:\x1b[0m     \x1b[1m{}\x1b[0m", task.name);
    if let Some(p) = provider {
        println!("  \x1b[90mProvider:\x1b[0m {}", p);
    }
    println!();
    println!("  \x1b[90mInstruction:\x1b[0m");
    for line in task.instruction()?.lines().take(3) {
        println!("  \x1b[37m{}\x1b[0m", line);
    }
    println!();
    println!("  \x1b[90m{}\x1b[0m", "‚îÄ".repeat(50));

    // Read source code
    let source_code = std::fs::read_to_string(&agent_path).context(format!(
        "Failed to read agent file: {}",
        agent_path.display()
    ))?;

    // Setup output directory
    let output = output_dir.unwrap_or_else(|| PathBuf::from("./benchmark_results"));
    let short_id = &Uuid::new_v4().as_simple().to_string()[..12];
    let trial_name = format!("bin-{}", short_id);
    let logs_dir = output.join(&trial_name).join(&task.name);
    std::fs::create_dir_all(&logs_dir)?;

    // Configure agent
    let config = BinaryAgentConfig {
        timeout_secs: (task.agent_timeout() * timeout_multiplier) as u64,
        api_key: api_key.map(String::from),
        api_provider: provider.map(String::from),
        api_model: model.map(String::from),
    };

    let start = std::time::Instant::now();
    let result = run_binary_agent(&source_code, &task, config, &logs_dir).await;
    let elapsed = start.elapsed().as_secs_f64();

    match result {
        Ok(r) => {
            // Print results
            println!();
            let (icon, pass_text) = if r.success {
                ("\x1b[32m‚úì\x1b[0m", "\x1b[1m\x1b[32mPASS\x1b[0m")
            } else {
                ("\x1b[31m‚úó\x1b[0m", "\x1b[1m\x1b[31mFAIL\x1b[0m")
            };
            println!("  {} \x1b[1m{}\x1b[0m  {}", icon, task.name, pass_text);
            println!(
                "    Reward: \x1b[{}m{:.4}\x1b[0m  Steps: {}  Time: {:.1}s",
                if r.reward > 0.0 { "32" } else { "90" },
                r.reward,
                r.steps,
                elapsed
            );

            if let Some(ref err) = r.error {
                println!();
                println!("    \x1b[33m‚ö† Error:\x1b[0m");
                for line in err.lines().take(15) {
                    println!("      \x1b[90m{}\x1b[0m", line);
                }
            }

            if !r.verification.output.is_empty() {
                println!();
                println!("    \x1b[90mVerification:\x1b[0m");
                for line in r.verification.output.lines().take(5) {
                    println!("      \x1b[90m{}\x1b[0m", line);
                }
            }

            println!();
            println!("  \x1b[90müìÅ Logs:\x1b[0m {}", logs_dir.display());
        }
        Err(e) => {
            println!("  \x1b[31m‚úó\x1b[0m Failed: {}", e);
            error!("Trial failed: {:?}", e);
        }
    }

    println!();

    Ok(())
}

/// Simple directory walker
fn walkdir(path: &std::path::Path) -> Vec<std::fs::DirEntry> {
    let mut files = vec![];
    if let Ok(entries) = std::fs::read_dir(path) {
        for entry in entries.flatten() {
            if entry.path().is_dir() {
                files.extend(walkdir(&entry.path()));
            } else {
                files.push(entry);
            }
        }
    }
    files
}
