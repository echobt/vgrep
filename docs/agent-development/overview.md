# Agent Development Guide

Build agents for Term Challenge using our SDK with streaming LLM support.

## SDK Installation

| Language | Installation |
|----------|-------------|
| Python | `pip install -e sdk/python` |
| TypeScript | `cd sdk/typescript && npm install && npm run build` |
| Rust | Add `term-sdk = { path = "sdk/rust" }` to Cargo.toml |

## Quick Start

### Python

```python
from term_sdk import Agent, Request, Response, run

class MyAgent(Agent):
    def solve(self, req: Request) -> Response:
        if req.step == 1:
            return Response.cmd("ls -la")
        return Response.done()

if __name__ == "__main__":
    run(MyAgent())
```

### TypeScript

```typescript
import { Agent, Request, Response, run } from 'term-sdk';

class MyAgent implements Agent {
  solve(req: Request): Response {
    if (req.step === 1) return Response.cmd("ls -la");
    return Response.done();
  }
}

run(new MyAgent());
```

### Rust

```rust
use term_sdk::{Agent, Request, Response, run};

struct MyAgent;

impl Agent for MyAgent {
    fn solve(&mut self, req: &Request) -> Response {
        if req.step == 1 { return Response::cmd("ls -la"); }
        Response::done()
    }
}

fn main() { run(&mut MyAgent); }
```

## LLM with Streaming

See responses in real-time:

### Python

```python
from term_sdk import Agent, Request, Response, LLM, LLMError, run

class StreamingAgent(Agent):
    def setup(self):
        self.llm = LLM()
    
    def solve(self, req: Request) -> Response:
        try:
            full_text = ""
            for chunk in self.llm.stream(
                f"Solve: {req.instruction}",
                model="claude-3-haiku"
            ):
                print(chunk, end="", flush=True)
                full_text += chunk
            
            return Response.from_llm(full_text)
        except LLMError as e:
            print(f"Error {e.code}: {e.message}")
            return Response.done()
```

### TypeScript

```typescript
import { Agent, Request, Response, LLM, LLMError, run } from 'term-sdk';

class StreamingAgent implements Agent {
  private llm = new LLM();

  async solve(req: Request): Promise<Response> {
    try {
      let fullText = "";
      for await (const chunk of this.llm.stream(
        `Solve: ${req.instruction}`,
        { model: "claude-3-haiku" }
      )) {
        process.stdout.write(chunk);
        fullText += chunk;
      }
      return Response.fromLLM(fullText);
    } catch (e) {
      if (e instanceof LLMError) console.error(`Error: ${e.code}`);
      return Response.done();
    }
  }
}

run(new StreamingAgent());
```

### Rust

```rust
use term_sdk::{Agent, Request, Response, LLM, run};

struct StreamingAgent { llm: LLM }

impl Agent for StreamingAgent {
    fn solve(&mut self, req: &Request) -> Response {
        match self.llm.ask_stream(&req.instruction, "claude-3-haiku", |chunk| {
            print!("{}", chunk);
            true
        }) {
            Ok(r) => Response::from_llm(&r.text),
            Err(e) => { eprintln!("Error: {}", e); Response::done() }
        }
    }
}

fn main() { run(&mut StreamingAgent { llm: LLM::new() }); }
```

## Multi-Model Usage

Use different models dynamically:

```python
from term_sdk import LLM

llm = LLM()

# Fast model for quick analysis
analysis = llm.ask("Analyze briefly", model="claude-3-haiku")

# Powerful model for complex reasoning
solution = llm.ask("Solve step by step", model="claude-3-opus")

# Code-optimized model
code = llm.ask("Write the bash command", model="gpt-4o")

# Check per-model stats
print(llm.get_stats())
```

## Error Handling

All SDKs use structured JSON errors:

```python
from term_sdk import LLM, LLMError

try:
    result = llm.ask("Question", model="claude-3-haiku")
except LLMError as e:
    print(f"Code: {e.code}")        # "rate_limit"
    print(f"Message: {e.message}")  # "Rate limit exceeded"
    print(f"Details: {e.details}")  # {"http_status": 429, ...}
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `authentication_error` | 401 | Invalid API key |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `no_model` | - | No model specified |
| `unknown_function` | - | Function not registered |

## Providers

| Provider | Env Variable | Models |
|----------|--------------|--------|
| OpenRouter (default) | `OPENROUTER_API_KEY` | Claude, GPT-4, Llama, Mixtral |
| Chutes | `CHUTES_API_KEY` | Llama, Qwen, Mixtral |

## Available Models

| Model | Speed | Cost | Best For |
|-------|-------|------|----------|
| `claude-3-haiku` | Fast | $ | Quick decisions |
| `claude-3-sonnet` | Medium | $$ | Balanced |
| `claude-3-opus` | Slow | $$$ | Complex reasoning |
| `gpt-4o` | Medium | $$ | Code generation |
| `gpt-4o-mini` | Fast | $ | Fast code tasks |
| `llama-3-70b` | Medium | $ | Open source |
| `mixtral-8x7b` | Fast | $ | Open source |

## Function Calling

```python
from term_sdk import LLM, Tool

llm = LLM()
llm.register_function("search", lambda query: f"Found: {query}")

tools = [Tool("search", "Search files", {
    "type": "object",
    "properties": {"query": {"type": "string"}}
})]

result = llm.chat_with_functions(
    [{"role": "user", "content": "Search for Python files"}],
    tools,
    model="claude-3-sonnet"
)
```

## Protocol

### Request (harness → agent)

```json
{
  "instruction": "Create hello.txt",
  "step": 2,
  "last_command": "ls -la",
  "output": "total 0...",
  "exit_code": 0,
  "cwd": "/app"
}
```

### Response (agent → harness)

```json
{
  "command": "echo 'Hello' > hello.txt",
  "text": "Creating file...",
  "task_complete": false
}
```

## Language Guides

- [Python Guide](python.md)
- [TypeScript Guide](typescript.md)
- [Rust Guide](rust.md)
