# Rust SDK

Build agents with streaming LLM support.

## Installation

Add to `Cargo.toml`:

```toml
[dependencies]
term-sdk = { path = "sdk/rust" }
```

## Quick Start

```rust
use term_sdk::{Agent, Request, Response, run};

struct MyAgent;

impl Agent for MyAgent {
    fn solve(&mut self, req: &Request) -> Response {
        if req.step == 1 {
            return Response::cmd("ls -la");
        }
        Response::done()
    }
}

fn main() {
    run(&mut MyAgent);
}
```

## Streaming LLM

```rust
use term_sdk::{Agent, Request, Response, LLM, run};

struct StreamingAgent {
    llm: LLM,
}

impl Agent for StreamingAgent {
    fn solve(&mut self, req: &Request) -> Response {
        let prompt = format!("Task: {}\nOutput: {:?}", req.instruction, req.output);
        
        // Stream with callback
        match self.llm.ask_stream(&prompt, "claude-3-haiku", |chunk| {
            print!("{}", chunk);
            true  // Return false to stop early
        }) {
            Ok(result) => Response::from_llm(&result.text),
            Err(e) => {
                // e is JSON: {"error": {"code": "...", "message": "..."}}
                eprintln!("Error: {}", e);
                Response::done()
            }
        }
    }
}

fn main() {
    let mut agent = StreamingAgent { llm: LLM::new() };
    run(&mut agent);
}
```

## Streaming API

```rust
use term_sdk::LLM;

let mut llm = LLM::new();

// Streaming with callback
let result = llm.ask_stream("Tell a story", "claude-3-haiku", |chunk| {
    print!("{}", chunk);
    true  // Return false to stop early
})?;
println!("\nTotal: {}", result.text);

// Non-streaming
let result = llm.ask("Question", "claude-3-haiku")?;
```

## Multi-Model Usage

```rust
use term_sdk::LLM;

let mut llm = LLM::new();

// Fast model for quick decisions
let quick = llm.ask("Should I use ls or find?", "claude-3-haiku")?;

// Powerful model for complex reasoning
let solution = llm.chat_with_model(
    &[Message::user("Solve step by step")],
    "claude-3-opus",
    Some(0.2),   // temperature
    None,        // max_tokens
    None,        // tools
)?;

// Per-model stats
for (model, stats) in llm.get_all_stats() {
    println!("{}: {} tokens, ${:.4}", model, stats.tokens, stats.cost);
}
```

## Error Handling

Errors are returned as JSON strings:

```rust
use term_sdk::LLM;

let mut llm = LLM::new();

match llm.ask("Question", "claude-3-haiku") {
    Ok(response) => println!("{}", response.text),
    Err(error_json) => {
        // error_json: {"error": {"code": "rate_limit", "message": "...", "details": {...}}}
        eprintln!("Error: {}", error_json);
        
        // Parse if needed
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(&error_json) {
            let code = parsed["error"]["code"].as_str().unwrap_or("unknown");
            let message = parsed["error"]["message"].as_str().unwrap_or("");
            eprintln!("Code: {}, Message: {}", code, message);
        }
    }
}
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `authentication_error` | 401 | Invalid API key |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `no_model` | - | No model specified |
| `unknown_function` | - | Function not registered |

## Function Calling

```rust
use term_sdk::{LLM, Tool, Message};

let mut llm = LLM::new();

// Register function
llm.register_function("search", |args| {
    let query = args.get("query").and_then(|v| v.as_str()).unwrap_or("");
    Ok(format!("Found: {}", query))
});

// Define tool
let tools = vec![Tool::new("search", "Search for files")
    .with_parameters(serde_json::json!({
        "type": "object",
        "properties": { "query": { "type": "string" } }
    }))];

// Chat with functions
let result = llm.chat_with_functions(
    &[Message::user("Search for Rust files")],
    &tools,
    "claude-3-sonnet",
    5,  // max iterations
)?;
```

## API Reference

### LLM

```rust
impl LLM {
    fn new() -> Self;
    fn with_provider(provider: Provider) -> Self;
    fn default_model(self, model: &str) -> Self;
    fn temperature(self, t: f32) -> Self;
    fn max_tokens(self, t: u32) -> Self;
    
    // Streaming
    fn ask_stream<F>(&mut self, prompt: &str, model: &str, on_chunk: F) -> Result<LLMResponse, String>
    where F: FnMut(&str) -> bool;
    
    // Non-streaming
    fn ask(&mut self, prompt: &str, model: &str) -> Result<LLMResponse, String>;
    fn chat(&mut self, messages: &[Message], tools: Option<&[Tool]>) -> Result<LLMResponse, String>;
    fn chat_with_functions(&mut self, messages, tools, model, max_iter) -> Result<LLMResponse, String>;
    
    // Functions
    fn register_function<F>(&mut self, name: &str, handler: F);
    
    // Stats
    fn get_stats(&self, model: Option<&str>) -> Option<ModelStats>;
    fn get_all_stats(&self) -> &HashMap<String, ModelStats>;
}
```

### Request

```rust
pub struct Request {
    pub instruction: String,
    pub step: u32,
    pub last_command: Option<String>,
    pub output: Option<String>,
    pub exit_code: Option<i32>,
    pub cwd: String,
}

impl Request {
    fn first(&self) -> bool;   // step == 1
    fn ok(&self) -> bool;      // exit_code == Some(0)
    fn failed(&self) -> bool;  // exit_code.is_some() && exit_code != Some(0)
    fn has(&self, pattern: &str) -> bool;
}
```

### Response

```rust
Response::cmd("ls -la")         // Execute command
Response::say("message")        // Text only
Response::done()                // Task complete
Response::from_llm(&text)       // Parse from LLM
```

## Providers

| Provider | Env Variable |
|----------|--------------|
| OpenRouter (default) | `OPENROUTER_API_KEY` |
| Chutes | `CHUTES_API_KEY` |

## Models

| Model | Speed | Cost |
|-------|-------|------|
| `claude-3-haiku` | Fast | $ |
| `claude-3-sonnet` | Medium | $$ |
| `claude-3-opus` | Slow | $$$ |
| `gpt-4o` | Medium | $$ |
| `gpt-4o-mini` | Fast | $ |
| `llama-3-70b` | Medium | $ |
