# Python SDK

Build agents with streaming LLM support.

## Installation

```bash
pip install -e sdk/python
```

## Quick Start

```python
from term_sdk import Agent, Request, Response, run

class MyAgent(Agent):
    def solve(self, req: Request) -> Response:
        if req.step == 1:
            return Response.cmd("ls -la")
        return Response.done()

if __name__ == "__main__":
    run(MyAgent())
```

## Streaming LLM

```python
from term_sdk import Agent, Request, Response, LLM, LLMError, run

class StreamingAgent(Agent):
    def setup(self):
        self.llm = LLM()
    
    def solve(self, req: Request) -> Response:
        try:
            # Stream chunks in real-time
            full_text = ""
            for chunk in self.llm.stream(
                f"Task: {req.instruction}\nOutput: {req.output}",
                model="claude-3-haiku"
            ):
                print(chunk, end="", flush=True)
                full_text += chunk
            
            return Response.from_llm(full_text)
        except LLMError as e:
            print(f"Error {e.code}: {e.message}")
            return Response.done()

if __name__ == "__main__":
    run(StreamingAgent())
```

## Streaming API

```python
from term_sdk import LLM

llm = LLM()

# Iterator - yields chunks
for chunk in llm.stream("Tell a story", model="claude-3-haiku"):
    print(chunk, end="")

# With callback - return False to stop
result = llm.ask_stream(
    "Solve this",
    model="gpt-4o",
    on_chunk=lambda text: True  # Return False to stop early
)
print(result.text)

# Non-streaming
result = llm.ask("Question", model="claude-3-haiku")
```

## Multi-Model Usage

```python
from term_sdk import LLM

llm = LLM()

# Fast model for quick decisions
quick = llm.ask("Should I use ls or find?", model="claude-3-haiku")

# Powerful model for complex reasoning
solution = llm.ask(
    "Solve step by step",
    model="claude-3-opus",
    temperature=0.2
)

# Code-optimized model
code = llm.ask("Write bash command", model="gpt-4o", max_tokens=500)

# Per-model stats
stats = llm.get_stats()
print(f"Haiku: {stats['per_model'].get('claude-3-haiku', {})}")
print(f"Total cost: ${stats['total_cost']:.4f}")
```

## Error Handling

```python
from term_sdk import LLM, LLMError

llm = LLM()

try:
    result = llm.ask("Question", model="claude-3-haiku")
except LLMError as e:
    print(f"Code: {e.code}")           # "rate_limit"
    print(f"Message: {e.message}")     # "Rate limit exceeded"
    print(f"Details: {e.details}")     # {"http_status": 429, ...}
    print(f"JSON: {e.to_json()}")      # Full JSON error
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `authentication_error` | 401 | Invalid API key |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `no_model` | - | No model specified |
| `unknown_function` | - | Function not registered |

## Function Calling

```python
from term_sdk import LLM, Tool

llm = LLM()

# Register function
llm.register_function("search", lambda query: f"Found: {query}")

# Define tool
tools = [Tool(
    name="search",
    description="Search for files",
    parameters={"type": "object", "properties": {"query": {"type": "string"}}}
)]

# Chat with functions
result = llm.chat_with_functions(
    messages=[{"role": "user", "content": "Search for Python files"}],
    tools=tools,
    model="claude-3-sonnet"
)
```

## API Reference

### LLM

```python
class LLM:
    def __init__(
        self,
        provider: str = "openrouter",  # or "chutes"
        default_model: str = None,
        temperature: float = 0.3,
        max_tokens: int = 4096,
    ): ...
    
    # Streaming
    def stream(self, prompt: str, model: str, ...) -> Iterator[str]: ...
    def ask_stream(self, prompt: str, model: str, on_chunk: Callable) -> LLMResponse: ...
    
    # Non-streaming
    def ask(self, prompt: str, model: str, ...) -> LLMResponse: ...
    def chat(self, messages: List[dict], model: str, ...) -> LLMResponse: ...
    def chat_with_functions(self, messages, tools, model, ...) -> LLMResponse: ...
    
    # Functions
    def register_function(self, name: str, handler: Callable): ...
    
    # Stats
    def get_stats(self, model: str = None) -> dict: ...
```

### Request

```python
@dataclass
class Request:
    instruction: str
    step: int
    last_command: str | None
    output: str | None
    exit_code: int | None
    cwd: str
    
    first: bool      # step == 1
    ok: bool         # exit_code == 0
    failed: bool     # exit_code != 0
    
    def has(*patterns) -> bool: ...
```

### Response

```python
Response.cmd("ls -la")         # Execute command
Response.say("message")        # Text only
Response.done()                # Task complete
Response.from_llm(text)        # Parse from LLM
```

## Providers

| Provider | Env Variable |
|----------|--------------|
| OpenRouter (default) | `OPENROUTER_API_KEY` |
| Chutes | `CHUTES_API_KEY` |

## Models

| Model | Speed | Cost |
|-------|-------|------|
| `claude-3-haiku` | Fast | $ |
| `claude-3-sonnet` | Medium | $$ |
| `claude-3-opus` | Slow | $$$ |
| `gpt-4o` | Medium | $$ |
| `gpt-4o-mini` | Fast | $ |
| `llama-3-70b` | Medium | $ |
