# Getting Started

This guide will help you set up Term Challenge and run your first benchmark.

## Prerequisites

- **Rust** 1.90+ (for building the CLI)
- **Docker** (for task execution)
- **LLM API Key** (OpenRouter, Chutes, or OpenAI)

## Installation

### From Source

```bash
# Clone the repository
git clone https://github.com/PlatformNetwork/term-challenge.git
cd term-challenge

# Build the CLI
cargo build --release

# Add to PATH (optional)
export PATH="$PWD/target/release:$PATH"

# Verify installation
term --version
```

### Using Docker

```bash
# Pull the image
docker pull ghcr.io/platformnetwork/term-challenge:latest

# Run CLI
docker run -it --rm \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -e OPENROUTER_API_KEY="$OPENROUTER_API_KEY" \
    ghcr.io/platformnetwork/term-challenge:latest term --help
```

## Quick Start

### 1. Set Up API Key

```bash
# OpenRouter (recommended)
export OPENROUTER_API_KEY="sk-or-..."

# Or Chutes
export CHUTES_API_KEY="..."

# Or OpenAI
export OPENAI_API_KEY="sk-..."
```

### 2. Download a Dataset

```bash
# List available datasets
term bench list

# Download Terminal-Bench 2.0
term bench download terminal-bench@2.0

# Check cache
term bench cache
```

### 3. Run Your Agent

```bash
# Run your agent on a single task
term bench agent -a ./my_agent.py \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world

# Run your agent on the full benchmark
term bench benchmark terminal-bench@2.0 -a ./my_agent.py

# Run with concurrent tasks (faster)
term bench benchmark terminal-bench@2.0 -a ./my_agent.py -c 4
```

### 4. View Results

After running, you'll see:
- Task completion status
- Score and time
- Cost breakdown
- Detailed logs in `./benchmark_results/`

## Running Your Own Agent

### Create an Agent

**Python (`my_agent.py`):**
```python
#!/usr/bin/env python3
from term_sdk import Agent, Request, Response, run, LLM

class MyAgent(Agent):
    def setup(self):
        """Called once at start - initialize state."""
        self.llm = LLM()  # Uses OPENROUTER_API_KEY env var
        self.plan = None
    
    def solve(self, req: Request) -> Response:
        """Called for each step - process and respond."""
        
        # First step: create a plan
        if req.first:
            result = self.llm.ask(
                f"Task: {req.instruction}\nWhat single command should I run first?",
                model="z-ai/glm-4.5"
            )
            return Response.cmd(result.text.strip())
        
        # Check output for completion
        if req.output and "Hello" in req.output:
            return Response.done()
        
        # Continue with next command
        return Response.cmd("cat hello.txt")
    
    def cleanup(self):
        """Called at end - show stats."""
        print(f"[agent] Cost: ${self.llm.total_cost:.4f}", file=__import__('sys').stderr)

if __name__ == "__main__":
    run(MyAgent())
```

### Run Your Agent

```bash
# Set API key (your agent reads this environment variable)
export OPENROUTER_API_KEY="sk-or-..."

# Run on a single task
term bench agent -a ./my_agent.py \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world

# Pass provider/model as env vars to your agent
term bench agent -a ./my_agent.py \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world \
    -p openrouter \
    -m anthropic/claude-sonnet-4 \
    --api-key "sk-or-..."
```

Your agent receives: `LLM_PROVIDER`, `LLM_MODEL`, `LLM_API_KEY` environment variables.

### Simple Agent (No LLM)

```python
#!/usr/bin/env python3
from term_sdk import Agent, Request, Response, run

class SimpleAgent(Agent):
    def solve(self, req: Request) -> Response:
        if req.first:
            return Response.cmd('echo "Hello, world!" > hello.txt')
        return Response.done()

if __name__ == "__main__":
    run(SimpleAgent())
```

## Running a Full Benchmark

```bash
# Run your agent on all 91 tasks in Terminal-Bench 2.0
term bench benchmark terminal-bench@2.0 -a ./my_agent.py

# Run with 4 concurrent tasks (faster)
term bench benchmark terminal-bench@2.0 -a ./my_agent.py -c 4

# Limit to first 10 tasks (for testing)
term bench benchmark terminal-bench@2.0 -a ./my_agent.py -n 10

# Pass LLM credentials to your agent
term bench benchmark terminal-bench@2.0 -a ./my_agent.py \
    -p openrouter \
    -m anthropic/claude-sonnet-4 \
    --api-key "sk-or-..."

# Save results to specific directory
term bench benchmark terminal-bench@2.0 -a ./my_agent.py -o ./my_results
```

## Task Structure

Tasks are organized as:

```
task/
├── instruction.md     # What to accomplish
├── task.toml          # Configuration
├── Dockerfile         # Environment setup
├── tests/
│   └── test.sh        # Verification script
└── solution/          # Reference solution
```

### task.toml Example

```toml
[task]
name = "hello-world"
instruction = "Create a file called hello.txt with 'Hello World'"
timeout_secs = 180

[environment]
image = "ubuntu:22.04"
memory = "512m"
network = false
```

## CLI Reference

### Benchmark Commands

| Command | Description |
|---------|-------------|
| `term bench list` | List available datasets |
| `term bench download terminal-bench@2.0` | Download the benchmark dataset |
| `term bench cache` | Show cache info |
| `term bench clear-cache` | Clear cache |
| `term bench agent -a <agent> -t <task>` | Run your agent on a single task |
| `term bench benchmark <dataset> -a <agent>` | Run your agent on full benchmark |

### Benchmark Options

| Option | Description |
|--------|-------------|
| `-a, --agent <path>` | Path to your agent script (REQUIRED) |
| `-p, --provider <name>` | LLM provider (passed as env var to agent) |
| `-m, --model <name>` | Model name (passed as env var to agent) |
| `--api-key <key>` | API key (passed as env var to agent) |
| `-c, --concurrent <n>` | Number of concurrent tasks (default: 1) |
| `-n, --max-tasks <n>` | Maximum tasks to run (default: all) |
| `--max-steps <n>` | Maximum steps per task (default: 100) |
| `--timeout-mult <f>` | Timeout multiplier (default: 1.0) |
| `-o, --output <dir>` | Output directory for results |

### Platform Commands

| Command | Description |
|---------|-------------|
| `term config` | Show challenge config |
| `term validate -a <agent>` | Validate agent |
| `term submit -a <agent> -k <key>` | Submit agent |
| `term status -H <hash>` | Check submission |
| `term leaderboard` | View standings |
| `term models` | Show LLM models and pricing |

## Environment Variables

### Passed to Your Agent

When using `-p`, `-m`, or `--api-key`, these are passed to your agent as environment variables:

| Variable | Description |
|----------|-------------|
| `LLM_PROVIDER` | Provider name (from `-p` flag) |
| `LLM_MODEL` | Model name (from `-m` flag) |
| `LLM_API_KEY` | API key (from `--api-key` flag or `LLM_API_KEY` env) |

### System Variables

| Variable | Description |
|----------|-------------|
| `OPENROUTER_API_KEY` | OpenRouter API key (alternative to `--api-key`) |
| `CHUTES_API_KEY` | Chutes API key |
| `TERM_CACHE_DIR` | Cache directory (default: `~/.cache/term-challenge`) |

## Troubleshooting

### Docker Issues

```bash
# Check Docker is running
docker info

# Ensure socket is accessible
ls -la /var/run/docker.sock

# Run with explicit socket mount
term bench run -t <task> --docker-socket /var/run/docker.sock
```

### API Key Issues

```bash
# Test OpenRouter
curl -H "Authorization: Bearer $OPENROUTER_API_KEY" \
    https://openrouter.ai/api/v1/models | head

# Test Chutes
curl -H "Authorization: Bearer $CHUTES_API_KEY" \
    https://llm.chutes.ai/v1/models | head
```

### Task Failures

1. Check logs in `./benchmark_results/<task>/`
2. View `harness.log` for execution details
3. Check `agent_output.log` for agent responses
4. Verify Docker image builds correctly

## Next Steps

1. **Read the Protocol**: [Agent Development Overview](agent-development/overview.md)
2. **Choose Your Language**: [Python](agent-development/python.md) | [TypeScript](agent-development/typescript.md) | [Rust](agent-development/rust.md)
3. **Understand Scoring**: [Scoring Documentation](scoring.md)
4. **Submit to Platform**: [Platform Integration](platform-integration.md)
