//! Terminal-Bench Integration Test
//!
//! Tests the Rust term-challenge code with real terminal-bench tasks
//! using OpenRouter for LLM inference.

#[allow(unused_imports)]
use std::path::PathBuf;
use std::time::Instant;
#[allow(unused_imports)]
use term_challenge::{
    ChallengeConfig, DistributionConfig, PackageType, PipelineAgentSubmission,
    PipelineEvaluationResult, PythonWhitelist, ReceiveResult, ReceiveStatus, Task, TaskConfig,
    TaskEvalResult, TaskRegistry, ValidatorDistributor, ValidatorInfo, WhitelistConfig,
};

fn get_api_key() -> String {
    std::env::var("OPENROUTER_API_KEY").unwrap_or_else(|_| "test-key-not-set".to_string())
}
const OPENROUTER_URL: &str = "https://openrouter.ai/api/v1/chat/completions";

/// Terminal-bench task definition (matching Python format)
#[allow(dead_code)]
#[derive(Debug, Clone)]
struct TerminalBenchTask {
    id: String,
    instruction: String,
    difficulty: String,
    category: String,
    timeout_secs: u64,
    test_timeout_secs: u64,
}

/// Get sample terminal-bench tasks (matching real dataset)
fn get_terminal_bench_tasks() -> Vec<TerminalBenchTask> {
    vec![
        TerminalBenchTask {
            id: "hello-world".to_string(),
            instruction: "Create a file called hello.txt in the current directory. Write \"Hello, world!\" to it. Make sure it ends in a newline. Don't make any other files or folders.".to_string(),
            difficulty: "easy".to_string(),
            category: "file-operations".to_string(),
            timeout_secs: 360,
            test_timeout_secs: 60,
        },
        TerminalBenchTask {
            id: "chess-best-move".to_string(),
            instruction: "You are given a chess position in FEN notation. Find the best move for the current player. Write your answer to best_move.txt in algebraic notation (e.g., e2e4, g1f3).".to_string(),
            difficulty: "medium".to_string(),
            category: "problem-solving".to_string(),
            timeout_secs: 300,
            test_timeout_secs: 60,
        },
        TerminalBenchTask {
            id: "csv-to-parquet".to_string(),
            instruction: "Convert the file data.csv to Parquet format. Save it as data.parquet in the same directory. Preserve all column types and data.".to_string(),
            difficulty: "easy".to_string(),
            category: "data-processing".to_string(),
            timeout_secs: 180,
            test_timeout_secs: 30,
        },
        TerminalBenchTask {
            id: "git-workflow-hack".to_string(),
            instruction: "Initialize a git repository, create a branch called 'feature', make a commit with message 'initial commit', then merge it back to main.".to_string(),
            difficulty: "medium".to_string(),
            category: "version-control".to_string(),
            timeout_secs: 300,
            test_timeout_secs: 60,
        },
        TerminalBenchTask {
            id: "configure-git-webserver".to_string(),
            instruction: "Set up a simple git web server using git-http-backend. Configure it to serve repositories from /var/git. Create a test repository.".to_string(),
            difficulty: "hard".to_string(),
            category: "system-admin".to_string(),
            timeout_secs: 600,
            test_timeout_secs: 120,
        },
    ]
}

/// Call OpenRouter LLM
fn call_llm(task: &TerminalBenchTask) -> Result<LLMResponse, String> {
    let client = reqwest::blocking::Client::new();

    let system_prompt = r#"You are a terminal command expert. You will be given a task to complete in a Linux terminal environment.

Respond ONLY with valid JSON containing:
- "analysis": Brief analysis of what needs to be done
- "plan": Step-by-step plan to complete the task
- "commands": Array of command objects with "keystrokes" (the command + \n) and "duration" (seconds to wait)
- "task_complete": boolean indicating if task will be complete after these commands

Example response:
{
  "analysis": "Need to create a file with specific content",
  "plan": "1. Use echo to write content to file",
  "commands": [
    {"keystrokes": "echo 'Hello, world!' > hello.txt\n", "duration": 0.5}
  ],
  "task_complete": true
}"#;

    let user_prompt = format!(
        "Task: {}\n\nCategory: {}\nDifficulty: {}\n\nProvide the commands to complete this task.",
        task.instruction, task.category, task.difficulty
    );

    let payload = serde_json::json!({
        "model": "openai/gpt-4o-mini",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1500
    });

    let start = Instant::now();

    let response = client
        .post(OPENROUTER_URL)
        .header("Authorization", format!("Bearer {}", get_api_key()))
        .header("Content-Type", "application/json")
        .header("HTTP-Referer", "https://term-challenge.test")
        .json(&payload)
        .send()
        .map_err(|e| format!("Request failed: {}", e))?;

    let elapsed = start.elapsed();

    if !response.status().is_success() {
        return Err(format!("API error: {}", response.status()));
    }

    let result: serde_json::Value = response.json().map_err(|e| format!("Parse error: {}", e))?;

    let content = result["choices"][0]["message"]["content"]
        .as_str()
        .ok_or("No content")?
        .to_string();

    let usage = &result["usage"];
    let input_tokens = usage["prompt_tokens"].as_u64().unwrap_or(0);
    let output_tokens = usage["completion_tokens"].as_u64().unwrap_or(0);

    Ok(LLMResponse {
        content,
        input_tokens,
        output_tokens,
        latency_ms: elapsed.as_millis() as u64,
    })
}

#[derive(Debug)]
struct LLMResponse {
    content: String,
    input_tokens: u64,
    output_tokens: u64,
    latency_ms: u64,
}

/// Parse agent response JSON
fn parse_response(content: &str) -> Result<AgentResponse, String> {
    // Find JSON in response
    let json_start = content.find('{').ok_or("No JSON found")?;
    let mut brace_count = 0;
    let mut json_end = json_start;

    for (i, c) in content[json_start..].chars().enumerate() {
        match c {
            '{' => brace_count += 1,
            '}' => {
                brace_count -= 1;
                if brace_count == 0 {
                    json_end = json_start + i + 1;
                    break;
                }
            }
            _ => {}
        }
    }

    let json_str = &content[json_start..json_end];
    let parsed: serde_json::Value =
        serde_json::from_str(json_str).map_err(|e| format!("JSON parse error: {}", e))?;

    let commands: Vec<Command> = parsed["commands"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .filter_map(|cmd| {
                    Some(Command {
                        keystrokes: cmd["keystrokes"].as_str()?.to_string(),
                        duration: cmd["duration"].as_f64().unwrap_or(1.0),
                    })
                })
                .collect()
        })
        .unwrap_or_default();

    Ok(AgentResponse {
        analysis: parsed["analysis"].as_str().unwrap_or("").to_string(),
        plan: parsed["plan"].as_str().unwrap_or("").to_string(),
        commands,
        task_complete: parsed["task_complete"].as_bool().unwrap_or(false),
    })
}

#[derive(Debug)]
struct AgentResponse {
    analysis: String,
    plan: String,
    commands: Vec<Command>,
    task_complete: bool,
}

#[derive(Debug)]
struct Command {
    keystrokes: String,
    duration: f64,
}

/// Evaluate agent response for a task
fn evaluate_task(task: &TerminalBenchTask, response: &AgentResponse) -> TaskEvaluation {
    let mut score = 0.0;
    let mut feedback = Vec::new();

    // Check if we have analysis
    if !response.analysis.is_empty() {
        score += 0.1;
        feedback.push("Has analysis".to_string());
    }

    // Check if we have a plan
    if !response.plan.is_empty() {
        score += 0.1;
        feedback.push("Has plan".to_string());
    }

    // Check if we have commands
    if !response.commands.is_empty() {
        score += 0.2;
        feedback.push(format!("Has {} commands", response.commands.len()));
    }

    // Task-specific evaluation
    match task.id.as_str() {
        "hello-world" => {
            let creates_file = response.commands.iter().any(|c| {
                c.keystrokes.contains("echo") && c.keystrokes.contains("hello.txt")
                    || c.keystrokes.contains(">") && c.keystrokes.contains("hello")
                    || c.keystrokes.contains("cat") && c.keystrokes.contains("hello")
                    || c.keystrokes.contains("printf") && c.keystrokes.contains("hello")
            });
            if creates_file {
                score += 0.6;
                feedback.push("Creates hello.txt correctly".to_string());
            } else {
                feedback.push("Missing file creation command".to_string());
            }
        }
        "chess-best-move" => {
            let has_chess_logic = response.commands.iter().any(|c| {
                c.keystrokes.contains("echo") && c.keystrokes.contains("best_move")
                    || c.keystrokes.contains("python")
                    || c.keystrokes.contains("stockfish")
            });
            if has_chess_logic {
                score += 0.6;
                feedback.push("Has chess move logic".to_string());
            }
        }
        "csv-to-parquet" => {
            let converts = response.commands.iter().any(|c| {
                c.keystrokes.contains("parquet")
                    || c.keystrokes.contains("pandas")
                    || c.keystrokes.contains("pyarrow")
                    || c.keystrokes.contains("python")
            });
            if converts {
                score += 0.6;
                feedback.push("Has conversion logic".to_string());
            }
        }
        "git-workflow-hack" => {
            let has_git = response
                .commands
                .iter()
                .any(|c| c.keystrokes.contains("git"));
            let has_branch = response
                .commands
                .iter()
                .any(|c| c.keystrokes.contains("branch") || c.keystrokes.contains("checkout"));
            let has_commit = response
                .commands
                .iter()
                .any(|c| c.keystrokes.contains("commit"));
            let has_merge = response
                .commands
                .iter()
                .any(|c| c.keystrokes.contains("merge"));

            if has_git {
                score += 0.15;
                feedback.push("Uses git".to_string());
            }
            if has_branch {
                score += 0.15;
                feedback.push("Creates branch".to_string());
            }
            if has_commit {
                score += 0.15;
                feedback.push("Makes commit".to_string());
            }
            if has_merge {
                score += 0.15;
                feedback.push("Merges branch".to_string());
            }
        }
        "configure-git-webserver" => {
            let has_server = response.commands.iter().any(|c| {
                c.keystrokes.contains("git-http-backend")
                    || c.keystrokes.contains("nginx")
                    || c.keystrokes.contains("apache")
                    || c.keystrokes.contains("httpd")
            });
            if has_server {
                score += 0.6;
                feedback.push("Has server configuration".to_string());
            }
        }
        _ => {
            // Default: give partial score if task_complete is set
            if response.task_complete {
                score += 0.4;
                feedback.push("Task marked complete".to_string());
            }
        }
    }

    TaskEvaluation {
        task_id: task.id.clone(),
        score,
        passed: score >= 0.6,
        feedback,
    }
}

#[allow(dead_code)]
#[derive(Debug)]
struct TaskEvaluation {
    task_id: String,
    score: f64,
    passed: bool,
    feedback: Vec<String>,
}

/// Full pipeline result
#[allow(dead_code)]
#[derive(Debug)]
struct PipelineResult {
    task_id: String,
    difficulty: String,
    category: String,
    llm_response: Option<LLMResponse>,
    agent_response: Option<AgentResponse>,
    evaluation: Option<TaskEvaluation>,
    error: Option<String>,
}

#[test]
#[ignore] // Run with: cargo test --test terminal_bench_integration -- --ignored --nocapture
fn test_terminal_bench_full_pipeline() {
    println!("\n{}", "=".repeat(70));
    println!("TERMINAL-BENCH INTEGRATION TEST - RUST PIPELINE");
    println!("{}\n", "=".repeat(70));

    // Step 1: Setup challenge config
    let config = ChallengeConfig::default();
    println!("[CONFIG] Min stake: {} TAO", config.min_stake_tao);
    println!(
        "[CONFIG] Max cost/task: ${}",
        config.pricing.max_cost_per_task_usd
    );
    println!(
        "[CONFIG] Task timeout: {}s\n",
        config.execution.max_task_timeout_secs
    );

    // Step 2: Get terminal-bench tasks
    let tasks = get_terminal_bench_tasks();
    println!("[TASKS] Loaded {} terminal-bench tasks:\n", tasks.len());
    for task in &tasks {
        println!("  - {} ({}) [{}]", task.id, task.difficulty, task.category);
    }
    println!();

    // Step 3: Run evaluation pipeline
    let mut results: Vec<PipelineResult> = Vec::new();
    let mut total_tokens = 0u64;
    let mut total_cost = 0.0f64;
    let pipeline_start = Instant::now();

    for (i, task) in tasks.iter().enumerate() {
        println!("{}", "-".repeat(60));
        println!(
            "[TASK {}/{}] {} ({})",
            i + 1,
            tasks.len(),
            task.id,
            task.difficulty
        );
        println!("{}", "-".repeat(60));
        println!(
            "Instruction: {}...",
            &task.instruction[..task.instruction.len().min(80)]
        );
        println!();

        // Call LLM
        print!("[LLM] Calling OpenRouter... ");
        let llm_result = call_llm(task);

        match llm_result {
            Ok(llm_response) => {
                println!("OK ({:.2}s)", llm_response.latency_ms as f64 / 1000.0);
                println!(
                    "[LLM] Tokens: {} in, {} out",
                    llm_response.input_tokens, llm_response.output_tokens
                );

                // Estimate cost (gpt-4o-mini pricing)
                let cost = (llm_response.input_tokens as f64 * 0.00015
                    + llm_response.output_tokens as f64 * 0.0006)
                    / 1000.0;
                total_cost += cost;
                total_tokens += llm_response.input_tokens + llm_response.output_tokens;

                // Parse response
                match parse_response(&llm_response.content) {
                    Ok(agent_response) => {
                        println!(
                            "[PARSE] Analysis: {}...",
                            &agent_response.analysis[..agent_response.analysis.len().min(50)]
                        );
                        println!("[PARSE] Commands: {}", agent_response.commands.len());

                        for (j, cmd) in agent_response.commands.iter().enumerate().take(3) {
                            println!("  {}. {} ({}s)", j + 1, cmd.keystrokes.trim(), cmd.duration);
                        }
                        if agent_response.commands.len() > 3 {
                            println!("  ... and {} more", agent_response.commands.len() - 3);
                        }

                        // Evaluate
                        let evaluation = evaluate_task(task, &agent_response);
                        let status = if evaluation.passed { "PASS" } else { "FAIL" };
                        println!("[EVAL] Score: {:.2} [{}]", evaluation.score, status);
                        println!("[EVAL] Feedback: {}", evaluation.feedback.join(", "));

                        results.push(PipelineResult {
                            task_id: task.id.clone(),
                            difficulty: task.difficulty.clone(),
                            category: task.category.clone(),
                            llm_response: Some(llm_response),
                            agent_response: Some(agent_response),
                            evaluation: Some(evaluation),
                            error: None,
                        });
                    }
                    Err(e) => {
                        println!("[PARSE] ERROR: {}", e);
                        results.push(PipelineResult {
                            task_id: task.id.clone(),
                            difficulty: task.difficulty.clone(),
                            category: task.category.clone(),
                            llm_response: Some(llm_response),
                            agent_response: None,
                            evaluation: None,
                            error: Some(e),
                        });
                    }
                }
            }
            Err(e) => {
                println!("ERROR: {}", e);
                results.push(PipelineResult {
                    task_id: task.id.clone(),
                    difficulty: task.difficulty.clone(),
                    category: task.category.clone(),
                    llm_response: None,
                    agent_response: None,
                    evaluation: None,
                    error: Some(e),
                });
            }
        }
        println!();
    }

    let pipeline_duration = pipeline_start.elapsed();

    // Step 4: Summary
    println!("\n{}", "=".repeat(70));
    println!("FINAL RESULTS");
    println!("{}\n", "=".repeat(70));

    let evaluated: Vec<_> = results.iter().filter(|r| r.evaluation.is_some()).collect();
    let passed: Vec<_> = evaluated
        .iter()
        .filter(|r| r.evaluation.as_ref().unwrap().passed)
        .collect();
    let avg_score: f64 = evaluated
        .iter()
        .map(|r| r.evaluation.as_ref().unwrap().score)
        .sum::<f64>()
        / evaluated.len().max(1) as f64;

    println!("Tasks evaluated: {}/{}", evaluated.len(), results.len());
    println!("Tasks passed:    {}/{}", passed.len(), evaluated.len());
    println!(
        "Average score:   {:.2} ({:.0}%)",
        avg_score,
        avg_score * 100.0
    );
    println!("Total tokens:    {}", total_tokens);
    println!("Estimated cost:  ${:.4}", total_cost);
    println!("Total time:      {:.2}s", pipeline_duration.as_secs_f64());
    println!();

    println!("Per-task breakdown:");
    println!(
        "{:<30} {:>10} {:>12} {:>8}",
        "Task", "Difficulty", "Score", "Status"
    );
    println!("{}", "-".repeat(65));

    for result in &results {
        let score = result.evaluation.as_ref().map(|e| e.score).unwrap_or(0.0);
        let status = match &result.evaluation {
            Some(e) if e.passed => "PASS",
            Some(_) => "FAIL",
            None => "ERROR",
        };
        println!(
            "{:<30} {:>10} {:>12.2} {:>8}",
            result.task_id, result.difficulty, score, status
        );
    }

    println!("\n{}", "=".repeat(70));

    // Verify coherent results
    assert!(evaluated.len() >= 4, "Should evaluate at least 4 tasks");
    assert!(avg_score > 0.3, "Average score should be > 30%");
    println!("Test PASSED - Results are coherent!");
}

#[test]
#[ignore]
fn test_whitelist_with_real_agent() {
    println!("\n=== WHITELIST VERIFICATION ===\n");

    // Real terminus2 agent code snippet
    let agent_code = r#"
import json
import re
import requests
from typing import Any

class Terminus2Agent:
    """Agent using OpenRouter for LLM."""
    
    @staticmethod
    def name():
        return "terminus2"
    
    def __init__(self, agent_id="terminus2", challenge_id="term-challenge", **kwargs):
        self.agent_id = agent_id
        self.model = kwargs.get("model_name", "openai/gpt-4o-mini")
    
    def solve(self, task_description: str) -> dict[str, Any]:
        # LLM call logic here
        return {"success": True, "commands": []}
"#;

    let config = ChallengeConfig::default();
    let whitelist_config = WhitelistConfig {
        allowed_stdlib: config.module_whitelist.allowed_stdlib.clone(),
        allowed_third_party: config.module_whitelist.allowed_third_party.clone(),
        forbidden_builtins: ["exec", "eval", "compile"]
            .iter()
            .map(|s| s.to_string())
            .collect(),
        max_code_size: 1024 * 1024,
        allow_subprocess: false,
        allow_network: true,
        allow_filesystem: false,
    };

    let whitelist = PythonWhitelist::new(whitelist_config);
    let result = whitelist.verify(agent_code);

    println!("Code size: {} bytes", agent_code.len());
    println!("Valid: {}", result.valid);
    println!("Imported modules: {:?}", result.imported_modules);
    println!("Errors: {:?}", result.errors);
    println!("Warnings: {:?}", result.warnings);

    assert!(result.valid, "Agent code should pass whitelist");
    println!("\nWhitelist verification PASSED!");
}

#[test]
#[ignore]
fn test_validator_stake_distribution() {
    println!("\n=== VALIDATOR STAKE DISTRIBUTION ===\n");

    let validators = vec![
        ValidatorInfo {
            hotkey: "5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY".to_string(),
            stake: 100_000_000_000_000,
            is_root: true,
        },
        ValidatorInfo {
            hotkey: "validator_top1".to_string(),
            stake: 80_000_000_000_000,
            is_root: false,
        },
        ValidatorInfo {
            hotkey: "validator_top2".to_string(),
            stake: 60_000_000_000_000,
            is_root: false,
        },
        ValidatorInfo {
            hotkey: "validator_top3".to_string(),
            stake: 40_000_000_000_000,
            is_root: false,
        },
        ValidatorInfo {
            hotkey: "validator_regular1".to_string(),
            stake: 20_000_000_000_000,
            is_root: false,
        },
        ValidatorInfo {
            hotkey: "validator_regular2".to_string(),
            stake: 10_000_000_000_000,
            is_root: false,
        },
        ValidatorInfo {
            hotkey: "validator_small".to_string(),
            stake: 5_000_000_000_000,
            is_root: false,
        },
    ];

    let config = DistributionConfig::default();
    let distributor = ValidatorDistributor::new(config);
    let (source_receivers, obfuscated_receivers) = distributor.classify_validators(&validators);

    println!("Total validators: {}", validators.len());
    println!();
    println!("SOURCE code receivers ({}):", source_receivers.len());
    for v in &source_receivers {
        let stake = validators
            .iter()
            .find(|x| &x.hotkey == v)
            .map(|x| x.stake / 1_000_000_000)
            .unwrap_or(0);
        println!("  - {} ({} TAO)", &v[..v.len().min(20)], stake);
    }
    println!();
    println!(
        "OBFUSCATED code receivers ({}):",
        obfuscated_receivers.len()
    );
    for v in &obfuscated_receivers {
        let stake = validators
            .iter()
            .find(|x| &x.hotkey == v)
            .map(|x| x.stake / 1_000_000_000)
            .unwrap_or(0);
        println!("  - {} ({} TAO)", v, stake);
    }

    assert!(source_receivers.len() <= 4, "Max 4 should receive source");
    assert!(
        source_receivers.contains(&"5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY".to_string()),
        "Root should receive source"
    );

    println!("\nValidator distribution PASSED!");
}
