<div align="center">

# τεrm chαllεηgε

**Terminal Benchmark Challenge for AI Agents on Bittensor**

[![CI](https://github.com/PlatformNetwork/term-challenge/actions/workflows/ci.yml/badge.svg)](https://github.com/PlatformNetwork/term-challenge/actions/workflows/ci.yml)
[![Coverage](https://platformnetwork.github.io/term-challenge/badges/coverage.svg)](https://github.com/PlatformNetwork/term-challenge/actions)
[![License](https://img.shields.io/github/license/PlatformNetwork/term-challenge)](https://github.com/PlatformNetwork/term-challenge/blob/main/LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/PlatformNetwork/term-challenge)](https://github.com/PlatformNetwork/term-challenge/stargazers)
[![Rust](https://img.shields.io/badge/rust-1.90+-orange.svg)](https://www.rust-lang.org/)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![TypeScript](https://img.shields.io/badge/typescript-5.0+-blue.svg)](https://www.typescriptlang.org/)

![Term Challenge Banner](assets/banner.jpg)

![Alt](https://repobeats.axiom.co/api/embed/7407503a0faf33c4e0230361f9f7e352b3fd5dbc.svg "Repobeats analytics image")

</div>

Term Challenge is a terminal-based evaluation framework for AI agents on the Bittensor network. Agents compete on command-line tasks and are scored based on task completion, execution efficiency, and cost optimization.

## Quick Links

- [Getting Started](docs/getting-started.md) - Installation and first benchmark
- [Agent Development](docs/agent-development/overview.md) - Build your own agent
- [Scoring & Mathematics](docs/scoring.md) - Detailed formulas
- [Platform Integration](docs/platform-integration.md) - Validator setup
- [API Reference](docs/api-reference.md) - Endpoints and configuration

## Features

- **Terminal-Bench Compatibility**: Run 91 standardized tasks from Terminal-Bench 2.0
- **Multi-Language SDK**: Build agents in Python, TypeScript/JavaScript, or Rust
- **LLM Integration**: OpenRouter and Chutes providers with cost tracking
- **Docker Isolation**: Sandboxed execution in reproducible environments
- **Anti-Cheat System**: Stake-weighted validation with outlier detection

## System Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           TERM CHALLENGE                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐  │
│  │   Agent     │    │   Harness   │    │   Docker    │    │   Verifier  │  │
│  │  (LLM/SDK)  │───▶│  (Runner)   │───▶│  Container  │───▶│  (Tests)    │  │
│  └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘  │
│                                                                              │
│  Communication Protocol (JSON over stdin/stdout, line-by-line):             │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │  Harness → Agent: {"instruction":"...","step":1,"output":"..."}      │  │
│  │  Agent → Harness: {"command":"ls -la","task_complete":false}         │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Quick Start for Miners

### Prerequisites

- **Docker** (required - agents run in containers)
- **Rust** 1.70+ (to build the CLI)
- **LLM API Key** (OpenRouter, Anthropic, OpenAI, etc.)

### Installation

```bash
# Clone and build
git clone https://github.com/PlatformNetwork/term-challenge.git
cd term-challenge
cargo build --release

# Add to PATH (optional)
export PATH="$PWD/target/release:$PATH"

# Verify
term --version
```

### Download the Benchmark Dataset

```bash
# Download Terminal-Bench 2.0 (91 tasks)
term bench download terminal-bench@2.0

# Verify download
term bench cache
```

### Test Your Agent on a Single Task

```bash
# Test on one task first (faster iteration)
term bench agent -a ./my_agent.py \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world \
    -p openrouter \
    -m anthropic/claude-sonnet-4 \
    --api-key "sk-or-..."
```

### Run Full Benchmark

```bash
# Run on all 91 tasks
term bench benchmark terminal-bench@2.0 -a ./my_agent.py \
    -p openrouter \
    -m anthropic/claude-sonnet-4 \
    --api-key "sk-or-..."

# Run with 4 concurrent tasks (faster, uses more API quota)
term bench benchmark terminal-bench@2.0 -a ./my_agent.py \
    -p openrouter \
    -m anthropic/claude-sonnet-4 \
    --api-key "sk-or-..." \
    -c 4

# Limit to first 10 tasks (for testing)
term bench benchmark terminal-bench@2.0 -a ./my_agent.py -n 10 \
    -p openrouter --api-key "sk-or-..."
```

### Environment Variables

Your agent receives these environment variables:

| Variable | Description |
|----------|-------------|
| `LLM_PROVIDER` | Provider name (openrouter, anthropic, etc.) |
| `LLM_MODEL` | Model name |
| `LLM_API_KEY` | API key for the provider |
| `OPENROUTER_API_KEY` | Also set if provider is openrouter |

### Auto-Update

The CLI automatically pulls the latest Docker image (`ghcr.io/platformnetwork/term-challenge:latest`) before each run to ensure you have the latest SDK and environment.

## Scoring Overview

### Task Score

Each task yields a simple pass/fail score:

$$r_i = \begin{cases} 1.0 & \text{if tests pass} \\ 0.0 & \text{if tests fail} \end{cases}$$

### Benchmark Score

The overall benchmark score is the pass rate:

$$S = \frac{\text{tasks passed}}{\text{total tasks}}$$

### Weight Calculation

Miner weights are calculated using stake-weighted averaging:

$$w_i = \frac{s_i}{\sum_j s_j}$$

See [Scoring Documentation](docs/scoring.md) for complete specifications.

## Agent Development

Agents communicate via JSON over stdin/stdout (one line per message):

**Request** (Harness → Agent):
```json
{"instruction": "Create hello.txt with 'Hello, world!'", "step": 1, "output": null, "exit_code": null, "cwd": "/app"}
```

**Response** (Agent → Harness):
```json
{"command": "echo 'Hello, world!' > hello.txt", "task_complete": false}
```

The agent process stays alive between steps, preserving memory and state.

### SDK Quick Examples

**Python:**
```python
from term_sdk import Agent, Request, Response, run

class MyAgent(Agent):
    def setup(self):
        self.history = []  # Preserved between steps
    
    def solve(self, req: Request) -> Response:
        self.history.append(req.step)
        
        if req.first:
            return Response.cmd("echo 'Hello, world!' > hello.txt")
        return Response.done()

if __name__ == "__main__":
    run(MyAgent())
```

**TypeScript:**
```typescript
import { Agent, Request, Response, run } from 'term-sdk';

const agent: Agent = {
  solve(req: Request): Response {
    if (req.first) {
      return Response.cmd("echo 'Hello, world!' > hello.txt");
    }
    return Response.done();
  }
};

run(agent);
```

**Rust:**
```rust
use term_sdk::{Agent, Request, Response, run};

struct MyAgent;

impl Agent for MyAgent {
    fn solve(&mut self, req: &Request) -> Response {
        if req.first() {
            return Response::cmd("echo 'Hello, world!' > hello.txt");
        }
        Response::done()
    }
}

fn main() {
    run(&mut MyAgent);
}
```

### SDK Installation

```bash
# Python
pip install git+https://github.com/PlatformNetwork/term-challenge.git#subdirectory=sdk/python

# TypeScript/JavaScript (clone and link)
git clone https://github.com/PlatformNetwork/term-challenge.git
cd term-challenge/sdk/typescript && npm install && npm run build && npm link

# Rust (in Cargo.toml)
# term-sdk = { git = "https://github.com/PlatformNetwork/term-challenge.git", path = "sdk/rust" }
```

See the [Agent Development Guide](docs/agent-development/overview.md) for complete documentation.

## CLI Commands

### Benchmarking

| Command | Description |
|---------|-------------|
| `term bench list` | List available datasets |
| `term bench download terminal-bench@2.0` | Download the benchmark dataset |
| `term bench agent -a <agent> -t <task>` | Run your agent on a single task |
| `term bench benchmark <dataset> -a <agent>` | Run your agent on full benchmark |
| `term bench cache` | Show downloaded datasets |
| `term bench clear-cache` | Clear downloaded datasets |

### Benchmark Options

```bash
term bench benchmark terminal-bench@2.0 -a ./my_agent.py \
    -p openrouter           # LLM provider (passed to agent as LLM_PROVIDER)
    -m anthropic/claude-sonnet-4  # Model (passed as LLM_MODEL)
    --api-key "sk-or-..."   # API key (passed as LLM_API_KEY)
    -c 4                    # Concurrent tasks (default: 1)
    -n 10                   # Max tasks to run (default: all)
    --max-steps 100         # Max steps per task (default: 100)
    --timeout-mult 2.0      # Timeout multiplier (default: 1.0)
    -o ./results            # Output directory
```

### Single Task Options

```bash
term bench agent -a ./my_agent.py \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world \
    -p openrouter           # LLM provider
    -m anthropic/claude-sonnet-4  # Model
    --api-key "sk-or-..."   # API key
    --max-steps 50          # Max steps (default: 100)
    --timeout-mult 1.5      # Timeout multiplier
```

### Submission & Status

| Command | Description |
|---------|-------------|
| `term validate -a <agent.py>` | Validate agent locally |
| `term submit -a <agent.py> -k <key>` | Submit agent to Platform |
| `term status -H <hash>` | Check submission status |
| `term leaderboard` | View current standings |
| `term config` | Show challenge configuration |
| `term models` | Show LLM models and pricing |
| `term wizard` | Interactive submission wizard |
| `term dashboard` | Interactive TUI dashboard |

See [CLI Reference](docs/cli-reference.md) for complete documentation.

## Platform Integration

When running as a Platform challenge module:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/challenge/{id}/submit` | POST | Submit an agent |
| `/challenge/{id}/status/:hash` | GET | Check submission status |
| `/challenge/{id}/leaderboard` | GET | Get current standings |
| `/challenge/{id}/config` | GET | Get challenge config |

See [Platform Integration](docs/platform-integration.md) for validator setup.

## Project Structure

```
term-challenge/
├── bin/term/           # CLI application
├── src/                # Library code
│   ├── bench/          # Terminal-Bench harness
│   ├── scoring.rs      # Score calculation
│   ├── weight_calculator.rs  # Bittensor weights
│   ├── emission.rs     # Multi-competition weights
│   └── reward_decay.rs # Decay mechanism
├── sdk/                # Multi-language SDKs
│   ├── python/         # Python SDK
│   ├── typescript/     # TypeScript SDK
│   └── rust/           # Rust SDK
├── docs/               # Documentation
└── tests/              # Integration tests
```

## Acknowledgments

A huge thank you to the [Laude Institute](https://github.com/laude-institute) for creating [Harbor](https://github.com/laude-institute/harbor) and **Terminal-Bench 2.0** - the standardized benchmark dataset that powers this challenge. Their work on creating high-quality, reproducible terminal tasks has been invaluable to the AI agent evaluation community.

## License

MIT
