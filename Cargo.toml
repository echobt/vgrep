[package]
name = "vgrep"
version = "0.1.0"
edition = "2021"
authors = ["Cortex Foundation"]
description = "Local semantic code search powered by llama.cpp"
license = "Apache-2.0"
repository = "https://github.com/CortexLM/vgrep"
homepage = "https://github.com/CortexLM/vgrep"
keywords = ["semantic-search", "grep", "llm", "embeddings", "code-search"]
categories = ["command-line-utilities", "development-tools", "text-processing"]
readme = "README.md"
rust-version = "1.75"

[[bin]]
name = "vgrep"
path = "src/main.rs"

[dependencies]
llama-cpp-2 = { git = "https://github.com/utilityai/llama-cpp-rs", branch = "main" }
hf-hub = "0.4"
clap = { version = "4.5", features = ["derive", "env"] }
tokio = { version = "1", features = ["full"] }
axum = "0.8"
tower-http = { version = "0.6", features = ["cors"] }
rusqlite = { version = "0.32", features = ["bundled"] }
ratatui = "0.29"
crossterm = "0.28"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
anyhow = "1"
thiserror = "2"
walkdir = "2"
ignore = "0.4"
notify = { version = "7", default-features = false, features = ["macos_kqueue"] }
dirs = "6"
indicatif = "0.17"
directories = "6"
chrono = { version = "0.4", features = ["serde"] }
sha2 = "0.10"
hex = "0.4"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
dialoguer = "0.12"
console = "0.15"
ctrlc = "3"

[dev-dependencies]
tempfile = "3"
assert_cmd = "2"
predicates = "3"

[features]
default = []
cuda = ["llama-cpp-2/cuda"]
metal = ["llama-cpp-2/metal"]
vulkan = ["llama-cpp-2/vulkan"]

[profile.release]
lto = true
codegen-units = 1
opt-level = 3
strip = true

[profile.dev]
opt-level = 1
