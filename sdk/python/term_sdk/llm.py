"""
LLM Client for Term Challenge agents.

Supports OpenRouter and Chutes providers with streaming.

Example:
    ```python
    from term_sdk import LLM, LLMError
    
    llm = LLM()
    
    # Regular call
    result = llm.ask("Hello", model="z-ai/glm-4.5")
    
    # Streaming - see response in real-time
    for chunk in llm.stream("Write a story", model="z-ai/glm-4.5"):
        print(chunk, end="", flush=True)
    
    # Error handling
    try:
        result = llm.ask("Question", model="z-ai/glm-4.5")
    except LLMError as e:
        print(f"Code: {e.code}")
        print(f"Message: {e.message}")
        print(f"Details: {e.details}")
    ```
"""

import os
import sys
import json
import time
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Callable, Iterator, Union
import httpx

from .types import Tool, FunctionCall


class LLMError(Exception):
    """
    Structured LLM error with JSON details.
    
    Attributes:
        code: Error code (e.g., "rate_limit", "invalid_model", "api_error")
        message: Human-readable error message
        details: Additional error details as dict
    
    Example:
        ```python
        try:
            result = llm.ask("Question", model="invalid-model")
        except LLMError as e:
            print(f"Error {e.code}: {e.message}")
            print(f"Details: {e.details}")
        ```
    """
    def __init__(self, code: str, message: str, details: Optional[Dict[str, Any]] = None):
        self.code = code
        self.message = message
        self.details = details or {}
        super().__init__(self.to_json())
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "error": {
                "code": self.code,
                "message": self.message,
                "details": self.details,
            }
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict())
    
    def __str__(self) -> str:
        return f"LLMError({self.code}): {self.message}"


class CostLimitExceeded(LLMError):
    """
    Fatal error: cost limit reached, agent should stop immediately.
    
    This exception is raised when the agent has exhausted its LLM budget.
    The agent runner will catch this and gracefully terminate the task.
    
    Attributes:
        limit: The cost limit in USD
        used: The amount used in USD
    """
    def __init__(self, message: str, limit: float = 0.0, used: float = 0.0):
        self.limit = limit
        self.used = used
        super().__init__(
            code="cost_limit_exceeded",
            message=message,
            details={"limit": limit, "used": used}
        )
    
    def __str__(self) -> str:
        return f"CostLimitExceeded: ${self.used:.4f} used of ${self.limit:.4f} limit"


@dataclass
class LLMResponse:
    """Response from LLM.
    
    Attributes:
        text: The response text content
        model: The model used
        tokens: Total tokens used
        cost: Cost in USD (after cache discount if applicable)
        latency_ms: Response latency in milliseconds
        function_calls: List of function/tool calls
        raw: Raw response data
        cached_tokens: Number of tokens read from cache (reduces cost)
        prompt_tokens: Number of input/prompt tokens
        completion_tokens: Number of output/completion tokens
    """
    text: str
    model: str
    tokens: int = 0
    cost: float = 0.0
    latency_ms: int = 0
    function_calls: List[FunctionCall] = field(default_factory=list)
    raw: Optional[Dict[str, Any]] = None
    # Cache info (OpenRouter with usage: {include: true})
    cached_tokens: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    
    def json(self) -> Optional[Dict]:
        """Parse response text as JSON."""
        try:
            text = self.text.strip()
            if "```" in text:
                import re
                match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
                if match:
                    text = match.group(1)
            start = text.find('{')
            end = text.rfind('}')
            if start >= 0 and end > start:
                return json.loads(text[start:end + 1])
        except:
            pass
        return None
    
    def has_function_calls(self) -> bool:
        return len(self.function_calls) > 0


def _log(msg: str):
    print(f"[llm] {msg}", file=sys.stderr)


# Provider configurations
PROVIDERS = {
    "openrouter": {
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "env_key": "OPENROUTER_API_KEY",
        "description": "OpenRouter - Multi-model gateway",
    },
    "chutes": {
        "url": "https://llm.chutes.ai/v1/chat/completions",
        "env_key": "CHUTES_API_KEY",
        "description": "Chutes - Fast inference",
    },
    "openai": {
        "url": "https://api.openai.com/v1/chat/completions",
        "env_key": "OPENAI_API_KEY",
        "description": "OpenAI - GPT models",
    },
    "anthropic": {
        "url": "https://api.anthropic.com/v1/messages",
        "env_key": "ANTHROPIC_API_KEY",
        "description": "Anthropic - Claude models",
        "is_anthropic": True,
    },
    "grok": {
        "url": "https://api.x.ai/v1/chat/completions",
        "env_key": "GROK_API_KEY",
        "description": "xAI - Grok models",
    },
}

# Model pricing per 1M tokens (input, output)
PRICING = {
    # OpenAI - Legacy models (chat/completions API)
    "gpt-4o": (2.5, 10.0),
    "gpt-4o-mini": (0.15, 0.6),
    "gpt-4-turbo": (10.0, 30.0),
    "gpt-4": (30.0, 60.0),
    "gpt-3.5-turbo": (0.5, 1.5),
    # OpenAI - GPT-4.1 series (responses API)
    "gpt-4.1": (2.0, 8.0),
    "gpt-4.1-mini": (0.4, 1.6),
    "gpt-4.1-nano": (0.1, 0.4),
    # OpenAI - GPT-5 series (responses API)
    "gpt-5": (1.25, 10.0),
    "gpt-5-mini": (0.25, 2.0),
    "gpt-5-nano": (0.05, 0.4),
    "gpt-5-pro": (15.0, 120.0),
    "gpt-5-codex": (1.25, 10.0),
    "gpt-5-chat": (1.25, 10.0),
    "gpt-5.1": (1.25, 10.0),
    "gpt-5.1-codex": (1.25, 10.0),
    "gpt-5.1-codex-mini": (0.25, 2.0),
    "gpt-5.1-codex-max": (1.25, 10.0),
    "gpt-5.1-chat": (1.25, 10.0),
    "gpt-5.2": (1.75, 14.0),
    "gpt-5.2-codex": (1.75, 14.0),
    "gpt-5.2-pro": (21.0, 168.0),
    "gpt-5.2-chat": (1.75, 14.0),
    # OpenAI - o-series reasoning models
    "o1": (15.0, 60.0),
    "o1-mini": (3.0, 12.0),
    "o1-pro": (150.0, 600.0),
    "o3": (2.0, 8.0),
    "o3-mini": (1.1, 4.4),
    "o3-pro": (20.0, 80.0),
    "o4-mini": (1.1, 4.4),
    # Anthropic
    "claude-3-opus": (15.0, 75.0),
    "claude-3-sonnet": (3.0, 15.0),
    "claude-3.5-sonnet": (3.0, 15.0),
    "claude-3-haiku": (0.25, 1.25),
    # Grok
    "grok-2": (2.0, 10.0),
    "grok-beta": (5.0, 15.0),
    # Open source
    "llama-3": (0.2, 0.2),
    "mixtral": (0.5, 0.5),
    "qwen": (0.2, 0.2),
    "glm-4": (0.25, 1.25),
}

# =============================================================================
# OpenAI Responses API Support (for GPT-4.1+, GPT-5.x models)
# =============================================================================
# These models use the new /v1/responses endpoint instead of /v1/chat/completions
# See: https://platform.openai.com/docs/api-reference/responses

OPENAI_RESPONSES_API_PREFIXES = [
    "gpt-4.1",  # GPT-4.1 series
    "gpt-5",    # All GPT-5.x models
]

def _is_openai_responses_model(model: str) -> bool:
    """
    Check if model uses OpenAI's /v1/responses API instead of /v1/chat/completions.
    
    GPT-4.1+ and GPT-5.x models use the new Responses API with different:
    - Request format: 'input' instead of 'messages', 'instructions' instead of system message
    - Response format: 'output' array instead of 'choices'
    - Conversation state: 'previous_response_id' for multi-turn
    """
    model_lower = model.lower()
    # Remove provider prefix if present (e.g., "openai/gpt-5")
    if "/" in model_lower:
        model_lower = model_lower.split("/")[-1]
    return any(model_lower.startswith(prefix) for prefix in OPENAI_RESPONSES_API_PREFIXES)

# Default models per provider
DEFAULT_MODELS = {
    "openrouter": "anthropic/claude-3.5-sonnet",
    "chutes": "deepseek-ai/DeepSeek-V3-0324",
    "openai": "gpt-4o-mini",
    "anthropic": "claude-3-5-sonnet-20241022",
    "grok": "grok-2-latest",
}


class LLM:
    """
    LLM client with streaming support for multiple providers.
    
    Providers:
        - openrouter: Multi-model gateway (default)
        - chutes: Fast inference
        - openai: GPT models
        - anthropic: Claude models
        - grok: xAI Grok models
    
    Args:
        provider: Provider name (default: "openrouter")
        default_model: Default model if not specified per-call
        temperature: Default sampling temperature
        max_tokens: Default maximum response tokens
        timeout: Request timeout in seconds (default: 300, or LLM_TIMEOUT env var)
    
    Prompt Caching:
        To use prompt caching with Anthropic models via OpenRouter, format your
        messages with cache_control breakpoints manually. The SDK will preserve
        the format and forward it to the provider.
        
        Example:
            ```python
            messages = [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": "Your long system prompt here...",
                            "cache_control": {"type": "ephemeral"}
                        }
                    ]
                },
                {"role": "user", "content": "Question"}
            ]
            response = llm.chat(messages, model="anthropic/claude-3.5-sonnet")
            ```
        
        See OpenRouter docs for details:
        https://openrouter.ai/docs/guides/best-practices/prompt-caching
    
    Environment Variables:
        - OPENROUTER_API_KEY: OpenRouter API key
        - CHUTES_API_KEY: Chutes API key
        - OPENAI_API_KEY: OpenAI API key
        - ANTHROPIC_API_KEY: Anthropic API key
        - GROK_API_KEY: xAI Grok API key
        - LLM_API_KEY: Override for any provider
    
    Example:
        ```python
        # OpenRouter (default)
        llm = LLM()
        result = llm.ask("Question", model="anthropic/claude-3.5-sonnet")
        
        # OpenAI
        llm = LLM(provider="openai")
        result = llm.ask("Question", model="gpt-4o-mini")
        
        # Anthropic
        llm = LLM(provider="anthropic")
        result = llm.ask("Question", model="claude-3-5-sonnet-20241022")
        
        # Grok
        llm = LLM(provider="grok")
        result = llm.ask("Question", model="grok-2-latest")
        
        # Streaming
        for chunk in llm.stream("Long question"):
            print(chunk, end="", flush=True)
        ```
    """
    
    def __init__(
        self,
        provider: str = "openrouter",
        default_model: Optional[str] = None,
        temperature: float = 0.3,
        max_tokens: int = 4096,
        timeout: Optional[int] = None,
    ):
        self.provider = provider
        self.temperature = temperature
        self.max_tokens = max_tokens
        # Timeout: user param > env var > default 300s
        self.timeout = timeout or int(os.environ.get("LLM_TIMEOUT", "300"))
        
        # Get provider config
        if provider not in PROVIDERS:
            raise LLMError(
                code="invalid_provider",
                message=f"Unknown provider: {provider}",
                details={"valid_providers": list(PROVIDERS.keys())}
            )
        
        config = PROVIDERS[provider]
        self._api_url = os.environ.get("LLM_API_URL", config["url"])
        self._api_key = os.environ.get("LLM_API_KEY") or os.environ.get(config["env_key"], "")
        self._is_anthropic = config.get("is_anthropic", False)
        
        # Platform bridge configuration (for term-challenge evaluation)
        # Flow: Agent -> Validator local proxy -> Central server
        self._agent_hash = os.environ.get("TERM_AGENT_HASH", "")
        self._validator_hotkey = os.environ.get("TERM_VALIDATOR_HOTKEY", "")
        self._platform_url = os.environ.get("TERM_PLATFORM_URL", "")
        # LLM_PROXY_URL is the local validator's proxy (e.g., http://localhost:8080)
        self._llm_proxy_url = os.environ.get("LLM_PROXY_URL", "")
        self._use_platform_bridge = bool(self._agent_hash and (self._llm_proxy_url or self._platform_url))
        
        if self._use_platform_bridge:
            if self._llm_proxy_url:
                # Use local validator proxy (preferred)
                self._api_url = f"{self._llm_proxy_url}/llm/proxy"
                _log(f"Using validator local proxy: {self._llm_proxy_url}")
            else:
                # Fallback: direct to central (for testing)
                self._api_url = f"{self._platform_url}/api/v1/llm/chat"
                _log(f"Using platform bridge direct: {self._platform_url}")
        
        # Set default model (user > provider default)
        self.default_model = default_model or DEFAULT_MODELS.get(provider)
        
        if not self._api_key and not self._use_platform_bridge:
            _log(f"Warning: LLM_API_KEY or {config['env_key']} not set")
        
        # Stats
        self.stats: Dict[str, Dict[str, Any]] = {}
        self.total_tokens = 0
        self.total_cost = 0.0
        self.request_count = 0
        
        # HTTP client
        self._client = httpx.Client(timeout=timeout)
        
        # Function handlers
        self._function_handlers: Dict[str, Callable] = {}
    
    def _get_model(self, model: Optional[str]) -> str:
        if model:
            return model
        if self.default_model:
            return self.default_model
        raise LLMError(
            code="no_model",
            message="No model specified",
            details={"hint": "Pass model= parameter or set default_model in LLM()"}
        )
    
    # =========================================================================
    # OpenAI Responses API Support (GPT-4.1+, GPT-5.x)
    # =========================================================================
    
    def _transform_messages_to_responses_input(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Tool]] = None
    ) -> tuple:
        """
        Transform chat messages format to OpenAI Responses API format.
        
        The Responses API uses:
        - 'input' array instead of 'messages'
        - 'instructions' for system messages
        - Different item types for user/assistant/tool messages
        
        Args:
            messages: Chat messages in OpenAI chat/completions format
            tools: Optional list of tools
            
        Returns:
            Tuple of (instructions, input_items, tools_list)
        """
        instructions = None
        input_items = []
        
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "system":
                # System messages become 'instructions' parameter
                if instructions:
                    instructions += "\n\n" + (content or "")
                else:
                    instructions = content
                    
            elif role == "user":
                # User messages become input items
                if isinstance(content, str):
                    input_items.append({
                        "type": "message",
                        "role": "user",
                        "content": [{"type": "input_text", "text": content}]
                    })
                elif isinstance(content, list):
                    # Multipart content (text + images)
                    converted_content = []
                    for part in content:
                        if isinstance(part, dict):
                            if part.get("type") == "text":
                                converted_content.append({
                                    "type": "input_text",
                                    "text": part.get("text", "")
                                })
                            elif part.get("type") == "image_url":
                                converted_content.append({
                                    "type": "input_image",
                                    "image_url": part.get("image_url", {}).get("url", "")
                                })
                    input_items.append({
                        "type": "message",
                        "role": "user",
                        "content": converted_content
                    })
                    
            elif role == "assistant":
                # Check for tool_calls in assistant message
                tool_calls = msg.get("tool_calls", [])
                if tool_calls:
                    # Add function call items for each tool call
                    for tc in tool_calls:
                        func = tc.get("function", {})
                        input_items.append({
                            "type": "function_call",
                            "id": tc.get("id", ""),
                            "call_id": tc.get("id", ""),
                            "name": func.get("name", ""),
                            "arguments": func.get("arguments", "{}")
                        })
                elif content:
                    # Regular assistant message
                    input_items.append({
                        "type": "message",
                        "role": "assistant",
                        "content": [{"type": "output_text", "text": content}]
                    })
                    
            elif role == "tool":
                # Tool results become function_call_output items
                input_items.append({
                    "type": "function_call_output",
                    "call_id": msg.get("tool_call_id", ""),
                    "output": content or ""
                })
        
        # Transform tools if provided
        tools_list = None
        if tools:
            tools_list = []
            for tool in tools:
                tools_list.append({
                    "type": "function",
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.parameters,
                    "strict": True  # Enable strict mode for better function calling
                })
        
        return instructions, input_items, tools_list
    
    def _parse_responses_api_response(
        self,
        data: Dict[str, Any],
        model: str,
        start: float
    ) -> LLMResponse:
        """
        Parse OpenAI /v1/responses API response format.
        
        Response structure:
        {
            "id": "resp_...",
            "status": "completed",
            "output": [
                {"type": "message", "content": [{"type": "output_text", "text": "..."}]},
                {"type": "function_call", "name": "...", "arguments": "...", "id": "..."}
            ],
            "usage": {"input_tokens": N, "output_tokens": M}
        }
        """
        # Extract text content and function calls from output
        text = ""
        function_calls = []
        
        for item in data.get("output", []):
            item_type = item.get("type", "")
            
            if item_type == "message":
                # Extract text from message content
                for content in item.get("content", []):
                    content_type = content.get("type", "")
                    if content_type == "output_text":
                        text += content.get("text", "")
                    elif content_type == "refusal":
                        text += f"[Refusal: {content.get('refusal', '')}]"
                        
            elif item_type == "function_call":
                # Extract function calls
                raw_args = item.get("arguments", "{}")
                try:
                    args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
                except json.JSONDecodeError:
                    args = {}
                
                function_calls.append(FunctionCall(
                    name=item.get("name", ""),
                    arguments=args if isinstance(args, dict) else {},
                    id=item.get("id") or item.get("call_id"),
                    raw_arguments=raw_args if isinstance(raw_args, str) else None
                ))
        
        # Extract usage information
        usage = data.get("usage", {})
        input_tokens = usage.get("input_tokens", 0)
        output_tokens = usage.get("output_tokens", 0)
        total_tokens = usage.get("total_tokens", input_tokens + output_tokens)
        
        # OpenAI Responses API doesn't return cost, so we use 0
        # Cost tracking is only available via OpenRouter which reports usage.cost
        cost = 0.0
        latency_ms = int((time.time() - start) * 1000)
        
        # Update stats
        self.total_tokens += total_tokens
        self.total_cost += cost
        self.request_count += 1
        self._update_model_stats(model, total_tokens, cost)
        
        _log(f"{model}: {total_tokens} tokens, ${cost:.4f}, {latency_ms}ms (Responses API)")
        
        return LLMResponse(
            text=text,
            model=data.get("model", model),
            tokens=total_tokens,
            cost=cost,
            latency_ms=latency_ms,
            function_calls=function_calls if function_calls else [],
            raw=data
        )
    
    def _chat_openai_responses(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        tools: Optional[List[Tool]],
        temperature: float,
        max_tokens: int,
        start: float,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> LLMResponse:
        """
        Handle OpenAI /v1/responses API calls for GPT-4.1+ and GPT-5.x models.
        
        This method transforms the standard chat format to the Responses API format
        and handles the response parsing.
        """
        # Transform messages to Responses API format
        instructions, input_items, tools_list = self._transform_messages_to_responses_input(messages, tools)
        
        # Build payload
        payload: Dict[str, Any] = {
            "model": model,
            "input": input_items,
            "temperature": temperature,
            "max_output_tokens": max_tokens,
            "store": False,  # Don't store responses on OpenAI's side
        }
        
        if instructions:
            payload["instructions"] = instructions
        
        if tools_list:
            payload["tools"] = tools_list
            payload["tool_choice"] = "auto"
        
        # Merge extra_body if provided
        if extra_body:
            for key, value in extra_body.items():
                if key not in payload:  # Don't override core params
                    # Handle max_completion_tokens -> max_output_tokens conversion
                    # The Responses API uses max_output_tokens, not max_completion_tokens
                    if key == "max_completion_tokens":
                        payload["max_output_tokens"] = value
                    else:
                        payload[key] = value
        
        # Use the Responses API endpoint
        url = "https://api.openai.com/v1/responses"
        
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }
        
        try:
            response = self._client.post(url, headers=headers, json=payload)
        except httpx.RequestError as e:
            raise LLMError(
                code="request_error",
                message=f"OpenAI Responses API request failed: {e}",
                details={"url": url, "model": model}
            )
        
        if not response.is_success:
            self._handle_api_error(response, model)
        
        data = response.json()
        
        # Check for API-level errors
        if data.get("status") == "failed":
            error = data.get("error", {})
            raise LLMError(
                code=error.get("code", "api_error"),
                message=error.get("message", "Unknown error"),
                details={"raw": data}
            )
        
        return self._parse_responses_api_response(data, model, start)
    
    def _stream_openai_responses(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        tools: Optional[List[Tool]],
        temperature: float,
        max_tokens: int,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> Iterator[str]:
        """
        Stream from OpenAI /v1/responses API for GPT-4.1+ and GPT-5.x models.
        
        The Responses API streaming uses Server-Sent Events (SSE) with events like:
        - response.output_item.added
        - response.output_text.delta
        - response.function_call_arguments.delta
        - response.completed
        """
        # Transform messages to Responses API format
        instructions, input_items, tools_list = self._transform_messages_to_responses_input(messages, tools)
        
        # Build payload with streaming enabled
        payload: Dict[str, Any] = {
            "model": model,
            "input": input_items,
            "temperature": temperature,
            "max_output_tokens": max_tokens,
            "store": False,
            "stream": True,  # Enable streaming
        }
        
        if instructions:
            payload["instructions"] = instructions
        
        if tools_list:
            payload["tools"] = tools_list
            payload["tool_choice"] = "auto"
        
        if extra_body:
            for key, value in extra_body.items():
                if key not in payload:
                    # Handle max_completion_tokens -> max_output_tokens conversion
                    # The Responses API uses max_output_tokens, not max_completion_tokens
                    if key == "max_completion_tokens":
                        payload["max_output_tokens"] = value
                    else:
                        payload[key] = value
        
        url = "https://api.openai.com/v1/responses"
        
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
        }
        
        try:
            with self._client.stream("POST", url, headers=headers, json=payload) as response:
                if not response.is_success:
                    # Read error body
                    error_text = ""
                    for chunk in response.iter_text():
                        error_text += chunk
                    raise LLMError(
                        code="stream_error",
                        message=f"OpenAI Responses API stream failed: {response.status_code}",
                        details={"error": error_text[:500], "model": model}
                    )
                
                for line in response.iter_lines():
                    if not line or not line.startswith("data: "):
                        continue
                    
                    data_str = line[6:]  # Remove "data: " prefix
                    if data_str == "[DONE]":
                        break
                    
                    try:
                        event = json.loads(data_str)
                        event_type = event.get("type", "")
                        
                        # Handle text deltas
                        if event_type == "response.output_text.delta":
                            delta = event.get("delta", "")
                            if delta:
                                yield delta
                                
                        # Could also handle function call deltas if needed
                        # elif event_type == "response.function_call_arguments.delta":
                        #     ...
                        
                    except json.JSONDecodeError:
                        continue
                        
        except httpx.RequestError as e:
            raise LLMError(
                code="stream_error",
                message=f"OpenAI Responses API stream request failed: {e}",
                details={"url": url, "model": model}
            )
    
    def _stream_openai_responses_full(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        tools: Optional[List[Tool]],
        temperature: float,
        max_tokens: int,
        on_chunk: Optional[Callable[[str], bool]] = None,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> LLMResponse:
        """
        Stream from OpenAI Responses API and collect full response with function calls.
        
        This version collects all data including function calls from the stream.
        """
        start = time.time()
        full_text = ""
        function_calls: List[FunctionCall] = []
        
        # Track function call building state
        current_function_call: Optional[Dict[str, Any]] = None
        function_arguments_buffer = ""
        
        # Transform messages
        instructions, input_items, tools_list = self._transform_messages_to_responses_input(messages, tools)
        
        payload: Dict[str, Any] = {
            "model": model,
            "input": input_items,
            "temperature": temperature,
            "max_output_tokens": max_tokens,
            "store": False,
            "stream": True,
        }
        
        if instructions:
            payload["instructions"] = instructions
        if tools_list:
            payload["tools"] = tools_list
            payload["tool_choice"] = "auto"
        if extra_body:
            for key, value in extra_body.items():
                if key not in payload:
                    # Handle max_completion_tokens -> max_output_tokens conversion
                    # The Responses API uses max_output_tokens, not max_completion_tokens
                    if key == "max_completion_tokens":
                        payload["max_output_tokens"] = value
                    else:
                        payload[key] = value
        
        url = "https://api.openai.com/v1/responses"
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
        }
        
        input_tokens = 0
        output_tokens = 0
        response_model = model
        
        try:
            with self._client.stream("POST", url, headers=headers, json=payload) as response:
                if not response.is_success:
                    error_text = ""
                    for chunk in response.iter_text():
                        error_text += chunk
                    raise LLMError(
                        code="stream_error",
                        message=f"Stream failed: {response.status_code}",
                        details={"error": error_text[:500]}
                    )
                
                for line in response.iter_lines():
                    if not line or not line.startswith("data: "):
                        continue
                    
                    data_str = line[6:]
                    if data_str == "[DONE]":
                        break
                    
                    try:
                        event = json.loads(data_str)
                        event_type = event.get("type", "")
                        
                        # Text delta
                        if event_type == "response.output_text.delta":
                            delta = event.get("delta", "")
                            if delta:
                                full_text += delta
                                if on_chunk and not on_chunk(delta):
                                    break
                        
                        # Function call started
                        elif event_type == "response.output_item.added":
                            item = event.get("item", {})
                            if item.get("type") == "function_call":
                                current_function_call = {
                                    "name": item.get("name", ""),
                                    "id": item.get("id") or item.get("call_id"),
                                    "arguments": ""
                                }
                                function_arguments_buffer = ""
                        
                        # Function call arguments delta
                        elif event_type == "response.function_call_arguments.delta":
                            if current_function_call:
                                delta = event.get("delta", "")
                                function_arguments_buffer += delta
                        
                        # Function call completed
                        elif event_type == "response.output_item.done":
                            item = event.get("item", {})
                            if item.get("type") == "function_call" and current_function_call:
                                raw_args = item.get("arguments", function_arguments_buffer)
                                try:
                                    args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
                                except json.JSONDecodeError:
                                    args = {}
                                
                                function_calls.append(FunctionCall(
                                    name=current_function_call.get("name", item.get("name", "")),
                                    arguments=args if isinstance(args, dict) else {},
                                    id=current_function_call.get("id") or item.get("id"),
                                    raw_arguments=raw_args if isinstance(raw_args, str) else None
                                ))
                                current_function_call = None
                                function_arguments_buffer = ""
                        
                        # Response completed - get usage
                        elif event_type == "response.completed":
                            resp = event.get("response", {})
                            usage = resp.get("usage", {})
                            input_tokens = usage.get("input_tokens", 0)
                            output_tokens = usage.get("output_tokens", 0)
                            response_model = resp.get("model", model)
                        
                    except json.JSONDecodeError:
                        continue
                        
        except httpx.RequestError as e:
            raise LLMError(
                code="stream_error",
                message=f"Stream request failed: {e}",
                details={"url": url}
            )
        
        # Calculate stats
        total_tokens = input_tokens + output_tokens
        if total_tokens == 0:
            # Estimate if not provided
            total_tokens = len(full_text) // 4
        
        # OpenAI Responses API streaming doesn't return cost, so use 0
        cost = 0.0
        latency_ms = int((time.time() - start) * 1000)
        
        self.total_tokens += total_tokens
        self.total_cost += cost
        self.request_count += 1
        self._update_model_stats(response_model, total_tokens, cost)
        
        _log(f"{response_model}: {total_tokens} tokens, ${cost:.4f}, {latency_ms}ms (Responses API stream)")
        
        return LLMResponse(
            text=full_text,
            model=response_model,
            tokens=total_tokens,
            cost=cost,
            latency_ms=latency_ms,
            function_calls=function_calls,
            raw=None
        )
    
    # =========================================================================
    # Public API
    # =========================================================================
    
    def register_function(self, name: str, handler: Callable):
        """Register a function handler for function calling."""
        self._function_handlers[name] = handler
    
    def ask(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        tools: Optional[List[Tool]] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
        request_builder: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
        response_parser: Optional[Callable[[Dict[str, Any]], "LLMResponse"]] = None,
    ) -> LLMResponse:
        """Ask a question (non-streaming).
        
        Args:
            prompt: User prompt
            model: Model to use
            system: System prompt
            tools: Optional tools for function calling
            temperature: Sampling temperature
            max_tokens: Max tokens in response
            extra_body: Extra parameters to merge into request body
            request_builder: Optional callback to customize request payload
            response_parser: Optional callback to parse custom response format
        """
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        return self.chat(messages, model=model, tools=tools, 
                        temperature=temperature, max_tokens=max_tokens,
                        extra_body=extra_body, request_builder=request_builder,
                        response_parser=response_parser)
    
    def stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> Iterator[str]:
        """
        Stream response chunks.
        
        Yields text chunks as they arrive.
        
        Args:
            prompt: User prompt
            model: Model to use
            system: System prompt
            temperature: Sampling temperature
            max_tokens: Max tokens in response
            extra_body: Extra parameters to merge into request body
        
        Example:
            ```python
            for chunk in llm.stream("Tell me a story", model="z-ai/glm-4.5"):
                print(chunk, end="", flush=True)
            ```
        """
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        yield from self.chat_stream(messages, model=model,
                                    temperature=temperature, max_tokens=max_tokens,
                                    extra_body=extra_body)
    
    def ask_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        on_chunk: Optional[Callable[[str], bool]] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> LLMResponse:
        """
        Stream with callback, return full response.
        
        Args:
            prompt: User prompt
            model: Model to use
            system: System prompt
            on_chunk: Callback for each chunk. Return False to stop.
            temperature: Sampling temperature
            max_tokens: Max tokens
            extra_body: Extra parameters to merge into request body
        
        Returns:
            LLMResponse with full text
        
        Example:
            ```python
            def check(chunk):
                print(chunk, end="")
                return "ERROR" not in chunk  # Stop on error
            
            result = llm.ask_stream("Solve", model="z-ai/glm-4.5", on_chunk=check)
            ```
        """
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        return self.chat_stream_full(messages, model=model, on_chunk=on_chunk,
                                     temperature=temperature, max_tokens=max_tokens,
                                     extra_body=extra_body)
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        tools: Optional[List[Tool]] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
        request_builder: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,
        response_parser: Optional[Callable[[Dict[str, Any]], "LLMResponse"]] = None,
    ) -> LLMResponse:
        """Chat (non-streaming).
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            model: Model name (uses default if not specified)
            tools: Optional list of tools for function calling
            temperature: Sampling temperature (optional - if not set, provider uses its default)
            max_tokens: Max tokens in response
            extra_body: Extra parameters to merge into request body
                       (e.g., {"thinking": {"type": "enabled"}, "top_p": 0.9})
            request_builder: Optional callback to customize the request payload.
                            Receives base payload dict, returns modified payload.
                            Example: lambda p: {**p, "custom_field": "value"}
            response_parser: Optional callback to parse custom response format.
                            Receives raw response dict, returns LLMResponse.
        """
        model = self._get_model(model)
        # Temperature is optional - only include if explicitly set by user
        temp = temperature  # None means let provider use its default
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        
        start = time.time()
        
        # Handle Anthropic's different API format
        if self._is_anthropic:
            return self._chat_anthropic(messages, model, tools, temp, tokens, start)
        
        # Handle OpenAI Responses API for GPT-4.1+ and GPT-5.x models (direct API only)
        # OpenRouter handles the transformation itself, so only for direct OpenAI provider
        if self.provider == "openai" and _is_openai_responses_model(model) and not self._use_platform_bridge:
            return self._chat_openai_responses(messages, model, tools, temp, tokens, start, extra_body)
        
        # Build payload - different format for platform bridge vs direct API
        if self._use_platform_bridge:
            # Build extra_params: merge extra_body with tools if present
            merged_extra_params: Dict[str, Any] = {}
            if extra_body:
                merged_extra_params.update(extra_body)
            if tools:
                merged_extra_params["tools"] = [t.to_dict() for t in tools]
                merged_extra_params["tool_choice"] = "auto"
            
            # Platform bridge format
            payload: Dict[str, Any] = {
                "agent_hash": self._agent_hash,
                "messages": messages,
                "model": model,
                "max_tokens": tokens,
                "task_id": os.environ.get("TERM_TASK_ID"),
                "extra_params": merged_extra_params if merged_extra_params else None,
            }
            # Only include temperature if explicitly set
            if temp is not None:
                payload["temperature"] = temp
            
            # Apply request_builder if provided
            if request_builder:
                payload = request_builder(payload)
            
            headers = {
                "Content-Type": "application/json",
            }
            if self._api_key:
                headers["Authorization"] = f"Bearer {self._api_key}"
        else:
            # Standard OpenAI-compatible format
            payload: Dict[str, Any] = {
                "model": model,
                "messages": messages,
                "stream": False,
            }
            
            # Only include temperature if explicitly set
            if temp is not None:
                payload["temperature"] = temp
            
            # Use max_completion_tokens if provided in extra_body, otherwise use max_tokens
            # This allows users to choose which parameter to use for different models
            # (o-series models require max_completion_tokens, others use max_tokens)
            if extra_body and "max_completion_tokens" in extra_body:
                # User explicitly set max_completion_tokens, don't add max_tokens
                pass
            else:
                payload["max_tokens"] = tokens
            
            if tools:
                payload["tools"] = [t.to_dict() for t in tools]
                payload["tool_choice"] = "auto"
            
            # Add extra_body parameters (e.g., thinking, top_p, max_completion_tokens, etc.)
            if extra_body:
                payload.update(extra_body)
            
            # Apply request_builder if provided (for full custom control)
            if request_builder:
                payload = request_builder(payload)
            
            headers = {
                "Authorization": f"Bearer {self._api_key}",
                "Content-Type": "application/json",
            }
        
        # Make HTTP request with graceful error handling for proxy mode
        try:
            response = self._client.post(self._api_url, headers=headers, json=payload)
        except httpx.RequestError as e:
            if self._use_platform_bridge:
                # Graceful fallback - return error to agent, don't crash
                raise LLMError(
                    code="proxy_unavailable",
                    message=f"LLM proxy request failed: {e}",
                    details={"proxy_url": self._api_url}
                )
            raise
        
        if not response.is_success:
            if self._use_platform_bridge:
                # Parse proxy error response gracefully
                try:
                    data = response.json()
                    error_msg = data.get("error", response.text)
                except Exception:
                    error_msg = response.text
                
                # Check for cost_limit_exceeded - this is fatal, agent must stop
                error_str = str(error_msg).lower()
                if "cost_limit_exceeded" in error_str or "cost limit" in error_str:
                    # Try to parse limit and used from error message
                    # Format: "cost_limit_exceeded: $X.XX used of $Y.YY limit"
                    import re
                    limit, used = 0.0, 0.0
                    match = re.search(r'\$?([\d.]+)\s*used\s*of\s*\$?([\d.]+)', error_str)
                    if match:
                        used = float(match.group(1))
                        limit = float(match.group(2))
                    raise CostLimitExceeded(
                        message=str(error_msg),
                        limit=limit,
                        used=used
                    )
                
                # Enhanced error message for proxy errors
                raw_text = response.text[:500] if response.text else "empty response"
                raise LLMError(
                    code="proxy_error",
                    message=f"Invalid response from central server: {error_msg}",
                    details={
                        "status_code": response.status_code,
                        "proxy_url": self._api_url,
                        "raw_response": raw_text,
                        "hint": "Check if central server is running and accessible"
                    }
                )
            self._handle_api_error(response, model)
        
        data = response.json()
        
        # Use custom response parser if provided
        if response_parser:
            return response_parser(data)
        
        # Handle platform bridge response format
        if self._use_platform_bridge:
            return self._parse_platform_response(data, model, start)
        
        return self._parse_response(data, model, start)
    
    def raw_request(
        self,
        url: str,
        body: Dict[str, Any],
        headers: Optional[Dict[str, str]] = None,
        method: str = "POST",
        response_parser: Optional[Callable[[Dict[str, Any]], "LLMResponse"]] = None,
    ) -> Union[Dict[str, Any], "LLMResponse"]:
        """Make a completely custom LLM request without any validation.
        
        This method gives you full control over the request - no automatic
        formatting, no validation, no parameter merging. Use this when you
        need to call a custom API endpoint with a specific body format.
        
        For platform bridge mode, the request is forwarded through the proxy
        with the raw body in extra_params.
        
        Args:
            url: Full URL to send the request to (or None to use platform bridge)
            body: Complete request body as dict - sent as-is
            headers: Optional custom headers (defaults to JSON content-type + auth)
            method: HTTP method (default: POST)
            response_parser: Optional callback to parse response into LLMResponse.
                            If not provided, returns raw response dict.
        
        Returns:
            Raw response dict, or LLMResponse if response_parser is provided
        
        Example:
            ```python
            # Custom endpoint with custom body
            response = llm.raw_request(
                url="https://custom-llm.example.com/v1/generate",
                body={
                    "prompt": "Hello",
                    "max_new_tokens": 100,
                    "custom_param": "value"
                }
            )
            
            # With custom response parser
            def parse_custom(data):
                return LLMResponse(
                    text=data.get("generated_text", ""),
                    model="custom",
                    tokens=data.get("token_count", 0),
                    cost=0.0
                )
            response = llm.raw_request(url, body, response_parser=parse_custom)
            
            # Through platform bridge (for evaluation)
            response = llm.raw_request(
                url=None,  # Use bridge
                body={"model": "custom", "prompt": "Hello", "custom_format": True}
            )
            ```
        """
        # Build headers
        if headers is None:
            headers = {
                "Content-Type": "application/json",
            }
            if self._api_key:
                headers["Authorization"] = f"Bearer {self._api_key}"
        
        # If using platform bridge and no URL specified, route through proxy
        if url is None or (self._use_platform_bridge and not url.startswith("http")):
            # Forward raw body through platform bridge
            proxy_payload = {
                "agent_hash": self._agent_hash,
                "messages": [],  # Empty - using raw body
                "model": body.get("model"),
                "max_tokens": body.get("max_tokens"),
                "temperature": body.get("temperature"),
                "task_id": os.environ.get("TERM_TASK_ID"),
                "extra_params": body,  # Full raw body as extra_params
                "raw_request": True,  # Signal to server to use extra_params as body
            }
            
            try:
                response = self._client.post(self._api_url, headers=headers, json=proxy_payload)
            except httpx.RequestError as e:
                raise LLMError(
                    code="proxy_unavailable",
                    message=f"Raw request through proxy failed: {e}",
                    details={"proxy_url": self._api_url}
                )
        else:
            # Direct request to specified URL
            try:
                if method.upper() == "POST":
                    response = self._client.post(url, headers=headers, json=body)
                elif method.upper() == "GET":
                    response = self._client.get(url, headers=headers, params=body)
                else:
                    response = self._client.request(method, url, headers=headers, json=body)
            except httpx.RequestError as e:
                raise LLMError(
                    code="request_failed",
                    message=f"Raw request failed: {e}",
                    details={"url": url}
                )
        
        if not response.is_success:
            raise LLMError(
                code="request_error",
                message=f"Request failed with status {response.status_code}",
                details={
                    "status_code": response.status_code,
                    "response": response.text[:500] if response.text else "empty"
                }
            )
        
        data = response.json()
        
        # Use custom response parser if provided
        if response_parser:
            return response_parser(data)
        
        return data
    
    def raw_chat(
        self,
        body: Dict[str, Any],
        model: Optional[str] = None,
    ) -> LLMResponse:
        """Send a raw chat request with full control over the body.
        
        This method sends the body exactly as provided to the LLM API,
        without any transformation. Useful when you need precise control
        over message format, tool_calls, etc.
        
        The body is sent through the platform bridge in evaluation mode,
        or directly to the provider API otherwise.
        
        Args:
            body: Complete request body dict. Should include:
                  - model: Model name
                  - messages: List of message dicts
                  - Any other parameters (tools, temperature, etc.)
            model: Optional model override (if not in body)
        
        Returns:
            LLMResponse with parsed content and function_calls
        
        Example:
            ```python
            response = llm.raw_chat({
                "model": "anthropic/claude-3.5-sonnet",
                "messages": [
                    {"role": "system", "content": "You are helpful."},
                    {"role": "user", "content": "Hello"},
                    {"role": "assistant", "content": "", "tool_calls": [...]},
                    {"role": "tool", "tool_call_id": "...", "content": "..."}
                ],
                "tools": [...],
                "temperature": 0.7,
                "max_tokens": 4096
            })
            ```
        """
        start = time.time()
        actual_model = model or body.get("model") or self.default_model
        
        if self._use_platform_bridge:
            # Send through platform bridge with raw_request flag
            # This tells the proxy to pass the body directly to OpenRouter
            proxy_payload = {
                "agent_hash": self._agent_hash,
                "messages": body.get("messages", []),
                "model": actual_model,
                "max_tokens": body.get("max_tokens", self.max_tokens),
                "temperature": body.get("temperature", self.temperature),
                "task_id": os.environ.get("TERM_TASK_ID"),
                "raw_request": True,  # Enable raw mode for full control
                "extra_params": {
                    k: v for k, v in body.items() 
                    if k not in ("messages", "model", "max_tokens", "temperature")
                },
            }
            
            headers = {"Content-Type": "application/json"}
            if self._api_key:
                headers["Authorization"] = f"Bearer {self._api_key}"
            
            try:
                response = self._client.post(self._api_url, headers=headers, json=proxy_payload)
            except httpx.RequestError as e:
                raise LLMError(
                    code="proxy_unavailable",
                    message=f"Raw chat through proxy failed: {e}",
                    details={"proxy_url": self._api_url}
                )
            
            if not response.is_success:
                try:
                    data = response.json()
                    error_msg = data.get("error", response.text)
                except Exception:
                    error_msg = response.text
                
                error_str = str(error_msg).lower()
                if "cost_limit_exceeded" in error_str or "cost limit" in error_str:
                    import re
                    limit, used = 0.0, 0.0
                    match = re.search(r'\$?([\d.]+)\s*used\s*of\s*\$?([\d.]+)', error_str)
                    if match:
                        used = float(match.group(1))
                        limit = float(match.group(2))
                    raise CostLimitExceeded(message=str(error_msg), limit=limit, used=used)
                
                raise LLMError(
                    code="proxy_error",
                    message=f"Raw chat error: {error_msg}",
                    details={"status_code": response.status_code}
                )
            
            data = response.json()
            return self._parse_platform_response(data, actual_model, start)
        
        else:
            # Direct API call with raw body
            headers = {
                "Authorization": f"Bearer {self._api_key}",
                "Content-Type": "application/json",
            }
            
            # Ensure model is in body
            if "model" not in body:
                body["model"] = actual_model
            
            try:
                response = self._client.post(self._api_url, headers=headers, json=body)
            except httpx.RequestError as e:
                raise LLMError(
                    code="request_failed",
                    message=f"Raw chat request failed: {e}",
                    details={"url": self._api_url}
                )
            
            if not response.is_success:
                self._handle_api_error(response, actual_model)
            
            data = response.json()
            return self._parse_response(data, actual_model, start)
    
    def _chat_anthropic(
        self,
        messages: List[Dict[str, str]],
        model: str,
        tools: Optional[List[Tool]],
        temperature: float,
        max_tokens: int,
        start: float,
    ) -> LLMResponse:
        """Handle Anthropic's different API format."""
        # Extract system message if present
        system_content = None
        filtered_messages = []
        for msg in messages:
            if msg.get("role") == "system":
                system_content = msg.get("content", "")
            else:
                filtered_messages.append(msg)
        
        payload: Dict[str, Any] = {
            "model": model,
            "messages": filtered_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        
        if system_content:
            payload["system"] = system_content
        
        if tools:
            payload["tools"] = [self._convert_tool_to_anthropic(t) for t in tools]
        
        headers = {
            "x-api-key": self._api_key,
            "anthropic-version": "2023-06-01",
            "Content-Type": "application/json",
        }
        
        response = self._client.post(self._api_url, headers=headers, json=payload)
        
        if not response.is_success:
            self._handle_api_error(response, model)
        
        data = response.json()
        
        return self._parse_anthropic_response(data, model, start)
    
    def _convert_tool_to_anthropic(self, tool: Tool) -> Dict[str, Any]:
        """Convert OpenAI tool format to Anthropic format."""
        return {
            "name": tool.name,
            "description": tool.description,
            "input_schema": tool.parameters,
        }
    
    def _parse_anthropic_response(self, data: Dict, model: str, start: float) -> LLMResponse:
        """Parse Anthropic API response."""
        content = data.get("content", [])
        text = ""
        function_calls = []
        
        for block in content:
            if block.get("type") == "text":
                text += block.get("text", "")
            elif block.get("type") == "tool_use":
                function_calls.append(FunctionCall(
                    name=block.get("name", ""),
                    arguments=block.get("input", {}),
                    id=block.get("id"),
                ))
        
        usage = data.get("usage", {})
        prompt_tokens = usage.get("input_tokens", 0)
        completion_tokens = usage.get("output_tokens", 0)
        total_tokens = prompt_tokens + completion_tokens
        
        # Anthropic doesn't return cost in their API, so use 0
        # Cost tracking is only available via OpenRouter which reports usage.cost
        cost = 0.0
        latency_ms = int((time.time() - start) * 1000)
        
        self.total_tokens += total_tokens
        self.total_cost += cost
        self.request_count += 1
        self._update_model_stats(model, total_tokens, cost)
        
        _log(f"{model}: {total_tokens} tokens, ${cost:.4f}, {latency_ms}ms")
        
        return LLMResponse(
            text=text,
            model=model,
            tokens=total_tokens,
            cost=cost,
            latency_ms=latency_ms,
            function_calls=function_calls,
            raw=data,
        )
    
    def chat_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> Iterator[str]:
        """Stream chat response chunks.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            model: Model to use
            temperature: Sampling temperature (optional - if not set, provider uses its default)
            max_tokens: Max tokens in response
            extra_body: Extra parameters to merge into request body
        """
        model = self._get_model(model)
        # Temperature is optional - only include if explicitly set by user
        temp = temperature  # None means let provider use its default
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        
        # Platform bridge streaming mode
        if self._use_platform_bridge:
            yield from self._chat_stream_proxy(messages, model, temp, tokens, extra_body)
            return
        
        # OpenAI Responses API streaming for GPT-4.1+ and GPT-5.x models
        if self.provider == "openai" and _is_openai_responses_model(model):
            yield from self._stream_openai_responses(messages, model, None, temp, tokens, extra_body)
            return
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
        }
        
        # Only include temperature if explicitly set
        if temp is not None:
            payload["temperature"] = temp
        
        # Use max_completion_tokens if provided in extra_body, otherwise use max_tokens
        if extra_body and "max_completion_tokens" in extra_body:
            pass  # User explicitly set max_completion_tokens
        else:
            payload["max_tokens"] = tokens
        
        # Add extra_body parameters
        if extra_body:
            payload.update(extra_body)
        
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }
        
        with self._client.stream("POST", self._api_url, headers=headers, json=payload) as response:
            if not response.is_success:
                self._handle_api_error(response, model)
            for line in response.iter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    try:
                        chunk = json.loads(data)
                        delta = chunk.get("choices", [{}])[0].get("delta", {})
                        content = delta.get("content", "")
                        if content:
                            yield content
                    except json.JSONDecodeError:
                        pass
    
    def _chat_stream_proxy(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> Iterator[str]:
        """Stream chat response through platform proxy."""
        # Use streaming proxy endpoint
        stream_url = self._api_url + "/stream"  # /llm/proxy/stream
        
        payload = {
            "agent_hash": self._agent_hash,
            "messages": messages,
            "model": model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "task_id": os.environ.get("TERM_TASK_ID"),
            "extra_params": extra_body,  # Forward custom params through bridge
        }
        
        headers = {
            "Content-Type": "application/json",
        }
        
        try:
            with self._client.stream("POST", stream_url, headers=headers, json=payload) as response:
                if not response.is_success:
                    try:
                        error_text = response.read().decode()
                        error_data = json.loads(error_text)
                        error_msg = error_data.get("error", error_text)
                    except Exception:
                        error_msg = f"HTTP {response.status_code}"
                    
                    # Check for cost_limit_exceeded - this is fatal
                    error_str = str(error_msg).lower()
                    if "cost_limit_exceeded" in error_str or "cost limit" in error_str:
                        import re
                        limit, used = 0.0, 0.0
                        match = re.search(r'\$?([\d.]+)\s*used\s*of\s*\$?([\d.]+)', error_str)
                        if match:
                            used = float(match.group(1))
                            limit = float(match.group(2))
                        raise CostLimitExceeded(
                            message=str(error_msg),
                            limit=limit,
                            used=used
                        )
                    
                    # Enhanced error message for proxy errors
                    raw_text = response.text[:500] if response.text else "empty response"
                    raise LLMError(
                        code="proxy_error",
                        message=f"Invalid response from central server: {error_msg}",
                        details={
                            "status_code": response.status_code,
                            "proxy_url": stream_url,
                            "raw_response": raw_text,
                            "hint": "Check if central server is running and accessible"
                        }
                    )
                
                for line in response.iter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            pass
        except httpx.RequestError as e:
            raise LLMError(
                code="proxy_unavailable",
                message=f"LLM proxy stream request failed: {e}",
                details={"proxy_url": stream_url}
            )
    
    def _chat_via_stream_proxy(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        tools: Optional[List[Tool]],
        temperature: Optional[float],
        max_tokens: int,
        start: float,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> LLMResponse:
        """Chat via streaming proxy to avoid timeout issues.
        
        Uses streaming internally but returns a complete LLMResponse.
        This prevents connection timeouts for long-running model calls.
        """
        # Build extra_params: merge extra_body with tools if present
        merged_extra_params: Dict[str, Any] = {}
        if extra_body:
            merged_extra_params.update(extra_body)
        if tools:
            merged_extra_params["tools"] = [t.to_dict() for t in tools]
            merged_extra_params["tool_choice"] = "auto"
        
        stream_url = self._api_url + "/stream"
        
        payload = {
            "agent_hash": self._agent_hash,
            "messages": messages,
            "model": model,
            "max_tokens": max_tokens,
            "task_id": os.environ.get("TERM_TASK_ID"),
            "extra_params": merged_extra_params if merged_extra_params else None,
        }
        if temperature is not None:
            payload["temperature"] = temperature
        
        headers = {"Content-Type": "application/json"}
        
        full_text = ""
        tool_calls: List[Dict[str, Any]] = []
        finish_reason = None
        
        try:
            with self._client.stream("POST", stream_url, headers=headers, json=payload) as response:
                if not response.is_success:
                    try:
                        error_text = response.read().decode()
                        error_data = json.loads(error_text)
                        error_msg = error_data.get("error", error_text)
                    except Exception:
                        error_msg = f"HTTP {response.status_code}"
                    
                    # Check for cost_limit_exceeded
                    error_str = str(error_msg).lower()
                    if "cost_limit_exceeded" in error_str or "cost limit" in error_str:
                        import re
                        limit, used = 0.0, 0.0
                        match = re.search(r'\$?([\d.]+)\s*used\s*of\s*\$?([\d.]+)', error_str)
                        if match:
                            used = float(match.group(1))
                            limit = float(match.group(2))
                        raise CostLimitExceeded(message=str(error_msg), limit=limit, used=used)
                    
                    raise LLMError(
                        code="proxy_error",
                        message=f"Streaming proxy error: {error_msg}",
                        details={"status_code": response.status_code, "proxy_url": stream_url}
                    )
                
                # Accumulate tool_calls state for streaming
                current_tool_calls: Dict[int, Dict[str, Any]] = {}
                
                for line in response.iter_lines():
                    # Skip SSE comments (e.g., ": OPENROUTER PROCESSING")
                    if line.startswith(":"):
                        continue
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            
                            # Check for mid-stream errors (OpenRouter format)
                            if "error" in chunk:
                                error_info = chunk["error"]
                                error_msg = error_info.get("message", str(error_info))
                                raise LLMError(
                                    code=error_info.get("code", "stream_error"),
                                    message=f"Mid-stream error: {error_msg}",
                                    details={"chunk": chunk}
                                )
                            
                            choice = chunk.get("choices", [{}])[0]
                            delta = choice.get("delta", {})
                            
                            # Check for error finish reason
                            if choice.get("finish_reason") == "error":
                                raise LLMError(
                                    code="stream_error",
                                    message="Stream terminated with error",
                                    details={"chunk": chunk}
                                )
                            
                            # Accumulate text content
                            content = delta.get("content", "")
                            if content:
                                full_text += content
                            
                            # Accumulate tool calls
                            if "tool_calls" in delta:
                                for tc in delta["tool_calls"]:
                                    idx = tc.get("index", 0)
                                    if idx not in current_tool_calls:
                                        current_tool_calls[idx] = {
                                            "id": tc.get("id", ""),
                                            "type": tc.get("type", "function"),
                                            "function": {"name": "", "arguments": ""}
                                        }
                                    if "id" in tc and tc["id"]:
                                        current_tool_calls[idx]["id"] = tc["id"]
                                    if "function" in tc:
                                        fn = tc["function"]
                                        if "name" in fn:
                                            current_tool_calls[idx]["function"]["name"] = fn["name"]
                                        if "arguments" in fn:
                                            current_tool_calls[idx]["function"]["arguments"] += fn["arguments"]
                            
                            # Capture finish reason
                            if choice.get("finish_reason"):
                                finish_reason = choice["finish_reason"]
                                
                        except json.JSONDecodeError:
                            pass
                
                # Convert accumulated tool_calls
                if current_tool_calls:
                    tool_calls = [current_tool_calls[i] for i in sorted(current_tool_calls.keys())]
                    
        except httpx.RequestError as e:
            raise LLMError(
                code="proxy_unavailable",
                message=f"LLM proxy stream request failed: {e}",
                details={"proxy_url": stream_url}
            )
        
        latency_ms = int((time.time() - start) * 1000)
        
        # Estimate tokens (actual not available in streaming)
        est_tokens = len(full_text) // 4
        cost = 0.0  # Cost not available in streaming
        
        self.total_tokens += est_tokens
        self.total_cost += cost
        self.request_count += 1
        self._update_model_stats(model, est_tokens, cost)
        
        _log(f"[platform] {model}: {est_tokens} tokens, ${cost:.4f}, {latency_ms}ms")
        
        # Convert accumulated tool_calls dicts to FunctionCall instances
        function_calls: List[FunctionCall] = []
        for tc in tool_calls:
            func = tc.get("function", {})
            raw_args = func.get("arguments", "{}")
            try:
                args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
            except json.JSONDecodeError:
                args = {}
            function_calls.append(FunctionCall(
                name=func.get("name", ""),
                arguments=args if isinstance(args, dict) else {},
                id=tc.get("id"),
                raw_arguments=raw_args if isinstance(raw_args, str) else None,
            ))
        
        return LLMResponse(
            text=full_text,
            model=model,
            tokens=est_tokens,
            cost=cost,
            latency_ms=latency_ms,
            function_calls=function_calls,
            raw=None,
        )
    
    def chat_stream_full(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        on_chunk: Optional[Callable[[str], bool]] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> LLMResponse:
        """Stream and collect full response.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            model: Model to use
            on_chunk: Callback for each chunk. Return False to stop.
            temperature: Sampling temperature
            max_tokens: Max tokens
            extra_body: Extra parameters to merge into request body
        """
        model = self._get_model(model)
        temp = temperature if temperature is not None else self.temperature
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        
        # Use specialized streaming for OpenAI Responses API (includes function_calls)
        if self.provider == "openai" and _is_openai_responses_model(model) and not self._use_platform_bridge:
            return self._stream_openai_responses_full(
                messages, model, None, temp, tokens, on_chunk, extra_body
            )
        
        start = time.time()
        full_text = ""
        
        for chunk in self.chat_stream(messages, model=model, 
                                      temperature=temperature, max_tokens=max_tokens,
                                      extra_body=extra_body):
            full_text += chunk
            if on_chunk and not on_chunk(chunk):
                break
        
        latency_ms = int((time.time() - start) * 1000)
        
        # Estimate tokens (actual count not available in streaming)
        est_tokens = len(full_text) // 4
        # No cost available in streaming without provider support, use 0
        cost = 0.0
        
        self.total_tokens += est_tokens
        self.total_cost += cost
        self.request_count += 1
        
        self._update_model_stats(model, est_tokens, cost)
        
        return LLMResponse(
            text=full_text,
            model=model,
            tokens=est_tokens,
            cost=cost,
            latency_ms=latency_ms,
        )
    
    def chat_with_functions(
        self,
        messages: List[Dict[str, str]],
        tools: List[Tool],
        model: Optional[str] = None,
        max_iterations: int = 10,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> LLMResponse:
        """Chat with automatic function execution."""
        messages = list(messages)
        
        for _ in range(max_iterations):
            response = self.chat(messages, model=model, tools=tools,
                               temperature=temperature, max_tokens=max_tokens)
            
            if not response.function_calls:
                return response
            
            for call in response.function_calls:
                try:
                    result = self.execute_function(call)
                    messages.append({
                        "role": "assistant",
                        "content": None,
                        "tool_calls": [{
                            "id": call.id,
                            "type": "function",
                            "function": {
                                "name": call.name,
                                "arguments": json.dumps(call.arguments),
                            }
                        }]
                    })
                    messages.append({
                        "role": "tool",
                        "tool_call_id": call.id,
                        "content": json.dumps(result) if not isinstance(result, str) else result,
                    })
                except Exception as e:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": call.id,
                        "content": f"Error: {e}",
                    })
        
        return response
    
    def execute_function(self, call: FunctionCall) -> Any:
        """Execute a registered function."""
        if call.name not in self._function_handlers:
            raise LLMError(
                code="unknown_function",
                message=f"Function '{call.name}' not registered",
                details={"registered_functions": list(self._function_handlers.keys())}
            )
        return self._function_handlers[call.name](**call.arguments)
    
    def _handle_api_error(self, response: httpx.Response, model: str):
        """Parse API error and raise LLMError with details."""
        status = response.status_code
        
        # Try to parse error from response body - DON'T truncate, show full error
        try:
            body = response.json()
            # Bridge returns error as string directly in "error" field
            if isinstance(body.get("error"), str):
                error_message = body["error"]
                error_type = "api_error"
            else:
                # OpenAI/Anthropic format: {"error": {"message": "...", "type": "..."}}
                error_info = body.get("error", {})
                error_message = error_info.get("message", response.text) if isinstance(error_info, dict) else str(error_info)
                error_type = error_info.get("type", "api_error") if isinstance(error_info, dict) else "api_error"
        except:
            error_message = response.text if response.text else "Unknown error"
            error_type = "api_error"
        
        # Map HTTP status to error code, but KEEP the actual error message
        if status == 401:
            code = "authentication_error"
        elif status == 403:
            code = "permission_denied"
        elif status == 404:
            code = "not_found"
        elif status == 429:
            code = "rate_limit"
        elif status == 500:
            code = "server_error"
        elif status == 503:
            code = "service_unavailable"
        else:
            code = error_type
        
        # Always use the actual error message from the response
        raise LLMError(
            code=code,
            message=error_message,
            details={
                "http_status": status,
                "model": model,
                "provider": self.provider,
            }
        )
    
    def _parse_platform_response(self, data: Dict, model: str, start: float) -> LLMResponse:
        """Parse platform bridge response format."""
        # Platform bridge response format:
        # {"success": true, "content": "...", "model": "...", "usage": {...}, "cost_usd": 0.001}
        # With tool calls:
        # {"success": true, "content": "...", "tool_calls": [...], ...}
        
        if not data.get("success", False):
            error = data.get("error", "Unknown platform error")
            raise LLMError(
                code="platform_error",
                message=error,
                details={"raw_response": data}
            )
        
        text = data.get("content", "") or ""
        response_model = data.get("model") or model
        
        usage = data.get("usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = usage.get("total_tokens", prompt_tokens + completion_tokens)
        
        # Extract cached tokens from prompt_tokens_details (OpenRouter with usage: {include: true})
        prompt_details = usage.get("prompt_tokens_details", {}) or {}
        cached_tokens = prompt_details.get("cached_tokens", 0) or 0
        
        cost = data.get("cost_usd", 0.0) or 0.0
        latency_ms = int((time.time() - start) * 1000)
        
        self.total_tokens += total_tokens
        self.total_cost += cost
        self.request_count += 1
        self._update_model_stats(response_model, total_tokens, cost)
        
        # Log with cache info if available
        if cached_tokens > 0:
            cache_pct = (cached_tokens / prompt_tokens * 100) if prompt_tokens > 0 else 0
            _log(f"[platform] {response_model}: {total_tokens} tokens ({cached_tokens} cached, {cache_pct:.0f}%), ${cost:.4f}, {latency_ms}ms")
        else:
            _log(f"[platform] {response_model}: {total_tokens} tokens, ${cost:.4f}, {latency_ms}ms")
        
        # Parse function calls / tool calls if present in platform response
        function_calls = []
        
        # Check for tool_calls directly in response (platform format)
        tool_calls_data = data.get("tool_calls", [])
        
        # Also check nested in choices[0].message.tool_calls (OpenAI format forwarded)
        if not tool_calls_data:
            choices = data.get("choices", [])
            if choices:
                message = choices[0].get("message", {})
                tool_calls_data = message.get("tool_calls", [])
        
        # Also check in raw_response if platform forwards it
        if not tool_calls_data and data.get("raw_response"):
            raw = data.get("raw_response", {})
            choices = raw.get("choices", [])
            if choices:
                message = choices[0].get("message", {})
                tool_calls_data = message.get("tool_calls", [])
        
        for tc in tool_calls_data or []:
            if isinstance(tc, dict):
                # OpenAI format: {"type": "function", "function": {"name": ..., "arguments": ...}}
                if tc.get("type") == "function":
                    func = tc.get("function", {})
                    raw_args = func.get("arguments", "{}")
                    try:
                        args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
                    except (json.JSONDecodeError, TypeError):
                        args = {} if isinstance(raw_args, str) else raw_args
                    function_calls.append(FunctionCall(
                        name=func.get("name", ""),
                        arguments=args,
                        id=tc.get("id"),
                        raw_arguments=raw_args if isinstance(raw_args, str) else None,
                    ))
                # Direct format: {"name": ..., "arguments": ...}
                elif tc.get("name"):
                    raw_args = tc.get("arguments", {})
                    try:
                        args = raw_args
                        if isinstance(args, str):
                            args = json.loads(args)
                    except (json.JSONDecodeError, TypeError):
                        args = {}
                    function_calls.append(FunctionCall(
                        name=tc.get("name", ""),
                        arguments=args if isinstance(args, dict) else {},
                        id=tc.get("id"),
                        raw_arguments=raw_args if isinstance(raw_args, str) else None,
                    ))
        
        return LLMResponse(
            text=text,
            model=response_model,
            tokens=total_tokens,
            cost=cost,
            latency_ms=latency_ms,
            function_calls=function_calls,
            raw=data,
            cached_tokens=cached_tokens,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
        )

    def _parse_response(self, data: Dict, model: str, start: float) -> LLMResponse:
        choice = data.get("choices", [{}])[0]
        message = choice.get("message", {})
        text = message.get("content", "") or ""
        
        function_calls = []
        for tc in message.get("tool_calls", []) or []:
            if tc.get("type") == "function":
                func = tc.get("function", {})
                raw_args = func.get("arguments", "{}")
                try:
                    args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
                except:
                    args = {}
                function_calls.append(FunctionCall(
                    name=func.get("name", ""),
                    arguments=args if isinstance(args, dict) else {},
                    id=tc.get("id"),
                    raw_arguments=raw_args if isinstance(raw_args, str) else None,
                ))
        
        usage = data.get("usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = prompt_tokens + completion_tokens
        
        # Extract cached tokens from prompt_tokens_details (OpenRouter with usage: {include: true})
        prompt_details = usage.get("prompt_tokens_details", {}) or {}
        cached_tokens = prompt_details.get("cached_tokens", 0) or 0
        
        # Use provider-reported cost if available (OpenRouter returns usage.cost)
        # OpenAI doesn't return cost, so default to 0
        cost = usage.get("cost", 0.0) or 0.0
        latency_ms = int((time.time() - start) * 1000)
        
        self.total_tokens += total_tokens
        self.total_cost += cost
        self.request_count += 1
        self._update_model_stats(model, total_tokens, cost)
        
        # Log with cache info if available
        if cached_tokens > 0:
            cache_pct = (cached_tokens / prompt_tokens * 100) if prompt_tokens > 0 else 0
            _log(f"{model}: {total_tokens} tokens ({cached_tokens} cached, {cache_pct:.0f}%), ${cost:.4f}, {latency_ms}ms")
        else:
            _log(f"{model}: {total_tokens} tokens, ${cost:.4f}, {latency_ms}ms")
        
        return LLMResponse(
            text=text,
            model=model,
            tokens=total_tokens,
            cost=cost,
            latency_ms=latency_ms,
            function_calls=function_calls,
            raw=data,
            cached_tokens=cached_tokens,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
        )
    
    def _update_model_stats(self, model: str, tokens: int, cost: float):
        if model not in self.stats:
            self.stats[model] = {"tokens": 0, "cost": 0.0, "requests": 0}
        self.stats[model]["tokens"] += tokens
        self.stats[model]["cost"] += cost
        self.stats[model]["requests"] += 1
    
    def _calculate_cost(self, model: str, prompt_tokens: int, completion_tokens: int) -> float:
        input_price, output_price = 0.5, 1.5
        for key, prices in PRICING.items():
            if key in model.lower():
                input_price, output_price = prices
                break
        return (prompt_tokens * input_price + completion_tokens * output_price) / 1_000_000
    
    def get_stats(self, model: Optional[str] = None) -> Dict[str, Any]:
        """Get usage stats."""
        if model:
            return self.stats.get(model, {"tokens": 0, "cost": 0.0, "requests": 0})
        return {
            "total_tokens": self.total_tokens,
            "total_cost": self.total_cost,
            "request_count": self.request_count,
            "per_model": self.stats,
        }
    
    def close(self):
        self._client.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, *args):
        self.close()
