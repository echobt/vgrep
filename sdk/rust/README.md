# Term SDK for Rust

Build agents with streaming LLM support.

## Installation

Add to `Cargo.toml`:

```toml
[dependencies]
term-sdk = { path = "sdk/rust" }
```

## Quick Start

```rust
use term_sdk::{Agent, Request, Response, run};

struct MyAgent;

impl Agent for MyAgent {
    fn solve(&mut self, req: &Request) -> Response {
        if req.step == 1 {
            return Response::cmd("ls -la");
        }
        if req.has("hello") {
            return Response::done();
        }
        Response::cmd("echo hello")
    }
}

fn main() {
    run(&mut MyAgent);
}
```

## With LLM (Streaming)

```rust
use term_sdk::{Agent, Request, Response, LLM, LLMError, run};

struct LLMAgent {
    llm: LLM,
}

impl Agent for LLMAgent {
    fn solve(&mut self, req: &Request) -> Response {
        let prompt = format!("Task: {}\nOutput: {:?}", req.instruction, req.output);
        
        // Streaming with callback
        match self.llm.ask_stream(&prompt, "claude-3-haiku", |chunk| {
            print!("{}", chunk);
            true  // Return false to stop early
        }) {
            Ok(result) => Response::from_llm(&result.text),
            Err(e) => {
                // Error is JSON: {"error": {"code": "...", "message": "..."}}
                eprintln!("LLM Error: {}", e);
                Response::done()
            }
        }
    }
}

fn main() {
    let mut agent = LLMAgent { llm: LLM::new() };
    run(&mut agent);
}
```

## Streaming API

```rust
use term_sdk::LLM;

let mut llm = LLM::new();

// Streaming with callback
let result = llm.ask_stream("Tell a story", "claude-3-haiku", |chunk| {
    print!("{}", chunk);
    true  // Return false to stop early
})?;
println!("\nTotal: {}", result.text);

// Non-streaming
let result = llm.ask("Question", "claude-3-haiku")?;
```

## Error Handling

Errors are returned as JSON strings:

```rust
use term_sdk::{LLM, LLMError};

let mut llm = LLM::new();

match llm.ask("Question", "claude-3-haiku") {
    Ok(response) => println!("{}", response.text),
    Err(error_json) => {
        // error_json is: {"error": {"code": "rate_limit", "message": "...", "details": {...}}}
        eprintln!("Error: {}", error_json);
        
        // Or parse it
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(&error_json) {
            let code = parsed["error"]["code"].as_str().unwrap_or("unknown");
            let message = parsed["error"]["message"].as_str().unwrap_or("");
            eprintln!("Code: {}, Message: {}", code, message);
        }
    }
}
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `invalid_provider` | - | Unknown provider |
| `no_model` | - | No model specified |
| `authentication_error` | 401 | Invalid API key |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `service_unavailable` | 503 | Service unavailable |
| `unknown_function` | - | Function not registered |

## Function Calling

```rust
use term_sdk::{LLM, Tool, Message};

let mut llm = LLM::new();

// Register function
llm.register_function("search", |args| {
    let query = args.get("query").and_then(|v| v.as_str()).unwrap_or("");
    Ok(format!("Results for {}", query))
});

// Define tool
let tools = vec![Tool::new("search", "Search for files")
    .with_parameters(serde_json::json!({
        "type": "object",
        "properties": { "query": { "type": "string" } }
    }))];

// Chat with functions
let result = llm.chat_with_functions(
    &[Message::user("Search for Rust files")],
    &tools,
    "claude-3-haiku",
    5,  // max iterations
)?;
```

## API Reference

### Request

| Field | Type | Description |
|-------|------|-------------|
| `instruction` | String | Task to complete |
| `step` | u32 | Step number (1-indexed) |
| `last_command` | Option<String> | Previous command |
| `output` | Option<String> | Command output |
| `exit_code` | Option<i32> | Exit code |
| `cwd` | String | Working directory |

Methods:
- `req.first()` - True on step 1
- `req.ok()` - True if exit_code == Some(0)
- `req.failed()` - True if exit_code is Some and != 0
- `req.has("pattern")` - Check output contains pattern

### Response

```rust
Response::cmd("ls -la")       // Execute command
Response::done()              // Task complete
Response::from_llm(&text)     // Parse from LLM output
Response::say("message")      // Text without command
```

### LLM

```rust
// OpenRouter (default)
let llm = LLM::new();
let llm = LLM::with_provider(Provider::OpenRouter);

// Chutes
let llm = LLM::with_provider(Provider::Chutes);

// With options
let llm = LLM::new()
    .default_model("claude-3-haiku")
    .temperature(0.5)
    .max_tokens(2048);
```

## Providers

| Provider | Env Variable | Models |
|----------|--------------|--------|
| OpenRouter | `OPENROUTER_API_KEY` | Claude, GPT-4, Llama, Mixtral |
| Chutes | `CHUTES_API_KEY` | Llama, Qwen, Mixtral |

## Environment Variables

| Variable | Description |
|----------|-------------|
| `LLM_API_KEY` | API key (primary) |
| `OPENROUTER_API_KEY` | OpenRouter API key |
| `CHUTES_API_KEY` | Chutes API key |
| `LLM_API_URL` | Custom API endpoint |
