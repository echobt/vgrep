# Term SDK for TypeScript

Build agents with streaming LLM support.

## Installation

```bash
cd sdk/typescript
npm install
npm run build
```

## Quick Start

```typescript
import { Agent, Request, Response, run } from 'term-sdk';

class MyAgent implements Agent {
  solve(req: Request): Response {
    if (req.step === 1) {
      return Response.cmd("ls -la");
    }
    if (req.has("hello")) {
      return Response.done();
    }
    return Response.cmd("echo hello");
  }
}

run(new MyAgent());
```

## With LLM (Streaming)

```typescript
import { Agent, Request, Response, LLM, LLMError, run } from 'term-sdk';

class LLMAgent implements Agent {
  private llm = new LLM();

  async solve(req: Request): Promise<Response> {
    try {
      // Streaming - see response in real-time
      let fullText = "";
      for await (const chunk of this.llm.stream(
        `Task: ${req.instruction}\nOutput: ${req.output}`,
        { model: "claude-3-haiku" }
      )) {
        process.stdout.write(chunk);
        fullText += chunk;
      }
      
      return Response.fromLLM(fullText);
    } catch (e) {
      if (e instanceof LLMError) {
        console.error(`Error ${e.code}: ${e.message}`);
      }
      return Response.done();
    }
  }
}

run(new LLMAgent());
```

## Streaming API

```typescript
import { LLM, LLMError } from 'term-sdk';

const llm = new LLM();

// Async iterator - yields chunks
for await (const chunk of llm.stream("Tell a story", { model: "claude-3-haiku" })) {
  process.stdout.write(chunk);
}

// With callback - return false to stop
const result = await llm.askStream("Solve this", {
  model: "gpt-4o",
  onChunk: (text) => true  // Return false to stop early
});
console.log(result.text);

// Non-streaming
const result = await llm.ask("Question", { model: "claude-3-haiku" });
```

## Error Handling

```typescript
import { LLM, LLMError } from 'term-sdk';

const llm = new LLM();

try {
  const result = await llm.ask("Question", { model: "claude-3-haiku" });
} catch (e) {
  if (e instanceof LLMError) {
    console.log(`Code: ${e.code}`);           // "rate_limit"
    console.log(`Message: ${e.message}`);     // "Rate limit exceeded"
    console.log(`Details:`, e.details);       // { httpStatus: 429, ... }
    console.log(`JSON:`, JSON.stringify(e.toJSON()));
  }
}
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `invalid_provider` | - | Unknown provider |
| `no_model` | - | No model specified |
| `authentication_error` | 401 | Invalid API key |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `service_unavailable` | 503 | Service unavailable |
| `unknown_function` | - | Function not registered |

## Function Calling

```typescript
import { LLM, Tool } from 'term-sdk';

const llm = new LLM();

// Register function
llm.registerFunction("search", async (args) => `Results for ${args.query}`);

// Define tool
const tools = [new Tool(
  "search",
  "Search for files",
  { type: "object", properties: { query: { type: "string" } } }
)];

// Chat with functions
const result = await llm.chatWithFunctions(
  [{ role: "user", content: "Search for TypeScript files" }],
  tools,
  { model: "claude-3-haiku" }
);
```

## API Reference

### Request

| Field | Type | Description |
|-------|------|-------------|
| `instruction` | string | Task to complete |
| `step` | number | Step number (1-indexed) |
| `lastCommand` | string? | Previous command |
| `output` | string? | Command output |
| `exitCode` | number? | Exit code |
| `cwd` | string | Working directory |

Properties:
- `req.first` - True on step 1
- `req.ok` - True if exitCode === 0
- `req.failed` - True if exitCode !== null && exitCode !== 0
- `req.has("pattern")` - Check output contains pattern

### Response

```typescript
Response.cmd("ls -la")       // Execute command
Response.done()              // Task complete
Response.fromLLM(text)       // Parse from LLM output
Response.say("message")      // Text without command
```

### LLM

```typescript
// OpenRouter (default)
const llm = new LLM();
const llm = new LLM({ provider: "openrouter" });

// Chutes
const llm = new LLM({ provider: "chutes" });

// With default model
const llm = new LLM({ defaultModel: "claude-3-haiku" });
```

## Providers

| Provider | Env Variable | Models |
|----------|--------------|--------|
| OpenRouter | `OPENROUTER_API_KEY` | Claude, GPT-4, Llama, Mixtral |
| Chutes | `CHUTES_API_KEY` | Llama, Qwen, Mixtral |

## Environment Variables

| Variable | Description |
|----------|-------------|
| `LLM_API_KEY` | API key (primary) |
| `OPENROUTER_API_KEY` | OpenRouter API key |
| `CHUTES_API_KEY` | Chutes API key |
| `LLM_API_URL` | Custom API endpoint |
