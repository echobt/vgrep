/**
 * Term SDK for TypeScript
 * 
 * Build agents with streaming LLM support.
 * Providers: OpenRouter, Chutes
 * 
 * @example Streaming
 * ```typescript
 * const llm = new LLM();
 * 
 * // Stream chunks
 * for await (const chunk of llm.stream("Tell a story", { model: "claude-3-haiku" })) {
 *   process.stdout.write(chunk);
 * }
 * 
 * // Stream with callback
 * const result = await llm.askStream("Solve", {
 *   model: "gpt-4o",
 *   onChunk: (text) => {
 *     console.log(text);
 *     return !text.includes("ERROR");  // Stop on error
 *   }
 * });
 * ```
 */

// ============================================================================
// Types
// ============================================================================

export interface RequestData {
  instruction: string;
  step: number;
  last_command: string | null;
  output: string | null;
  exit_code: number | null;
  cwd: string;
}

export class Request {
  readonly instruction: string;
  readonly step: number;
  readonly lastCommand: string | null;
  readonly output: string | null;
  readonly exitCode: number | null;
  readonly cwd: string;

  constructor(data: RequestData) {
    this.instruction = data.instruction;
    this.step = data.step;
    this.lastCommand = data.last_command;
    this.output = data.output;
    this.exitCode = data.exit_code;
    this.cwd = data.cwd || "/app";
  }

  static parse(json: string): Request {
    return new Request(JSON.parse(json));
  }

  get first(): boolean { return this.step === 1; }
  get ok(): boolean { return this.exitCode === 0; }
  get failed(): boolean { return this.exitCode !== null && this.exitCode !== 0; }

  has(...patterns: string[]): boolean {
    if (!this.output) return false;
    const lower = this.output.toLowerCase();
    return patterns.some(p => lower.includes(p.toLowerCase()));
  }
}

export class Response {
  command: string | null;
  text: string | null;
  taskComplete: boolean;
  data: Record<string, any> | null;

  constructor(
    command: string | null = null,
    text: string | null = null,
    taskComplete = false,
    data: Record<string, any> | null = null
  ) {
    this.command = command;
    this.text = text;
    this.taskComplete = taskComplete;
    this.data = data;
  }

  static cmd(command: string, text?: string): Response {
    return new Response(command, text ?? null, false);
  }

  static say(text: string): Response {
    return new Response(null, text, false);
  }

  static done(text?: string): Response {
    return new Response(null, text ?? null, true);
  }

  withText(text: string): Response {
    this.text = text;
    return this;
  }

  withData(data: Record<string, any>): Response {
    this.data = data;
    return this;
  }

  complete(): Response {
    this.taskComplete = true;
    return this;
  }

  toJSON(): string {
    const obj: any = { command: this.command, task_complete: this.taskComplete };
    if (this.text) obj.text = this.text;
    if (this.data) obj.data = this.data;
    return JSON.stringify(obj);
  }

  static fromLLM(text: string): Response {
    text = text.trim();
    const codeMatch = text.match(/```(?:json)?\s*(\{[\s\S]*?\})\s*```/);
    if (codeMatch) text = codeMatch[1];
    const start = text.indexOf('{');
    const end = text.lastIndexOf('}');
    if (start >= 0 && end > start) {
      try {
        const data = JSON.parse(text.slice(start, end + 1));
        return new Response(data.command ?? null, data.text ?? null, data.task_complete ?? false, data.data ?? null);
      } catch { }
    }
    return Response.done();
  }
}

// ============================================================================
// Function Calling
// ============================================================================

export interface FunctionCall {
  name: string;
  arguments: Record<string, any>;
  id?: string;
}

export class Tool {
  name: string;
  description: string;
  parameters: Record<string, any>;

  constructor(name: string, description: string, parameters: Record<string, any> = {}) {
    this.name = name;
    this.description = description;
    this.parameters = parameters;
  }

  toJSON(): any {
    return {
      type: "function",
      function: { name: this.name, description: this.description, parameters: this.parameters }
    };
  }
}

// ============================================================================
// Agent
// ============================================================================

export interface Agent {
  setup?(): void | Promise<void>;
  solve(request: Request): Response | Promise<Response>;
  cleanup?(): void | Promise<void>;
}

// Aliases for compatibility
export type AgentRequest = Request;
export type AgentResponse = Response;

// ============================================================================
// Runner
// ============================================================================

function log(msg: string): void {
  console.error(`[agent] ${msg}`);
}

/**
 * Run an agent in the Term Challenge harness.
 * 
 * Reads requests from stdin (line by line), calls agent.solve(), writes response to stdout.
 * The agent process stays alive between steps, preserving memory/state.
 */
export async function run(agent: Agent): Promise<void> {
  try {
    // Setup once at start
    if (agent.setup) await agent.setup();
    
    // Read requests line by line (allows persistent process)
    const readline = await import('readline');
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
      terminal: false
    });
    
    for await (const line of rl) {
      const trimmed = line.trim();
      if (!trimmed) continue;
      
      try {
        // Parse request
        const request = Request.parse(trimmed);
        log(`Step ${request.step}: ${request.instruction.slice(0, 50)}...`);
        
        // Solve
        const response = await agent.solve(request);
        
        // Output (single line JSON)
        console.log(response.toJSON());
        
        // If task complete, exit
        if (response.taskComplete) break;
        
      } catch (e) {
        log(`Error in step: ${e}`);
        console.log(Response.done().toJSON());
        break;
      }
    }
    
    // Cleanup when done
    if (agent.cleanup) await agent.cleanup();
    
  } catch (e) {
    log(`Fatal error: ${e}`);
    console.log(Response.done().toJSON());
  }
}

// ============================================================================
// LLM Errors
// ============================================================================

export interface LLMErrorDetails {
  httpStatus?: number;
  model?: string;
  provider?: string;
  rawError?: string;
  validProviders?: string[];
  hint?: string;
  registeredFunctions?: string[];
}

export class LLMError extends Error {
  code: string;
  details: LLMErrorDetails;

  constructor(code: string, message: string, details: LLMErrorDetails = {}) {
    super(JSON.stringify({ error: { code, message, details } }));
    this.name = 'LLMError';
    this.code = code;
    this.details = details;
  }

  toJSON(): { error: { code: string; message: string; details: LLMErrorDetails } } {
    return {
      error: {
        code: this.code,
        message: this.message,
        details: this.details,
      }
    };
  }

  toString(): string {
    return `LLMError(${this.code}): ${this.message}`;
  }
}

// ============================================================================
// LLM Client with Streaming
// ============================================================================

export type Provider = 'openrouter' | 'chutes';

export interface LLMOptions {
  provider?: Provider;
  defaultModel?: string;
  temperature?: number;
  maxTokens?: number;
  timeout?: number;
}

export interface ChatOptions {
  model?: string;
  tools?: Tool[];
  temperature?: number;
  maxTokens?: number;
  onChunk?: (chunk: string) => boolean;  // Return false to stop
}

export interface LLMResponse {
  text: string;
  model: string;
  tokens: number;
  cost: number;
  latencyMs: number;
  functionCalls: FunctionCall[];
  raw?: any;
}

export interface Message {
  role: 'system' | 'user' | 'assistant' | 'tool';
  content: string | null;
  tool_calls?: any[];
  tool_call_id?: string;
}

export interface ModelStats {
  tokens: number;
  cost: number;
  requests: number;
}

type FunctionHandler = (args: Record<string, any>) => any | Promise<any>;

const PROVIDERS: Record<Provider, { url: string; envKey: string }> = {
  openrouter: { url: 'https://openrouter.ai/api/v1/chat/completions', envKey: 'OPENROUTER_API_KEY' },
  chutes: { url: 'https://llm.chutes.ai/v1/chat/completions', envKey: 'CHUTES_API_KEY' },
};

const PRICING: Record<string, [number, number]> = {
  'claude-3-haiku': [0.25, 1.25],
  'claude-3-sonnet': [3.0, 15.0],
  'claude-3-opus': [15.0, 75.0],
  'gpt-4o': [5.0, 15.0],
  'gpt-4o-mini': [0.15, 0.6],
  'llama-3': [0.2, 0.2],
  'mixtral': [0.5, 0.5],
  'qwen': [0.2, 0.2],
};

/**
 * LLM client with streaming support.
 * 
 * @example
 * ```typescript
 * const llm = new LLM();
 * 
 * // Regular call
 * const result = await llm.ask("Question", { model: "claude-3-haiku" });
 * 
 * // Streaming
 * for await (const chunk of llm.stream("Story", { model: "claude-3-opus" })) {
 *   process.stdout.write(chunk);
 * }
 * 
 * // Stream with early stop
 * const result = await llm.askStream("Task", {
 *   model: "gpt-4o",
 *   onChunk: (text) => !text.includes("DONE")
 * });
 * ```
 */
export class LLM {
  private provider: Provider;
  private defaultModel?: string;
  private temperature: number;
  private maxTokens: number;
  private timeout: number;
  private apiUrl: string;
  private apiKey: string;
  private functionHandlers: Map<string, FunctionHandler> = new Map();
  private stats: Map<string, ModelStats> = new Map();

  totalTokens = 0;
  totalCost = 0;
  requestCount = 0;

  constructor(options: LLMOptions = {}) {
    this.provider = options.provider || 'openrouter';
    this.defaultModel = options.defaultModel;
    this.temperature = options.temperature ?? 0.3;
    this.maxTokens = options.maxTokens ?? 4096;
    this.timeout = options.timeout ?? 120000;

    const config = PROVIDERS[this.provider];
    if (!config) {
      throw new LLMError('invalid_provider', `Unknown provider: ${this.provider}`, {
        validProviders: Object.keys(PROVIDERS),
      });
    }
    this.apiUrl = process.env.LLM_API_URL || config.url;
    this.apiKey = process.env.LLM_API_KEY || process.env[config.envKey] || '';

    if (!this.apiKey) {
      console.error(`[llm] Warning: LLM_API_KEY or ${config.envKey} not set`);
    }
  }

  private getModel(model?: string): string {
    if (model) return model;
    if (this.defaultModel) return this.defaultModel;
    throw new LLMError('no_model', 'No model specified', {
      hint: 'Pass model option or set defaultModel in LLM constructor',
    });
  }

  registerFunction(name: string, handler: FunctionHandler): void {
    this.functionHandlers.set(name, handler);
  }

  async ask(prompt: string, options: ChatOptions = {}): Promise<LLMResponse> {
    const messages: Message[] = [{ role: 'user', content: prompt }];
    return this.chat(messages, options);
  }

  async* stream(prompt: string, options: ChatOptions = {}): AsyncGenerator<string> {
    const messages: Message[] = [{ role: 'user', content: prompt }];
    yield* this.chatStream(messages, options);
  }

  async askStream(prompt: string, options: ChatOptions = {}): Promise<LLMResponse> {
    const messages: Message[] = [{ role: 'user', content: prompt }];
    return this.chatStreamFull(messages, options);
  }

  async chat(messages: Message[], options: ChatOptions = {}): Promise<LLMResponse> {
    const model = this.getModel(options.model);
    const start = Date.now();

    const payload: any = {
      model,
      messages,
      temperature: options.temperature ?? this.temperature,
      max_tokens: options.maxTokens ?? this.maxTokens,
      stream: false,
    };

    if (options.tools?.length) {
      payload.tools = options.tools.map(t => t.toJSON());
      payload.tool_choice = "auto";
    }

    const response = await fetch(this.apiUrl, {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${this.apiKey}`, 'Content-Type': 'application/json' },
      body: JSON.stringify(payload),
      signal: AbortSignal.timeout(this.timeout),
    });

    if (!response.ok) {
      await this.handleApiError(response, model);
    }

    const data = await response.json() as any;
    return this.parseResponse(data, model, start);
  }

  async* chatStream(messages: Message[], options: ChatOptions = {}): AsyncGenerator<string> {
    const model = this.getModel(options.model);

    const payload = {
      model,
      messages,
      temperature: options.temperature ?? this.temperature,
      max_tokens: options.maxTokens ?? this.maxTokens,
      stream: true,
    };

    const response = await fetch(this.apiUrl, {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${this.apiKey}`, 'Content-Type': 'application/json' },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      await this.handleApiError(response, model);
    }
    if (!response.body) {
      throw new LLMError('no_response_body', 'No response body from API', { model, provider: this.provider });
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          if (data === '[DONE]') return;
          try {
            const chunk = JSON.parse(data);
            const content = chunk.choices?.[0]?.delta?.content || '';
            if (content) yield content;
          } catch { }
        }
      }
    }
  }

  async chatStreamFull(messages: Message[], options: ChatOptions = {}): Promise<LLMResponse> {
    const model = this.getModel(options.model);
    const start = Date.now();
    let fullText = '';

    for await (const chunk of this.chatStream(messages, options)) {
      fullText += chunk;
      if (options.onChunk && !options.onChunk(chunk)) break;
    }

    const latencyMs = Date.now() - start;
    const estTokens = Math.ceil(fullText.length / 4);
    const cost = this.calculateCost(model, estTokens / 2, estTokens / 2);

    this.totalTokens += estTokens;
    this.totalCost += cost;
    this.requestCount++;
    this.updateModelStats(model, estTokens, cost);

    return { text: fullText, model, tokens: estTokens, cost, latencyMs, functionCalls: [] };
  }

  async chatWithFunctions(messages: Message[], tools: Tool[], options: ChatOptions & { maxIterations?: number } = {}): Promise<LLMResponse> {
    const maxIterations = options.maxIterations ?? 10;
    const msgs = [...messages];

    for (let i = 0; i < maxIterations; i++) {
      const response = await this.chat(msgs, { ...options, tools });
      if (response.functionCalls.length === 0) return response;

      for (const call of response.functionCalls) {
        try {
          const result = await this.executeFunction(call);
          msgs.push({
            role: 'assistant', content: null,
            tool_calls: [{ id: call.id, type: 'function', function: { name: call.name, arguments: JSON.stringify(call.arguments) } }]
          });
          msgs.push({ role: 'tool', tool_call_id: call.id, content: typeof result === 'string' ? result : JSON.stringify(result) });
        } catch (e) {
          msgs.push({ role: 'tool', tool_call_id: call.id, content: `Error: ${e}` });
        }
      }
    }
    return this.chat(msgs, { ...options, tools });
  }

  async executeFunction(call: FunctionCall): Promise<any> {
    const handler = this.functionHandlers.get(call.name);
    if (!handler) {
      throw new LLMError('unknown_function', `Function '${call.name}' not registered`, {
        registeredFunctions: Array.from(this.functionHandlers.keys()),
      });
    }
    return handler(call.arguments);
  }

  private async handleApiError(response: globalThis.Response, model: string): Promise<never> {
    const status = response.status;
    let errorMessage = '';
    let errorType = 'api_error';

    try {
      const body = await response.json() as any;
      errorMessage = body.error?.message || response.statusText;
      errorType = body.error?.type || 'api_error';
    } catch {
      errorMessage = response.statusText || 'Unknown error';
    }

    let code: string;
    let message: string;

    switch (status) {
      case 401:
        code = 'authentication_error';
        message = 'Invalid API key';
        break;
      case 403:
        code = 'permission_denied';
        message = 'Access denied for this model or endpoint';
        break;
      case 404:
        code = 'not_found';
        message = `Model '${model}' not found`;
        break;
      case 429:
        code = 'rate_limit';
        message = 'Rate limit exceeded';
        break;
      case 500:
        code = 'server_error';
        message = 'Provider server error';
        break;
      case 503:
        code = 'service_unavailable';
        message = 'Provider service temporarily unavailable';
        break;
      default:
        code = errorType;
        message = errorMessage;
    }

    throw new LLMError(code, message, {
      httpStatus: status,
      model,
      provider: this.provider,
      rawError: errorMessage,
    });
  }

  private parseResponse(data: any, model: string, start: number): LLMResponse {
    const choice = data.choices?.[0] || {};
    const message = choice.message || {};
    const text = message.content || '';

    const functionCalls: FunctionCall[] = [];
    for (const tc of message.tool_calls || []) {
      if (tc.type === 'function') {
        let args = {};
        try { args = JSON.parse(tc.function?.arguments || '{}'); } catch { }
        functionCalls.push({ name: tc.function?.name || '', arguments: args, id: tc.id });
      }
    }

    const promptTokens = data.usage?.prompt_tokens || 0;
    const completionTokens = data.usage?.completion_tokens || 0;
    const tokens = promptTokens + completionTokens;
    const cost = this.calculateCost(model, promptTokens, completionTokens);
    const latencyMs = Date.now() - start;

    this.totalTokens += tokens;
    this.totalCost += cost;
    this.requestCount++;
    this.updateModelStats(model, tokens, cost);

    console.error(`[llm] ${model}: ${tokens} tokens, $${cost.toFixed(4)}, ${latencyMs}ms`);

    return { text, model, tokens, cost, latencyMs, functionCalls, raw: data };
  }

  private updateModelStats(model: string, tokens: number, cost: number): void {
    const stats = this.stats.get(model) || { tokens: 0, cost: 0, requests: 0 };
    stats.tokens += tokens;
    stats.cost += cost;
    stats.requests++;
    this.stats.set(model, stats);
  }

  private calculateCost(model: string, promptTokens: number, completionTokens: number): number {
    let [inputPrice, outputPrice] = [0.5, 1.5];
    for (const [key, prices] of Object.entries(PRICING)) {
      if (model.toLowerCase().includes(key)) { [inputPrice, outputPrice] = prices; break; }
    }
    return (promptTokens * inputPrice + completionTokens * outputPrice) / 1_000_000;
  }

  getStats(model?: string): ModelStats | { totalTokens: number; totalCost: number; requestCount: number; perModel: Record<string, ModelStats> } {
    if (model) return this.stats.get(model) || { tokens: 0, cost: 0, requests: 0 };
    const perModel: Record<string, ModelStats> = {};
    this.stats.forEach((v, k) => { perModel[k] = v; });
    return { totalTokens: this.totalTokens, totalCost: this.totalCost, requestCount: this.requestCount, perModel };
  }
}
