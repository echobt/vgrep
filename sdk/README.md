# Term Challenge SDK

Build agents with streaming LLM support.

## Providers

- **OpenRouter** (default) - Claude, GPT-4, Llama, Mixtral
- **Chutes** - Llama, Qwen, Mixtral

## Features

- **Streaming** - See LLM responses in real-time
- **Multi-Model** - Use different models per call
- **Function Calling** - Define custom tools
- **Structured Errors** - JSON error responses with codes

## Quick Start

### Python

```python
from term_sdk import Agent, Request, Response, LLM, LLMError, run

class MyAgent(Agent):
    def setup(self):
        self.llm = LLM()
    
    def solve(self, req: Request) -> Response:
        try:
            for chunk in self.llm.stream("Solve", model="claude-3-haiku"):
                print(chunk, end="", flush=True)
            return Response.from_llm(chunk)
        except LLMError as e:
            print(f"Error {e.code}: {e.message}")
            return Response.done()

if __name__ == "__main__":
    run(MyAgent())
```

### TypeScript

```typescript
import { Agent, Request, Response, LLM, LLMError, run } from 'term-sdk';

class MyAgent implements Agent {
  private llm = new LLM();

  async solve(req: Request): Promise<Response> {
    try {
      for await (const chunk of this.llm.stream("Solve", { model: "claude-3-haiku" })) {
        process.stdout.write(chunk);
      }
      return Response.fromLLM(chunk);
    } catch (e) {
      if (e instanceof LLMError) console.error(`Error ${e.code}`);
      return Response.done();
    }
  }
}

run(new MyAgent());
```

### Rust

```rust
use term_sdk::{Agent, Request, Response, LLM, run};

struct MyAgent { llm: LLM }

impl Agent for MyAgent {
    fn solve(&mut self, req: &Request) -> Response {
        match self.llm.ask_stream("Solve", "claude-3-haiku", |c| { print!("{}", c); true }) {
            Ok(r) => Response::from_llm(&r.text),
            Err(e) => { eprintln!("Error: {}", e); Response::done() }
        }
    }
}

fn main() { run(&mut MyAgent { llm: LLM::new() }); }
```

## Streaming

### Python
```python
# Iterator
for chunk in llm.stream("Question", model="claude-3-haiku"):
    print(chunk, end="")

# With callback
result = llm.ask_stream("Question", model="gpt-4o", on_chunk=lambda c: True)
```

### TypeScript
```typescript
// Async iterator
for await (const chunk of llm.stream("Question", { model: "claude-3-haiku" })) {
  process.stdout.write(chunk);
}

// With callback
const result = await llm.askStream("Question", { model: "gpt-4o", onChunk: (c) => true });
```

### Rust
```rust
let result = llm.ask_stream("Question", "claude-3-haiku", |chunk| {
    print!("{}", chunk);
    true  // false to stop
})?;
```

## Error Handling

All SDKs return structured JSON errors:

```json
{
  "error": {
    "code": "rate_limit",
    "message": "Rate limit exceeded",
    "details": {
      "http_status": 429,
      "model": "claude-3-haiku",
      "provider": "openrouter"
    }
  }
}
```

### Error Codes

| Code | HTTP | Description |
|------|------|-------------|
| `invalid_provider` | - | Unknown provider |
| `no_model` | - | No model specified |
| `authentication_error` | 401 | Invalid API key |
| `permission_denied` | 403 | Access denied |
| `not_found` | 404 | Model not found |
| `rate_limit` | 429 | Rate limit exceeded |
| `server_error` | 500 | Provider error |
| `service_unavailable` | 503 | Service unavailable |
| `unknown_function` | - | Function not registered |

### Python
```python
from term_sdk import LLMError

try:
    result = llm.ask("Q", model="claude-3-haiku")
except LLMError as e:
    print(e.code, e.message, e.details)
```

### TypeScript
```typescript
import { LLMError } from 'term-sdk';

try {
  await llm.ask("Q", { model: "claude-3-haiku" });
} catch (e) {
  if (e instanceof LLMError) console.log(e.code, e.details);
}
```

### Rust
```rust
match llm.ask("Q", "claude-3-haiku") {
    Ok(r) => println!("{}", r.text),
    Err(json) => eprintln!("{}", json),  // JSON error string
}
```

## Environment Variables

| Variable | Description |
|----------|-------------|
| `LLM_API_KEY` | API key (primary) |
| `OPENROUTER_API_KEY` | OpenRouter API key |
| `CHUTES_API_KEY` | Chutes API key |
| `LLM_API_URL` | Custom API endpoint |

## Models

| Model | Speed | Cost |
|-------|-------|------|
| `claude-3-haiku` | Fast | $ |
| `claude-3-sonnet` | Medium | $$ |
| `claude-3-opus` | Slow | $$$ |
| `gpt-4o` | Medium | $$ |
| `gpt-4o-mini` | Fast | $ |
| `llama-3-70b` | Medium | $ |
| `mixtral-8x7b` | Fast | $ |

## Installation

### Python
```bash
pip install -e sdk/python
```

### TypeScript
```bash
cd sdk/typescript && npm install && npm run build
```

### Rust
```toml
[dependencies]
term-sdk = { path = "sdk/rust" }
```

## Documentation

- [Python SDK](./python/README.md)
- [TypeScript SDK](./typescript/README.md)
- [Rust SDK](./rust/README.md)
